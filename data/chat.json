[
  {
    "id": "01KDHRKM6VGTKAQ7G4541ZF8GH",
    "title": "first deep search for this, and then create a gene",
    "createdAt": "2025-12-28T05:58:54.940Z",
    "updatedAt": "2025-12-28T06:01:12.893Z",
    "userId": "01K9ABA6MQ9K64MY7M4AEBCAP2",
    "messages": [
      {
        "id": "01KDHRKTP4H6SXRR5H7SX6AFDD",
        "role": "user",
        "content": "first deep search for this, and then create a general assisstant report and then summarize the output and then create gta style image based on the summary \"how to build stream analytics using clickhouse database and best ways to increase pipeline performance",
        "createdAt": "2025-12-28T05:59:01.572Z"
      },
      {
        "id": "01KDHRMDKFP0GH7B6WP3NF4TFB",
        "role": "assistant",
        "content": "Todo list generated. Please approve to execute.",
        "agentId": "orchestrator",
        "metadata": {
          "todos": [
            {
              "id": "01KDHRMDJSQH06D04CKFGS2W0M",
              "title": "Perform a deep search for stream analytics using ClickHouse",
              "description": "Conduct a comprehensive web search to gather information on how to build stream analytics using ClickHouse database and the best ways to increase pipeline performance.",
              "status": "completed",
              "agentId": "search",
              "agentType": "search",
              "dependencies": [],
              "input": {
                "body": {
                  "userPrompt": "how to build stream analytics using ClickHouse database and best ways to increase pipeline performance",
                  "search_tool_name": "parallel_ai-search-pro",
                  "max_results": 5
                },
                "extra_body": {}
              },
              "createdAt": "2025-12-28T05:59:20.922Z",
              "completedAt": "2025-12-28T06:00:38.297Z",
              "output": "[\n  {\n    \"title\": \"Real-time Analytics with ClickHouse\",\n    \"url\": \"https://clickhouse.com/use-cases/real-time-analytics\",\n    \"snippet\": \"* [Use cases](/use-cases)\\n* /\\n* Real-time analytics\\n\\n# Real-time analytics with ClickHouse\\n\\nIngest millions of rows per second. Handle the most heavily concurrent workloads. All without compromising query speed. Create user-facing dashboards and applications with instant responsiveness\\n\\nDrive well-informed business decisions with the most up-to-date data\\n\\nRun SQL-based analytics effortlessly on your most diverse, high-cardinality datasets\\n\\nSeamlessly integrate static or streaming data from a wide range of sources\\n\\nUse your favorite visualization tools like Grafana, Tableau, Superset, and more\\n\\n[Get started today](https://console.clickhouse.cloud/signUp?loc=use-case-real-time-analytics) [Contact sales](/company/contact?loc=use-case-real-time-analytics)\\n\\n## The real-time database that truly shines at scale. Count on blazing performance when low latency really matters. Discover why companies are choosing ClickHouse for their real-time analytics applications.\\n01\\n\\nBest-in-class query performance that your mission-critical and time-sensitive applications can depend on. 02\\n\\nBuilt for continuous ingest with native integrations and managed pipelines for your favorite streaming sources. 03\\n\\nHigh query concurrency so you can build interactive apps your users love without worrying about responsiveness at scale. 04\\n\\nUse the tools you love through our extensive ecosystem of integrations, including popular products like Tableau, Metabase, DBT, and many more. > \\\"This year we actually exceeded a thousand active replicas. That's **processing hundreds of millions of inserted rows every second** , which actually corresponds to quite a significantly larger number of events because we've been using a lot of sampling.\\\" > \\n>\\n\\n[> \\\"We have multiple clusters deployed on our hardware with hundreds of hosts. Our main cluster is now geo-replicated, and we designate some replicas for read-heavy operations and others for write-heavy operations.\\nMany of our tables are quite large, **with trillions and trillions of rows, as well as tens of columns. ** \\\" > >](/blog/how-clickhouse-powers-ahrefs-the-worlds-most-active-web-crawler)\\n\\n[> \\\"Weâ€™ve had a positive experience with ClickHouse. It allowed us to scale LangSmith to production workloads and provide a service where users can log all of their data. We couldnâ€™t have accomplished this without ClickHouse.\\\" > >](/blog/langchain-why-we-choose-clickhouse-to-power-langchain)\\n\\n[> \\\"In the post-evaluation of each database against our criteria (with metrics ranging from query performance to cost), **ClickHouse emerged as the unrivaled frontrunner. ** It excelled across the board, even astonishingly so in certain domains, and proved more cost-efficient.\\\"\\n ... \\n[](/user-stories?loc=gitlab) [](/user-stories?loc=mux) [](/user-stories?loc=netapp) [](/user-stories?loc=servicenow) [](/user-stories?loc=posthog) [](/blog/weights-and-biases-scale-ai-development?loc=carousel) [](/user-stories?loc=deutsche-bank&search=deutsche) [](/blog/behind-the-scenes-how-clickhouse-helps-vimeo-power-video-analytics-at-scale?loc=carousel) [](/videos/funnel-analytics-and-distributed-queries-in-clickhouse) [](/blog/unlocking-the-power-of-onchain-analytics-how-nansen-transformed-their-data-infrastructure-with-clickhouse-cloud) [](/blog/poolside-using-clickhouse-to-build-next-gen-ai-for-software-development) [](/user-stories?loc=sony&search=sony) [](/blog/hifis-migration-from-bigquery-to-clickhouse?loc=carousel) [](/user-stories?loc=vercel) [](/videos/ramp?loc=carousel) [](/blog/transforming-ad-tech-how-cognitiv-uses-clickhouse-to-build-better-machine-learning-models?loc=carousel) [](/user-stories?loc=statsig) [](/user-stories?loc=didi&search=didi)\\n[](/blog/prefect-event-driven-workflow-orchestration-powered-by-clickhouse?loc=prefect) [](/user-stories?loc=cursor) [](/blog/how-trip.com-migrated-from-elasticsearch-and-built-a-50pb-logging-solution-with-clickhouse) [](/videos/sierraai?loc=sierra) [](/blog/seeing-the-big-picture-shopees-journey-to-distributed-tracing-with-clickhouse?loc=carousel) [](/blog/clickhouse-replaces-postgres-to-power-real-time-analytics-in-common-room-customer-portal?loc=carousel) [](/blog/tekion-adopts-clickhouse-cloud-to-power-application-performance-and-metrics-monitoring?loc=carousel) [](/user-stories?loc=whatnot) [](/videos/scaling-graphite-with-clickhouse?loc=carousel) [](/blog/why-astronomer-chose-clickhouse-to-power-its-new-data-observability-platform-astro-observe) [](/blog/corsearch-replaces-mysql-with-clickhouse-for-content-and-brand-protection?loc=carousel) [](/blog/upollos-journey-from-bigquery-to-clickhouse?loc=carousel) [](/blog/amp-clickhouse-oss-to-clickhouse-cloud?loc=carousel)\\n[](/blog/electrum-driving-net-zero-carbon-future-with-clickhouse) [](/user-stories?loc=spoton) [](/blog/how-memorial-sloan-kettering-cancer-center-is-using-clickhouse-to-accelerate-cancer-research) [](/blog/migrating-to-clickhouse-cloud-in-just-three-months-with-no-downtime-a-120-terabyte-data-journey-and-its-benefits) [](/videos/vantage) [](/blog/how-solarwinds-uses-clickhouse-for-realtime-observability-at-scale) [](/blog/longbridge-technology-simplifies-their-architecture-and-achieves-10x-performance-boost-with-clickhouse) [](/blog/how-harvey-uses-clickhouse-for-proactive-threat-detection) [](/user-stories?loc=royal-bank&search=royal+bank) [](/blog/observing-in-style-how-poizon-rebuilt-its-data-platform-with-clickhouse-enterprise-edition) [](/blog/how-real-estate-analytics-made-its-data-pipeline-50x-faster-with-clickhouse) [](/blog/how-anthropic-is-using-clickhouse-to-scale-observability-for-ai-era)\\n ... \\n[](/user-stories?loc=servicenow) [](/user-stories?loc=posthog) [](/blog/weights-and-biases-scale-ai-development?loc=carousel) [](/user-stories?loc=deutsche-bank&search=deutsche) [](/blog/behind-the-scenes-how-clickhouse-helps-vimeo-power-video-analytics-at-scale?loc=carousel) [](/videos/funnel-analytics-and-distributed-queries-in-clickhouse) [](/blog/unlocking-the-power-of-onchain-analytics-how-nansen-transformed-their-data-infrastructure-with-clickhouse-cloud) [](/blog/poolside-using-clickhouse-to-build-next-gen-ai-for-software-development) [](/user-stories?loc=sony&search=sony) [](/blog/hifis-migration-from-bigquery-to-clickhouse?loc=carousel) [](/user-stories?loc=vercel) [](/videos/ramp?loc=carousel) [](/blog/transforming-ad-tech-how-cognitiv-uses-clickhouse-to-build-better-machine-learning-models?loc=carousel) [](/user-stories?loc=statsig) [](/user-stories?loc=didi&search=didi) [](/blog/prefect-event-driven-workflow-orchestration-powered-by-clickhouse?loc=prefect)\\n ... \\n[](/blog/how-memorial-sloan-kettering-cancer-center-is-using-clickhouse-to-accelerate-cancer-research) [](/blog/migrating-to-clickhouse-cloud-in-just-three-months-with-no-downtime-a-120-terabyte-data-journey-and-its-benefits) [](/videos/vantage) [](/blog/how-solarwinds-uses-clickhouse-for-realtime-observability-at-scale) [](/blog/longbridge-technology-simplifies-their-architecture-and-achieves-10x-performance-boost-with-clickhouse) [](/blog/how-harvey-uses-clickhouse-for-proactive-threat-detection) [](/user-stories?loc=royal-bank&search=royal+bank) [](/blog/observing-in-style-how-poizon-rebuilt-its-data-platform-with-clickhouse-enterprise-edition) [](/blog/how-real-estate-analytics-made-its-data-pipeline-50x-faster-with-clickhouse) [](/blog/how-anthropic-is-using-clickhouse-to-scale-observability-for-ai-era) [](/blog/scaling-observabilty-for-thousands-of-gpus-at-character-ai)\\n\\n## Real-time applications and dashboards\\n\\nClickHouse is used across industries to power systems and\\napplications where real-time analysis, evaluation, and querying are critical. User-facing dashboards and apps\\n\\nE-commerce optimization\\n\\nRetail analytics\\n\\nSupply chain optimization\\n\\nFraud detection\\n\\nThreat prevention\\n\\nProactive maintenance\\n\\nIntelligent automation\\n\\nUser behavior analytics\\n\\nTrend evaluation and monitoring\\n\\nGaming analytics\\n\\nAdvertising and marketing analysis\\n\\n## ClickHouse for real-time analytics\\n\\nWhether youâ€™re building user-facing dashboards, instantly responsive applications, or analyzing data on the fly, ClickHouse has you covered. Weâ€™re built to ensure that even the most sophisticated data analysis can be done intuitively, using simple SQL. Our parallelized query execution engine, best-in-class compression rates, and column-oriented design deliver unparalleled performance at scale so that you can focus on insights and forget worrying about infrastructure.\\n### Data ingestion\\n\\n### Data transformations and queries\\n\\n### Applications, dashboards, and more\\n\\n### Unlock real-time insights and user experiences\\n\\nWhether youâ€™re performing live analysis to drive business outcomes or building interactive user-facing applications, ClickHouse delivers the unparalleled speed-to-insight you can depend on. ### Simplify your SQL\\n\\nClickHouse supports an extensive library of domain-specific functions that transform even the most complex queries into simple SQL statements. With ClickHouse, real-time data exploration is easy and powerful. ### Manage data efficiently\\n\\nWith highly optimized compression techniques and our fully parallelized query pipeline, ClickHouse maximizes CPU efficiency and provides the power to quickly process huge volumes of compressed data. ### Scale effortlessly\\n\\nClickHouse scales both vertically and horizontally to meet the demands of any workload - read and write - to systems with hundreds of cores and petabytes of storage.\\nClickHouse provides the scalability you need to effortlessly handle increasing data volumes and workloads over time. ## Supporting references\\n\\nFor detailed guides about how to get started with ClickHouse for real-time analytics workloads, follow along in our blog:\\n\\n1. [Asynchronous Data Inserts in ClickHouse](/blog/asynchronous-data-inserts-in-clickhouse)\\n2. [Real-time event streaming with ClickHouse, Confluent Cloud and ClickPipes](/blog/real-time-event-streaming-with-confluent-cloud-clickhouse-and-clickpipes)\\n3. [Adding Real-Time Analytics to a Supabase Application With ClickHouse](/blog/building-real-time-applications-with-clickhouse-and-hex-notebook-keeper-engine)\\n4. [Using Materialized Views in ClickHouse](/blog/using-materialized-views-in-clickhouse)\\n5. [Working with Time Series Data in ClickHouse](/blog/working-with-time-series-data-and-functions-ClickHouse)\\n6.\\n[Change Data Capture (CDC) with PostgreSQL and ClickHouse - Part 1](/blog/clickhouse-postgresql-change-data-capture-cdc-part-1)\\n7. [Change Data Capture (CDC) with PostgreSQL and ClickHouse - Part 2](/blog/clickhouse-postgresql-change-data-capture-cdc-part-2)\\n8. [How Cloudflare Processes Hundreds of Millions of Rows per Second with ClickHouse](/blog/how-cloudflare-processes-hundreds-of-millions-of-rows-per-second-with-clickhouse)\\n\\n## Get started with ClickHouse Cloud for free\\n\\nWe'll get you started on a 30 day trial and $300 credits to spend at your own pace. [Create a free account](https://console.clickhouse.cloud/signUp?loc=real-time-use-case-getstarted-footer)\\n\\nStay informed on feature releases, product roadmap, support, and cloud offerings! Loading form...\\n\\n[Star us on Github](https://github.com/ClickHouse/ClickHouse)\\n\\nÂ© 2025 ClickHouse, Inc. HQ in the Bay Area, CA and Amsterdam, NL.\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"clickhouse.com\",\n    \"source_title\": \"Real-time Analytics with ClickHouse\",\n    \"source_link\": \"https://clickhouse.com/use-cases/real-time-analytics\"\n  },\n  {\n    \"title\": \"How to optimize ClickHouseÂ® for high-throughput streaming\",\n    \"url\": \"https://www.tinybird.co/blog/clickhouse-streaming-analytics\",\n    \"snippet\": \"[Back to Blog](/blog)\\n\\nShare this article:\\n\\n[](http://www.facebook.com/share.php?u=https://www.tinybird.co/blog/clickhouse-streaming-analytics) [](https://x.com/intent/tweet?original_referer=https://www.tinybird.co/blog/clickhouse-streaming-analytics&utm_source=x&url=https://www.tinybird.co/blog/clickhouse-streaming-analytics&utm_medium=social&utm_source=x&source=tweetbutton&text=How%20to%20optimize%20ClickHouse%C2%AE%20for%20high-throughput%20streaming) [](http://www.linkedin.com/shareArticle?mini=true&url=https://www.tinybird.co/blog/clickhouse-streaming-analytics&utm_medium=social&utm_source=linkedin) [](https://news.ycombinator.com/submitlink?u=https://www.tinybird.co/blog/clickhouse-streaming-analytics&t=How%20to%20optimize%20ClickHouse%C2%AE%20for%20high-throughput%20streaming)\\n\\nTinybird is not affiliated with, associated with, or sponsored by ClickHouse, Inc. ClickHouseÂ® is a registered trademark of ClickHouse, Inc.\\n\\n[Back](/blog)\\n\\nOct 17, 2025\\n\\n# How to optimize ClickHouse\\n<sup>Â®</sup> for high-throughput streaming\\n\\nLearn how to optimize ClickHouse <sup>Â®</sup> for million-row-per-second streaming analytics with expert tips on table design, Kafka ingestion, and query performance tuning. [AI Resources](/blog/categories/ai-resources)\\n\\n[Cameron Archer Tech Writer](/blog/authors/cameron-archer)\\n\\nStreaming analytics systems process millions of events per second, but most databases struggle to ingest data fast enough while still returning query results in milliseconds. ClickHouse <sup>Â®</sup> is built differently: its columnar storage, vectorized execution, and merge tree architecture can handle both high-velocity writes and sub-second analytical queries on billions of rows. This guide covers how to optimize ClickHouse <sup>Â®</sup> for high-throughput streaming workloads, from schema design and ingestion tuning to query optimization and infrastructure decisions.\\nYou'll learn the specific settings, patterns, and tradeoffs that determine whether your system handles thousands or millions of events per second. Unlike complex stream processors like Apache Flink, which [most teams don't actually need](https://www.tinybird.co/blog-posts/flink-is-95-problem) , ClickHouse <sup>Â®</sup> provides a simpler approach using familiar SQL. ## Why ClickHouse <sup>Â®</sup> excels at high-throughput streaming analytics\\n\\nClickHouse <sup>Â®</sup> is a columnar database designed for analytical processing, which makes it particularly well-suited for streaming analytics workloads. The database stores data by column rather than by row, which means queries that aggregate or filter specific fields can skip reading unnecessary data entirely. This columnar approach delivers compression ratios of [10x or more](https://clickhouse.com/engineering-resources/database-compression) compared to row-oriented databases.\\nWhen you're ingesting millions of events per second, that compression directly reduces both storage costs and the amount of data that queries need to scan. ClickHouse <sup>Â®</sup> uses [vectorized query execution](https://clickhouse.com/docs/en/introduction/performance.html) , processing data in batches rather than one row at a time. Modern CPUs can apply the same operation to multiple values simultaneously through SIMD instructions, which is why [ClickHouse <sup>Â®</sup> queries](https://www.tinybird.co/blog-posts/clickhouse-query-examples) can return results in under 100 milliseconds even when scanning billions of rows. The database scales both vertically and horizontally. You can add more CPU cores and memory to a single server, or you can distribute data across multiple shards with replication for fault tolerance.\\n## Core architecture for real-time ingestion and queries\\n\\nA typical [streaming analytics setup](https://www.tinybird.co/blog-posts/real-time-streaming-data-architectures-that-scale) with ClickHouse <sup>Â®</sup> involves four main components that work together. Each component handles a specific part of the data flow, from ingestion to serving queries. ### 1\\\\. Streaming sources to buffer tables\\n\\nData arrives from sources like Apache Kafka, webhooks, or HTTP APIs. ClickHouse <sup>Â®</sup> can consume directly from Kafka using the Kafka table engine, which creates a consumer that reads messages and writes them to a target table. Buffer tables accumulate incoming data in memory before flushing to disk. This reduces the number of small data parts created during high-velocity ingestion, which otherwise slows down background merges and queries. You can also batch inserts on the client side, which gives you more control over batch sizes and error handling.\\n ... \\nEach materialized view writes to its own MergeTree table that you can query independently. The tradeoff is that backfilling becomes more complex. If you need to recalculate historical aggregates, you'll need to rebuild the materialized view or manually insert corrected data. ### 3\\\\. Denormalised analytics tables\\n\\nClickHouse <sup>Â®</sup> performs best when related data lives in the same table rather than split across multiple tables with joins. This means duplicating user attributes into each event row instead of storing them separately. For example, instead of joining an events table with a users table, you might embed user\\\\_name, user\\\\_tier, and user\\\\_region directly into each event. This increases storage slightly but eliminates expensive join operations. The tradeoff is write amplification. When a user attribute changes, you may need to update many rows. For streaming analytics where reads vastly outnumber writes, this tradeoff usually makes sense. ### 4\\\\.\\n ... \\nInstead, it creates granules (blocks of rows) and stores min/max values for each granule, letting queries skip entire granules that don't match filters. ### 2\\\\. Partition by time and cardinality\\n\\nPartitioning splits a table into separate directories based on a partition key, typically a time-based column like date or hour. This makes dropping old data fast and can speed up queries that filter by the partition key. Too many partitions hurt performance though. Each partition creates separate files on disk, and ClickHouse <sup>Â®</sup> tracks metadata for all of them. A good guideline is keeping partitions under a few thousand. For most streaming workloads, daily or hourly partitions work well. Late-arriving data can still be inserted after a partition is finalized, but it may trigger additional merges. If late arrivals are common, consider using longer partition intervals. ### 3\\\\.\\nApply codecs and compression wisely\\n\\nClickHouse <sup>Â®</sup> supports several compression codecs that trade CPU time for storage savings. LZ4 is the default and balances speed with compression. ZSTD provides better compression ratios but uses more CPU during both compression and decompression. Per-column codecs can optimize specific data types:\\n\\n* **Delta and DoubleDelta:** Work well for timestamps and monotonically increasing values\\n* **Gorilla:** Designed for floating-point time series data\\n* **LZ4 vs ZSTD:** LZ4 for hot data queried frequently, ZSTD for cold data rarely accessed\\n\\nHigher compression uses more CPU during writes and reads. For data queried often, LZ4 minimizes query latency. For data rarely accessed, ZSTD significantly reduces storage costs. ### 4\\\\. Avoid costly mutations and TTL clashes\\n\\n`ALTER TABLE` operations that modify or delete existing data (mutations) are expensive because they rewrite entire data parts.\\nFrequent mutations slow ingestion by competing for disk I/O and CPU with background merges. For workloads that need updates or deletes, ReplacingMergeTree with a version column offers an alternative. This engine automatically deduplicates rows with the same `ORDER BY` key during merges, keeping only the row with the highest version number. TTL (Time To Live) rules automatically delete old data based on a time column. However, TTL operations can trigger large numbers of merges if not configured carefully. Align TTL intervals with partition boundaries to minimize merge overhead. ## Optimizing merge tree settings for streaming workloads\\n\\nClickHouse <sup>Â®</sup> continuously merges small data parts into larger ones in the background. Tuning these merge settings significantly impacts both ingestion throughput and query performance. ### Adjusting `max_partitions_to_read`\\n\\nThe `max_partitions_to_read` setting limits how many partitions a single query can scan.\\n ... \\n## Minimising query latency on hot data\\n\\nEven with optimized ingestion, queries can be slow if they scan too much data or don't use ClickHouse <sup>Â®</sup> 's indexing features. For visualizing performance metrics, you can [connect Grafana to ClickHouse <sup>Â®</sup>](https://www.tinybird.co/blog-posts/clickhouse-grafana-example) to monitor query latency and throughput in real-time. ### Bloom filters and skip indexes\\n\\nSkip indexes (also called data skipping indexes) let ClickHouse <sup>Â®</sup> skip entire granules of data that don't match a query's `WHERE` clause. Bloom filters ( `tokenbf_v1` ) are particularly useful for high-cardinality string columns like user IDs or session IDs. To add a Bloom filter index, use `ALTER TABLE ADD INDEX` with the `tokenbf_v1` type. The index is built asynchronously during background merges. Choose indexes based on your most common query patterns. If you frequently filter by `user_id` , add a Bloom filter on that column.\\n ... \\n### Balancing `max_threads` and concurrency\\n\\nThe `max_threads` setting controls how many CPU cores ClickHouse <sup>Â®</sup> uses for a single query. More threads can speed up large queries but increase CPU usage and can starve other queries of resources. For streaming analytics workloads with many concurrent queries, you may want to limit `max_threads` to prevent any single query from monopolizing the CPU. A value of 4-8 threads per query is often a good starting point. The `max_concurrent_queries` setting limits the total number of queries that can run simultaneously. This prevents overload during traffic spikes and ensures each query gets a reasonable share of CPU and memory. ## Next steps to ship streaming analytics features with Tinybird\\n\\n[Tinybird](https://tinybird.co) is a [managed ClickHouse <sup>Â®</sup> platform](https://www.tinybird.co/product/managed-clickhouse) designed for developers who want to integrate real-time analytics into applications without managing infrastructure.\\nThe platform handles ingestion, storage, query optimization, and API creation. You can get started with Tinybird by creating a free account at <https://cloud.tinybird.co/signup> . The free tier includes generous limits for development and testing, with no credit card required. Tinybird provides rapid ingestion from Kafka, webhooks, and other streaming sources, along with real-time SQL pipes that transform and aggregate data as it arrives. You can deploy parameterized API endpoints with built-in authentication, rate limiting, and row-level security. The platform includes a CLI for local development and CI/CD integration, allowing you to define data pipelines as code, test them locally, and deploy to production with a single command. ## FAQs about streaming analytics in ClickHouse <sup>Â®</sup>\\n\\n### How do I secure per-tenant analytics APIs? Row-level security (RLS) filters data based on the authenticated user or API token, ensuring each tenant only sees their own data.\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"tinybird.co\",\n    \"source_title\": \"How to optimize ClickHouseÂ® for high-throughput streaming\",\n    \"source_link\": \"https://www.tinybird.co/blog/clickhouse-streaming-analytics\"\n  },\n  {\n    \"title\": \"How we use ClickHouse as a real-time stream processing engine | Mux\",\n    \"url\": \"https://www.mux.com/blog/how-we-use-clickhouse-as-a-real-time-stream-processing-engine\",\n    \"snippet\": \"[Skip to content]()\\n\\n[Mux Logo Toggle Mux Brand Popover](/)\\n\\n[Blog Blog](/blog)\\n\\nSearch\\n\\nCopied Share Copied Share\\n\\n[Talk to us Talk to us](/sales-contact)\\n\\n[Log in Log in](https://dashboard.mux.com/login)\\n\\nPublished on December 18, 2024 (about 1 year ago)\\n\\n# How we use ClickHouse as a real-time stream processing engine\\n\\n[](/team/faith-szeto) [](/team/josh-wien)\\n\\nBy [Faith](/team/faith-szeto) and [Josh](/team/josh-wien) â€¢ 18 min read â€¢ [Engineering](/blog/category/engineering)\\n\\n* * *\\n\\nAt Mux, building video at scale comes with many interesting challenges. Weâ€™ve built systems that stream millions of minutes every hour, but the fun doesnâ€™t stop there. Every second of video that gets uploaded, watched, or deleted must be pulled from sources like CDN logs and tracked in a way that can be used by customer dashboards, billing & analytics, and our [Delivery Usage API](https://docs.mux.com/api-reference) . Thatâ€™sâ€¦ a lot of data.\\nFor a long time, we had relied on our trusty old usage pipeline to accomplish this. But as Mux scaled, the existing system became limiting in terms of product and performance. To support new features (e.g. high usage alerting), we needed to achieve fresher data, higher throughput, and faster queries in our pipeline. In this post, we will cover:\\n\\n* The limitations of our existing pipeline\\n* The changes we made to improve performance and scale\\n* Reflections and future plans\\n\\n## [How our architecture looked in the past]()\\n\\nHere is a representation of the former pipeline:\\n\\n### [**CDN log processing**]()\\n\\nWhen a view occurs, the activity is logged within one of our CDNs. Log Importer pulls CDN logs from cloud bucket stores, [performing exactly-once processing and enrichment](/blog/processing-cdn-logs-exactly-once-with-kafka-transactions) . Enriched log messages are sent to Kafka where they are further processed and aggregated by the Media Usage Processor (MUP) Flink application.\\nAggregate rows are written to Postgres, which serves read queries by downstream consumers. ### [**Internal event stream processing**]()\\n\\nIn various regions, internal CDC events are queued, transformed and then mirrored into a single-region Kafka cluster. Asset Events Flink further transforms and enriches the events, then updates Postgres with the finalized results. ### [**How we were using Flink**]()\\n\\n[Apache Flink](https://flink.apache.org/) is a popular stream processing engine which can perform stateful computations with exactly-once guarantees. Common use cases include anomaly detection and real-time analytics. Our MUP Flink application made use of [Window functions](https://flink.apache.org/2015/12/04/introducing-stream-windows-in-apache-flink/) in order to collect various aggregates like â€œseconds deliveredâ€ for a given video. For a given entity (e.g. video asset or environment) it would keep track of hourly aggregates, then finalize and write them to Postgres.\\nBefore finalizing any row, it had to delay a certain amount of time (determined by [maxOutOfOrderness](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/built_in/) , affectionately dubbed â€œMOOâ€ ðŸ®) to be reasonably sure no late records would arrive. Since we had no guarantees about ordering of CDN log records, the MOO value had to be at least an hour in order to ensure we finalized hourly windows only after all records had been processed. Asset Events Flink was simpler - its main function was to perform stateless transformations on incoming events before writing them to Postgres. Maintaining a distributed Flink deployment for this felt like overkill. ### [**Challenges with our Flink architecture**]()\\n\\nThe old system served us well for a long time, but our needs changed as we expanded our user-base and feature set.\\n* During popular live events, like the US presidential election, traffic surges could cause MUP Flink to become a throughput bottleneck unless scaled. With careful tuning and verification this could be fully automated, but at the moment scaling still involved some manual intervention. * While our user base continued to grow, we were also adding new features and breakdowns to our billing dashboard. Increasing user traffic and aggregate complexity led to very slow queries (>1s). * Operational challenges: Our Flink deployments were managed using the [Flink Kubernetes Operator](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.6/) and implemented with a different language (Java) than the rest of our stack (Go). Coupled with Flinkâ€™s steep learning curve, this meant we had a tough time debugging or deploying changes with high confidence. The technology being an outlier from our usual stack meant there was little support in terms of tooling, which made ops tedious.\\n* The performance issues with our self-hosted Postgres repeatedly required delicate sharding and vertical scaling mitigations. New features were also blocked:\\n\\n* Alerts: The ability to alert on high usage was a popular feature request among customers. However, our current aggregates would be stale by multiple hours by the time they were available for any alerting. * Better visualization and analytics: We also wanted to enhance our billing dashboards with more detail and dimensions. But the required additional load on our Postgres was a complete no-go at this point. ### [Time to rethink our approach]()\\n\\nAs growing pains and blocked features grew more pressing, we decided it was time for a rearchitecture. We needed to overcome these key issues:\\n\\n1. Poor aggregate query performance\\n2. Poor data freshness\\n3. Operational challenges\\n\\nAnd, whatever solution we came up with needed to meet the following functional requirements:\\n\\n1. Can power high usage alerting\\n2.\\nCan provide detailed usage breakdowns with hourly granularity for the past 90 days\\n3. Continue to support billing & invoicing jobs\\n\\n## [Laying out the options]()\\n\\nAfter careful research and discussion, we agreed that we had three options:\\n\\n### [**Option 1a: Have ClickHouse serve read queries**]()\\n\\nWe could keep our existing setup, but swap in [ClickHouse](https://clickhouse.com/docs/en/intro) , a database optimized for fast analytics queries, as the read-only replica for Postgres. This would require the least work, but only addressed our poor aggregate query performance, and would exacerbate operational challenges. ### [**Option 1b: Have Flink write to ClickHouse**]()\\n\\nInstead of keeping both Postgres and ClickHouse, we could replace Postgres with ClickHouse entirely. This would be pretty similar to the previous option, but without impacting data freshness issues or ops.\\n ... \\nSome organizations dedicate entire teams to operating Flink clusters. While we could cultivate team expertise, none of us were excited about working with Java. In all honesty, this really biased us against this option. **Pros**\\n\\n* Would reduce data staleness\\n* Might improve read query performance\\n* Stream processing component can remain horizontally scalable\\n\\n**Cons**\\n\\n* Operability burden\\n* Data freshness still not ideal for our use cases\\n* Aggregate queries are unlikely to be improved to the level we require\\n* Working with Javaâ€¦\\n\\n### [**Option 3: Use ClickHouse as our stream-processing engine _and_ persistent data store, replacing both Postgres and Flink**]()\\n\\nWhile we donâ€™t have a team of Flink experts, we _do_ have a team of ClickHouse experts. [ClickHouse](https://clickhouse.com/docs/en/intro) is an OLAP column-oriented database that uses a SQL-like query language. It is useful for â€œcomplex calculations over massive datasetsâ€, like real-time analytics over huge data streams.\\n ... \\nFor example, on an incoming record we had to classify its type, determine if and how much we should bill on, and extract nested objects for processing. Explaining how we achieved all of this stream processing logic in our schema would be difficult without a good understanding of ClickHouse, so we'll save the gory details for a future post. The basic gist is this: we used Kafka Table Engines to ingest from Kafka, then sent the data through a series of cascading [Materialized Views](https://clickhouse.com/docs/en/materialized-view) and [Null Tables](https://clickhouse.com/docs/en/engines/table-engines/special/null) , which allowed us to pre-aggregate and transform the data several times before dumping it into rollup tables, which were optimized for aggregate queries. ### [**Additional considerations**]()\\n\\nIn some high volume pipelines, performance can be prioritized over accuracy. The critical nature of our data prevented us from making this tradeoff.\\n ... \\nEven small amounts of lost data or changes to usage categorization will add up to substantial billing inaccuracies at our scale, and we needed to prove that the new system would be just as reliable as the old. We started by copying over all of the existing data in Postgres to ClickHouse and feeding in new data via Kafka, which was made easier thanks to ClickHouseâ€™s Postgres table engine and our aggregating schema design, but we still needed to verify that there were no dropped or duplicated usage events. Working closely with our Analytics team, we did extensive comparisons in our data warehouse between the Postgres and ClickHouse datasets. Then, going a step further to validate the data we would return to our customers via the Usage API or invoices, we also set up a query shadowing system, where any read query run against the Postgres database would be followed by sending the equivalent query to ClickHouse.\\n ... \\nBecause both of these issues only occurred at scale, we would have missed these without our integrity checks. Instead, we caught them quickly and were able to fix them well ahead of release, by which time there was little room left for doubt. When it finally came time to switch over to the new usage pipeline, the query shadowing system again made this easy and safe. Simply flipping a configuration made clients of the usage databases return the results from ClickHouse instead of Postgres, while still logging the diffs for post-release monitoring. We kept the old system running so that we could cut over back to Postgres just as easily in case an issue arose. Thanks to our careful approach to validation beforehand, however, we never had to, and the old usage pipeline was decommissioned a month later. Weâ€™ll miss you. Ensuring data integrity is an important part of any data migration, even more so when working with business-critical data.\\n ... \\n## [What this approach unlocks for our future]()\\n\\nWeâ€™re pleased with how the new architecture has opened up a whole suite of new features that had not been feasible to build on top of the old system. The improved aggregation and query performance allows us to access the data more often and in new ways, enabling us to provide more granular billing representations to internal teams and our customers. Internal teams will soon gain the ability to recognize revenue with much greater fidelity than before, saving hours of work each month, and customers will be able to dive deeper into their spend through new dashboard visualizations and APIs. On top of that, the increased freshness of the data will no doubt benefit these features, and it also unblocks billing alerting, addressing a frequently requested feature and helping customers to monitor and control their usage without having to watch or track it themselves.\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"mux.com\",\n    \"source_title\": \"How we use ClickHouse as a real-time stream processing engine | Mux\",\n    \"source_link\": \"https://www.mux.com/blog/how-we-use-clickhouse-as-a-real-time-stream-processing-engine\"\n  },\n  {\n    \"title\": \"Guide for Query optimization | ClickHouse Docs\",\n    \"url\": \"https://clickhouse.com/docs/optimize/query-optimization\",\n    \"snippet\": \"[Skip to main content]()\\n\\n[Skip to main content]()\\n\\n* [](/docs/)\\n* [Performance and optimizations](/docs/operations/overview)\\n* Query optimization\\n\\n[Edit this page](https://github.com/ClickHouse/clickhouse-docs/blob/main/docs/guides/best-practices/query-optimization.md)\\n\\n# A simple guide for query optimization\\n\\nThis section aims to illustrate through common scenarios how to use different performance and optimization techniques, such as [analyzer](/docs/operations/analyzer) , [query profiling](/docs/operations/optimizing-performance/sampling-query-profiler) or [avoid nullable Columns](/docs/optimize/avoid-nullable-columns) , in order to improve your ClickHouse query performances. ## Understand query performance [â€‹]( \\\"Direct link to Understand query performance\\\")\\n\\nThe best moment to think about performance optimization is when you're setting up your [data schema](/docs/data-modeling/schema-design) before ingesting data into ClickHouse for the first time.\\nBut let's be honest; it is difficult to predict how much your data will grow or what types of queries will be executed. If you have an existing deployment with a few queries that you want to improve, the first step is understanding how those queries perform and why some execute in a few milliseconds while others take longer. ClickHouse has a rich set of tools to help you understand how your query is getting executed and the resources consumed to perform the execution. In this section, we will look at those tools and how to use them. ## General considerations [â€‹]( \\\"Direct link to General considerations\\\")\\n\\nTo understand query performance, let's look at what happens in ClickHouse when a query is executed. The following part is deliberately simplified and takes some shortcuts; the idea here is not to drown you with details but to get you up to speed with the basic concepts. For more information you can read about [query analyzer](/docs/operations/analyzer) .\\nFrom a very high-level standpoint, when ClickHouse executes a query, the following happens:\\n\\n* **Query parsing and analysis**\\n\\nThe query is parsed and analyzed, and a generic query execution plan is created. * **Query optimization**\\n\\nThe query execution plan is optimized, unnecessary data is pruned, and a query pipeline is built from the query plan. * **Query pipeline execution**\\n\\nThe data is read and processed in parallel. This is the stage where ClickHouse actually executes the query operations such as filtering, aggregations, and sorting. * **Final processing**\\n\\nThe results are merged, sorted, and formatted into a final result before being sent to the client. In reality, many [optimizations](/docs/concepts/why-clickhouse-is-so-fast) are taking place, and we will discuss them a bit more in this guide, but for now, those main concepts give us a good understanding of what is happening behind the scenes when ClickHouse executes a query.\\nWith this high-level understanding, let's examine the tooling ClickHouse provides and how we can use it to track the metrics that affect query performance. ## Dataset [â€‹]( \\\"Direct link to Dataset\\\")\\n\\nWe'll use a real example to illustrate how we approach query performances. Let's use the NYC Taxi dataset, which contains taxi ride data in NYC. First, we start by ingesting the NYC taxi dataset with no optimization. Below is the command to create the table and insert data from an S3 bucket. Note that we infer the schema from the data voluntarily, which is not optimized.\\n```\\n-- Create table with inferred schema\\nCREATE TABLE trips_small_inferred\\nORDER BY () EMPTY\\nAS SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/clickhouse-academy/nyc_taxi_2009-2010.parquet');\\n\\n-- Insert data into table with inferred schema\\nINSERT INTO trips_small_inferred\\nSELECT *\\nFROM s3Cluster\\n('default','https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/clickhouse-academy/nyc_taxi_2009-2010.parquet');\\n```\\n\\nLet's have a look to the table schema automatically inferred from the data.\\n ... \\n[query logs](/docs/operations/system-tables/query_log) . This data is stored in the table `system.query_log` . For each executed query, ClickHouse logs statistics such as query execution time, number of rows read, and resource usage, such as CPU, memory usage, or filesystem cache hits. Therefore, the query log is a good place to start when investigating slow queries. You can easily spot the queries that take a long time to execute and display the resource usage information for each one. Let's find the top five long-running queries on our NYC taxi dataset.\\n ... \\n|Name |Elapsed |Rows processed |Peak memory |\\n| --- | --- | --- | --- |\\n|Query 1 |1\\\\.699 sec |329\\\\.04 million |440\\\\.24 MiB |\\n|Query 2 |1\\\\.419 sec |329\\\\.04 million |546\\\\.75 MiB |\\n|Query 3 |1\\\\.414 sec |329\\\\.04 million |451\\\\.53 MiB |\\n\\n\\nLet's understand a bit better what the queries achieve. * Query 1 calculates the distance distribution in rides with an average speed of over 30 miles per hour. * Query 2 finds the number and average cost of rides per week. * Query 3 calculates the average time of each trip in the dataset. None of these queries are doing very complex processing, except the first query that calculates the trip time on the fly every time the query executes. However, each of these queries takes more than one second to execute, which, in the ClickHouse world, is a very long time. We can also note the memory usage of these queries; more or less 400 Mb for each query is quite a lot of memory. Also, each query appears to read the same number of rows (i.e., 329.04 million).\\n ... \\nFinally, the result is sorted and outputted. Here, we can note that no primary keys are used, which makes sense as we didn't define any when we created the table. As a result, ClickHouse is doing a full scan of the table for the query. **Explain Pipeline**\\n\\nEXPLAIN Pipeline shows the concrete execution strategy for the query. There, you can see how ClickHouse actually executed the generic query plan we looked at previously. ```\\nEXPLAIN PIPELINE\\nWITH\\n    dateDiff('s', pickup_datetime, dropoff_datetime) AS trip_time,\\n    (trip_distance / trip_time) * 3600 AS speed_mph\\nSELECT quantiles(0.5, 0.75, 0.9, 0.99)(trip_distance)\\nFROM nyc_taxi.trips_small_inferred\\nWHERE speed_mph > 30\\n\\nQuery id: c7e11e7b-d970-4e35-936c-ecfc24e3b879\\n\\n    â”Œâ”€explainâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\\n 1. â”‚ (Expression)                                                                        â”‚\\n 2.\\n ... \\nDepending on how you ingested your data, you might have leveraged ClickHouse [capabilities](/docs/interfaces/schema-inference) to infer the table schema based on the ingested data. While this is very practical to get started, if you want to optimize your query performance, you'll need to review the data schema to best fit your use case. ### Nullable [â€‹]( \\\"Direct link to Nullable\\\")\\n\\nAs described in the [best practices documentation](/docs/best-practices/select-data-types) , avoid nullable columns wherever possible. It is tempting to use them often, as they make the data ingestion mechanism more flexible, but they negatively affect performance as an additional column has to be processed every time. Running an SQL query that counts the rows with a NULL value can easily reveal the columns in your tables that actually need a Nullable value.\\n ... \\n|Name |Run 1 - Elapsed |Elapsed |Rows processed |Peak memory |\\n| --- | --- | --- | --- | --- |\\n|Query 1 |1\\\\.699 sec |1\\\\.353 sec |329\\\\.04 million |337\\\\.12 MiB |\\n|Query 2 |1\\\\.419 sec |1\\\\.171 sec |329\\\\.04 million |531\\\\.09 MiB |\\n|Query 3 |1\\\\.414 sec |1\\\\.188 sec |329\\\\.04 million |265\\\\.05 MiB |\\n\\n\\nWe notice some improvements in both query time and memory usage. Thanks to the optimization in the data schema, we reduce the total volume of data that represents our data, leading to improved memory consumption and reduced processing time. Let's check the size of the tables to see the difference.\\n ... \\n## The importance of primary keys [â€‹]( \\\"Direct link to The importance of primary keys\\\")\\n\\nPrimary keys in ClickHouse work differently than in most traditional database systems. In those systems, primary keys enforce uniqueness and data integrity. Any attempt to insert duplicate primary key values is rejected, and a B-tree or hash-based index is usually created for fast lookup. In ClickHouse, the primary key's [objective](/docs/guides/best-practices/sparse-primary-indexes) is different; it does not enforce uniqueness or help with data integrity. Instead, it is designed to optimize query performance. The primary key defines the order in which the data is stored on disk and is implemented as a sparse index that stores pointers to the first row of each granule. > Granules in ClickHouse are the smallest units of data read during query execution. They contain up to a fixed number of rows, determined by index\\\\_granularity, with a default value of 8192 rows.\\nGranules are stored contiguously and sorted by the primary key. > \\n> \\n\\nSelecting a good set of primary keys is important for performance, and it's actually common to store the same data in different tables and use different sets of primary keys to speed up a specific set of queries. Other options supported by ClickHouse, such as Projection or Materialized view, allow you to use a different set of primary keys on the same data. The second part of this blog series will cover this in more detail. ### Choose primary keys [â€‹]( \\\"Direct link to Choose primary keys\\\")\\n\\nChoosing the correct set of primary keys is a complex topic, and it might require trade-offs and experiments to find the best combination. For now, we're going to follow these simple practices:\\n\\n* Use fields that are used to filter in most queries\\n* Choose columns with lower cardinality first\\n* Consider a time-based component in your primary key, as filtering by time on a timestamp dataset is pretty common.\\n ... \\nâ”‚                 Condition: and((pickup_datetime in (-Inf, 1238543999]), (pickup_datetime in [1230768000, +Inf))) â”‚\\n13. â”‚                 Parts: 9/9                                                                                       â”‚\\n14. â”‚                 Granules: 5061/40167                                                                             â”‚\\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\\n```\\n\\nThanks to the primary key, only a subset of the table granules has been selected. This alone greatly improves the query performance since ClickHouse has to process significantly less data. ## Next steps [â€‹]( \\\"Direct link to Next steps\\\")\\n\\nHopefully this guide gets a good understanding on how to investigate slow queries with ClickHouse and how to make them faster.\\n ... \\n[Previous Performance and optimizations overview](/docs/operations/overview) [Next Primary indexes](/docs/guides/best-practices/sparse-primary-indexes)\\n\\nÂ© 2016â€“2025 ClickHouse, Inc.\\n\\n[Trademark](https://clickhouse.com/legal/trademark-policy) Â· [Privacy](https://clickhouse.com/legal/privacy-policy) Â· [Security](https://trust.clickhouse.com/) Â· [Terms of Service](https://clickhouse.com/legal/agreements/terms-of-service)\\n\\nÂ© 2016â€“2025 ClickHouse, Inc.\\n\\n[Trademark](https://clickhouse.com/legal/trademark-policy) Â· [Privacy](https://clickhouse.com/legal/privacy-policy) Â· [Security](https://trust.clickhouse.com/) Â· [Terms of Service](https://clickhouse.com/legal/agreements/terms-of-service)\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"clickhouse.com\",\n    \"source_title\": \"Guide for Query optimization | ClickHouse Docs\",\n    \"source_link\": \"https://clickhouse.com/docs/optimize/query-optimization\"\n  },\n  {\n    \"title\": \"Architecture Overview | ClickHouse Docs\",\n    \"url\": \"https://clickhouse.com/docs/academic_overview\",\n    \"snippet\": \"](https://clickhouse.com/integrations)\\n* Use cases\\n  \\n    + [Real-time analytics](https://clickhouse.com/use-cases/real-time-analytics)\\n    + [Machine Learning & Generative AI](https://clickhouse.com/use-cases/machine-learning-and-data-science)\\n    + [Business Intelligence](https://clickhouse.com/use-cases/data-warehousing)\\n    + [Logs, Events, Traces](https://clickhouse.com/use-cases/observability)\\n    + [All use cases](https://clickhouse.com/use-cases)\\n  \\n  [All use cases](https://clickhouse.com/use-cases)\\n* [Documentation](https://clickhouse.com/docs)\\n* Resources\\n  \\n    + [User stories](https://clickhouse.com/user-stories)\\n    + [Blog](https://clickhouse.com/blog)\\n    + [Events](https://clickhouse.com/company/events)\\n    + [Learning and certification](https://clickhouse.com/learn)\\n    + Comparison\\n        \\n        - [BigQuery](https://clickhouse.com/comparison/bigquery)\\n        - [PostgreSQL](https://clickhouse.com/comparison/postgresql)\\n        -\\n ... \\nanalyzer](/docs/guides/developer/understanding-query-execution-with-the-analyzer)\\n    + [Using JOINs in ClickHouse](/docs/guides/joining-tables)\\n    + [Stored procedures & query parameters](/docs/guides/developer/stored-procedures-and-prepared-statements)\\n* [Performance and optimizations](/docs/operations/overview)\\n  \\n    + [Query optimization](/docs/optimize/query-optimization)\\n    + [Primary indexes](/docs/guides/best-practices/sparse-primary-indexes)\\n    + [Query parallelism](/docs/optimize/query-parallelism)\\n    + [Partitioning key](/docs/optimize/partitioning-key)\\n    + [Data skipping indexes](/docs/optimize/skipping-indexes)\\n    + [PREWHERE optimization](/docs/optimize/prewhere)\\n    + [Bulk inserts](/docs/optimize/bulk-inserts)\\n    + [Asynchronous Inserts](/docs/optimize/asynchronous-inserts)\\n    + [Avoid mutations](/docs/optimize/avoid-mutations)\\n    + [Avoid nullable Columns](/docs/optimize/avoid-nullable-columns)\\n    + [Avoid optimize\\n ... \\nRecurring queries additionally provide an opportunity to adapt the physical database layout to the workload. As a result, databases should offer pruning techniques that allow optimizing frequent queries. Depending on the query priority, databases must further grant equal or prioritized access to shared system resources such as CPU, memory, disk and network I/O, even if a large number of queries run simultaneously. 3. **Diverse landscapes of data stores, storage locations, and formats** . To integrate with existing data architectures, modern analytical databases should exhibit a high degree of openness to read and write external data in any system, location, or format. 4. **A convenient query language with support for performance introspection** . Real-world usage of OLAP databases poses additional \\\"soft\\\" requirements.\\nFor example, instead of a niche programming language, users often prefer to interface with databases in an expressive SQL dialect with nested data types and a broad range of regular, aggregation, and window functions. Analytical databases should also provide sophisticated tooling to introspect the performance of the system or individual queries. 5. **Industry-grade robustness and versatile deployment** . As commodity hardware is unreliable, databases must provide data replication for robustness against node failures. Also, databases should run on any hardware, from old laptops to powerful servers. Finally, to avoid the overhead of garbage collection in JVM-based programs and enable bare-metal performance (e.g. SIMD), databases are ideally deployed as native binaries for the target platform. Figure 1: ClickHouse timeline. ## 2 ARCHITECTURE [â€‹]( \\\"Direct link to 2 ARCHITECTURE\\\")\\n\\nFigure 2: The high-level architecture of the ClickHouse database engine.\\nAs shown by [Figure 2,]() the ClickHouse engine is split into three main layers: the query processing layer (described in Section [4)]() , the storage layer (Section [3)]() , and the integration layer (Section [5)]() . Besides these, an access layer manages user sessions and communication with applications via different protocols. There are orthogonal components for threading, caching, role-based access control, backups, and continuous monitoring. ClickHouse is built in C++ as a single, statically-linked binary without dependencies. Query processing follows the traditional paradigm of parsing incoming queries, building and optimizing logical and physical query plans, and execution. ClickHouse uses a vectorized execution model similar to MonetDB/X100 [[11]]() , in combination with opportunistic code compilation [[53]]() . Queries can be written in a feature-rich SQL dialect, PRQL [[76]]() , or Kusto's KQL [[50]]() .\\nThe storage layer consists of different table engines that encapsulate the format and location of table data. Table engines fall into three categories: The first category is the [MergeTree](/docs/concepts/glossary) A MergeTree in ClickHouse is a table engine designed for high data ingest rates and large data volumes. It is the core storage engine in ClickHouse, providing features such as columnar storage, custom partitioning, sparse primary indexes, and support for background data merges. \\\\* family of table engines which represent the primary persistence format in ClickHouse. Based on the idea of LSM trees [[60]]() , tables are split into horizontal, sorted [parts](/docs/concepts/glossary) A physical file (or directory) on disk that stores a portion of the table's data. This is different from a partition, which is a logical division of a table's data that is created using a partition key. , which are continuously merged by a background process.\\n ... \\nAs an optimization, the columns of a small part (smaller than 10 MB by default) are stored consecutively in a single file to increase the spatial locality for reads and writes. The rows of a part are further logically divided into groups of 8192 records, called granules. A [granule](/docs/concepts/glossary) A granule is a batch of rows in an uncompressed block. When reading data, ClickHouse accesses granules, but not individual rows, which enables faster data processing in analytical workloads. A granule contains 8192 rows by default. The primary index contains one entry per granule. represents the smallest indivisible data unit processed by the scan and index lookup operators in ClickHouse. Reads and writes of on-disk data are, however, not performed at the [granule](/docs/concepts/glossary) A granule is a batch of rows in an uncompressed block. When reading data, ClickHouse accesses granules, but not individual rows, which enables faster data processing in analytical workloads.\\n ... \\nEach block contains columnar data which is processed together to enhance performance during query execution. By processing data in blocks, ClickHouse utilizes CPU cores efficiently by minimizing cache misses and facilitating vectorized execution. ClickHouse uses various compression algorithms, such as LZ4, ZSTD, and Delta, to compress data in blocks. values. These indexes are best used with data with a small local cardinality, i.e. \\\"clumped together\\\" values. 3. Bloom filter indices [[9]]() build for row, token, or n-gram values with a configurable false positive rate. These indices support text search [[73]]() , but unlike min-max and set indices, they cannot be used for range or negative predicates. ### 3\\\\.3 Merge-time Data Transformation [â€‹]( \\\"Direct link to 3-3-merge-time-data-transformation\\\")\\n\\nBusiness intelligence and observability use cases often need to handle data generated at constantly high rates or in bursts.\\n ... \\n### 4\\\\.3 Multi-Node Parallelization [â€‹]( \\\"Direct link to 4.3 Multi-Node Parallelization\\\")\\n\\nIf the source table of a query is sharded, the query optimizer on the node that received the query (initiator node) tries to perform as much work as possible on other nodes. Results from other nodes can be integrated into different points of the query plan. Depending on the query, remote nodes may either 1. stream raw source table columns to the initiator node, 2. filter the source columns and send the surviving rows, 3. execute filter and aggregation steps and send local result groups with partial aggregation states, or 4. run the entire query including filters, aggregation, and sorting. Node 2 ... N in [Figure 8]() show plan fragments executed on other nodes holding shards of the hits table. These nodes filter and group the local data and send the result to the initiator node. The GroupStateMerge operator on node 1 merges the local and remote results before the results groups are finally sorted.\\n ... \\nMore recently, analytical execution kernels (e.g. Photon [[5]]() and Velox [[62]]() ) offer co-modified data processing for use in different analytical, streaming, and machine learning applications. The most similar databases to ClickHouse, in terms of goals and design principles, are Druid [[78]]() and Pinot [[34]]() . Both systems target real-time analytics with high data ingestion rates. Like ClickHouse, tables are split into horizontal [parts](/docs/concepts/glossary) A physical file (or directory) on disk that stores a portion of the table's data. This is different from a partition, which is a logical division of a table's data that is created using a partition key. called segments. While ClickHouse continuously merges smaller [parts](/docs/concepts/glossary) A physical file (or directory) on disk that stores a portion of the table's data. This is different from a partition, which is a logical division of a table's data that is created using a partition key.\\n ... \\nThe query processing layer optimizes queries using a wealth of techniques, and parallelizes execution across all server and [cluster](/docs/concepts/glossary) A collection of nodes (servers) that work together to store and process data. resources. Integration table engines and functions provide a convenient way to interact with other data management systems and data formats seamlessly. Through benchmarks, we demonstrate that ClickHouse is amongst the fastest analytical databases on the market, and we showed significant improvements in the performance of typical queries in real-world deployments of ClickHouse throughout the years. All features and enhancements planned for 2024 can be found on the public roadmap [[18]]() . Planned improvements include support for user transactions, PromQL [[69]]() as an alternative query language, a new datatype for semi-structured data (e.g.\\n ... \\nIn Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data (Athens, Greece) (SIGMOD '11). Association for Computing Machinery, New York, NY, USA, 37â€“48. <https://doi.org/10.1145/1989323.1989328>\\n* [8] Daniel Gomez Blanco. 2023. Practical OpenTelemetry. Springer Nature. * [9] Burton H. Bloom. 1970. Space/Time Trade-Ofs in Hash Coding with Allowable Errors. Commun. ACM 13, 7 (1970), 422â€“426. [https://doi.org/10.1145/362686. ](https://doi.org/10.1145/362686.362692) [362692](https://doi.org/10.1145/362686.362692)\\n* [10] Peter Boncz, Thomas Neumann, and Orri Erling. 2014. TPC-H Analyzed: Hidden Messages and Lessons Learned from an Infuential Benchmark. In Performance Characterization and Benchmarking. 61â€“76. [https://doi.org/10.1007/978-3-319-](https://doi.org/10.1007/978-3-319-04936-6_5) [04936-6\\\\_5](https://doi.org/10.1007/978-3-319-04936-6_5)\\n* [11] Peter Boncz, Marcin Zukowski, and Niels Nes. 2005. MonetDB/X100: Hyper-Pipelining Query Execution. In CIDR.\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"clickhouse.com\",\n    \"source_title\": \"Architecture Overview | ClickHouse Docs\",\n    \"source_link\": \"https://clickhouse.com/docs/academic_overview\"\n  }\n]",
              "tokenUsage": null,
              "duration": 2548,
              "cost": null,
              "responseFormat": "search-card",
              "chainMetadata": {
                "input": "first deep search for this, and then create a general assisstant report and then summarize the output and then create gta style image based on the summary \"how to build stream analytics using clickhouse database and best ways to increase pipeline performance",
                "executedAt": "2025-12-28T06:00:38.297Z",
                "output": "[\n  {\n    \"title\": \"Real-time Analytics with ClickHouse\",\n    \"url\": \"https://clickhouse.com/use-cases/real-time-analytics\",\n    \"snippet\": \"* [Use cases](/use-cases)\\n* /\\n* Real-time analytics\\n\\n# Real-time analytics with ClickHouse\\n\\nIngest millions of rows per second. Handle the most heavily concurrent workloads. All without compromising query speed. Create user-facing dashboards and applications with instant responsiveness\\n\\nDrive well-informed business decisions with the most up-to-date data\\n\\nRun SQL-based analytics effortlessly on your most diverse, high-cardinality datasets\\n\\nSeamlessly integrate static or streaming data from a wide range of sources\\n\\nUse your favorite visualization tools like Grafana, Tableau, Superset, and more\\n\\n[Get started today](https://console.clickhouse.cloud/signUp?loc=use-case-real-time-analytics) [Contact sales](/company/contact?loc=use-case-real-time-analytics)\\n\\n## The real-time database that truly shines at scale. Count on blazing performance when low latency really matters. Discover why companies are choosing ClickHouse for their real-time analytics applications.\\n01\\n\\nBest-in-class query performance that your mission-critical and time-sensitive applications can depend on. 02\\n\\nBuilt for continuous ingest with native integrations and managed pipelines for your favorite streaming sources. 03\\n\\nHigh query concurrency so you can build interactive apps your users love without worrying about responsiveness at scale. 04\\n\\nUse the tools you love through our extensive ecosystem of integrations, including popular products like Tableau, Metabase, DBT, and many more. > \\\"This year we actually exceeded a thousand active replicas. That's **processing hundreds of millions of inserted rows every second** , which actually corresponds to quite a significantly larger number of events because we've been using a lot of sampling.\\\" > \\n>\\n\\n[> \\\"We have multiple clusters deployed on our hardware with hundreds of hosts. Our main cluster is now geo-replicated, and we designate some replicas for read-heavy operations and others for write-heavy operations.\\nMany of our tables are quite large, **with trillions and trillions of rows, as well as tens of columns. ** \\\" > >](/blog/how-clickhouse-powers-ahrefs-the-worlds-most-active-web-crawler)\\n\\n[> \\\"Weâ€™ve had a positive experience with ClickHouse. It allowed us to scale LangSmith to production workloads and provide a service where users can log all of their data. We couldnâ€™t have accomplished this without ClickHouse.\\\" > >](/blog/langchain-why-we-choose-clickhouse-to-power-langchain)\\n\\n[> \\\"In the post-evaluation of each database against our criteria (with metrics ranging from query performance to cost), **ClickHouse emerged as the unrivaled frontrunner. ** It excelled across the board, even astonishingly so in certain domains, and proved more cost-efficient.\\\"\\n ... \\n[](/user-stories?loc=gitlab) [](/user-stories?loc=mux) [](/user-stories?loc=netapp) [](/user-stories?loc=servicenow) [](/user-stories?loc=posthog) [](/blog/weights-and-biases-scale-ai-development?loc=carousel) [](/user-stories?loc=deutsche-bank&search=deutsche) [](/blog/behind-the-scenes-how-clickhouse-helps-vimeo-power-video-analytics-at-scale?loc=carousel) [](/videos/funnel-analytics-and-distributed-queries-in-clickhouse) [](/blog/unlocking-the-power-of-onchain-analytics-how-nansen-transformed-their-data-infrastructure-with-clickhouse-cloud) [](/blog/poolside-using-clickhouse-to-build-next-gen-ai-for-software-development) [](/user-stories?loc=sony&search=sony) [](/blog/hifis-migration-from-bigquery-to-clickhouse?loc=carousel) [](/user-stories?loc=vercel) [](/videos/ramp?loc=carousel) [](/blog/transforming-ad-tech-how-cognitiv-uses-clickhouse-to-build-better-machine-learning-models?loc=carousel) [](/user-stories?loc=statsig) [](/user-stories?loc=didi&search=didi)\\n[](/blog/prefect-event-driven-workflow-orchestration-powered-by-clickhouse?loc=prefect) [](/user-stories?loc=cursor) [](/blog/how-trip.com-migrated-from-elasticsearch-and-built-a-50pb-logging-solution-with-clickhouse) [](/videos/sierraai?loc=sierra) [](/blog/seeing-the-big-picture-shopees-journey-to-distributed-tracing-with-clickhouse?loc=carousel) [](/blog/clickhouse-replaces-postgres-to-power-real-time-analytics-in-common-room-customer-portal?loc=carousel) [](/blog/tekion-adopts-clickhouse-cloud-to-power-application-performance-and-metrics-monitoring?loc=carousel) [](/user-stories?loc=whatnot) [](/videos/scaling-graphite-with-clickhouse?loc=carousel) [](/blog/why-astronomer-chose-clickhouse-to-power-its-new-data-observability-platform-astro-observe) [](/blog/corsearch-replaces-mysql-with-clickhouse-for-content-and-brand-protection?loc=carousel) [](/blog/upollos-journey-from-bigquery-to-clickhouse?loc=carousel) [](/blog/amp-clickhouse-oss-to-clickhouse-cloud?loc=carousel)\\n[](/blog/electrum-driving-net-zero-carbon-future-with-clickhouse) [](/user-stories?loc=spoton) [](/blog/how-memorial-sloan-kettering-cancer-center-is-using-clickhouse-to-accelerate-cancer-research) [](/blog/migrating-to-clickhouse-cloud-in-just-three-months-with-no-downtime-a-120-terabyte-data-journey-and-its-benefits) [](/videos/vantage) [](/blog/how-solarwinds-uses-clickhouse-for-realtime-observability-at-scale) [](/blog/longbridge-technology-simplifies-their-architecture-and-achieves-10x-performance-boost-with-clickhouse) [](/blog/how-harvey-uses-clickhouse-for-proactive-threat-detection) [](/user-stories?loc=royal-bank&search=royal+bank) [](/blog/observing-in-style-how-poizon-rebuilt-its-data-platform-with-clickhouse-enterprise-edition) [](/blog/how-real-estate-analytics-made-its-data-pipeline-50x-faster-with-clickhouse) [](/blog/how-anthropic-is-using-clickhouse-to-scale-observability-for-ai-era)\\n ... \\n[](/user-stories?loc=servicenow) [](/user-stories?loc=posthog) [](/blog/weights-and-biases-scale-ai-development?loc=carousel) [](/user-stories?loc=deutsche-bank&search=deutsche) [](/blog/behind-the-scenes-how-clickhouse-helps-vimeo-power-video-analytics-at-scale?loc=carousel) [](/videos/funnel-analytics-and-distributed-queries-in-clickhouse) [](/blog/unlocking-the-power-of-onchain-analytics-how-nansen-transformed-their-data-infrastructure-with-clickhouse-cloud) [](/blog/poolside-using-clickhouse-to-build-next-gen-ai-for-software-development) [](/user-stories?loc=sony&search=sony) [](/blog/hifis-migration-from-bigquery-to-clickhouse?loc=carousel) [](/user-stories?loc=vercel) [](/videos/ramp?loc=carousel) [](/blog/transforming-ad-tech-how-cognitiv-uses-clickhouse-to-build-better-machine-learning-models?loc=carousel) [](/user-stories?loc=statsig) [](/user-stories?loc=didi&search=didi) [](/blog/prefect-event-driven-workflow-orchestration-powered-by-clickhouse?loc=prefect)\\n ... \\n[](/blog/how-memorial-sloan-kettering-cancer-center-is-using-clickhouse-to-accelerate-cancer-research) [](/blog/migrating-to-clickhouse-cloud-in-just-three-months-with-no-downtime-a-120-terabyte-data-journey-and-its-benefits) [](/videos/vantage) [](/blog/how-solarwinds-uses-clickhouse-for-realtime-observability-at-scale) [](/blog/longbridge-technology-simplifies-their-architecture-and-achieves-10x-performance-boost-with-clickhouse) [](/blog/how-harvey-uses-clickhouse-for-proactive-threat-detection) [](/user-stories?loc=royal-bank&search=royal+bank) [](/blog/observing-in-style-how-poizon-rebuilt-its-data-platform-with-clickhouse-enterprise-edition) [](/blog/how-real-estate-analytics-made-its-data-pipeline-50x-faster-with-clickhouse) [](/blog/how-anthropic-is-using-clickhouse-to-scale-observability-for-ai-era) [](/blog/scaling-observabilty-for-thousands-of-gpus-at-character-ai)\\n\\n## Real-time applications and dashboards\\n\\nClickHouse is used across industries to power systems and\\napplications where real-time analysis, evaluation, and querying are critical. User-facing dashboards and apps\\n\\nE-commerce optimization\\n\\nRetail analytics\\n\\nSupply chain optimization\\n\\nFraud detection\\n\\nThreat prevention\\n\\nProactive maintenance\\n\\nIntelligent automation\\n\\nUser behavior analytics\\n\\nTrend evaluation and monitoring\\n\\nGaming analytics\\n\\nAdvertising and marketing analysis\\n\\n## ClickHouse for real-time analytics\\n\\nWhether youâ€™re building user-facing dashboards, instantly responsive applications, or analyzing data on the fly, ClickHouse has you covered. Weâ€™re built to ensure that even the most sophisticated data analysis can be done intuitively, using simple SQL. Our parallelized query execution engine, best-in-class compression rates, and column-oriented design deliver unparalleled performance at scale so that you can focus on insights and forget worrying about infrastructure.\\n### Data ingestion\\n\\n### Data transformations and queries\\n\\n### Applications, dashboards, and more\\n\\n### Unlock real-time insights and user experiences\\n\\nWhether youâ€™re performing live analysis to drive business outcomes or building interactive user-facing applications, ClickHouse delivers the unparalleled speed-to-insight you can depend on. ### Simplify your SQL\\n\\nClickHouse supports an extensive library of domain-specific functions that transform even the most complex queries into simple SQL statements. With ClickHouse, real-time data exploration is easy and powerful. ### Manage data efficiently\\n\\nWith highly optimized compression techniques and our fully parallelized query pipeline, ClickHouse maximizes CPU efficiency and provides the power to quickly process huge volumes of compressed data. ### Scale effortlessly\\n\\nClickHouse scales both vertically and horizontally to meet the demands of any workload - read and write - to systems with hundreds of cores and petabytes of storage.\\nClickHouse provides the scalability you need to effortlessly handle increasing data volumes and workloads over time. ## Supporting references\\n\\nFor detailed guides about how to get started with ClickHouse for real-time analytics workloads, follow along in our blog:\\n\\n1. [Asynchronous Data Inserts in ClickHouse](/blog/asynchronous-data-inserts-in-clickhouse)\\n2. [Real-time event streaming with ClickHouse, Confluent Cloud and ClickPipes](/blog/real-time-event-streaming-with-confluent-cloud-clickhouse-and-clickpipes)\\n3. [Adding Real-Time Analytics to a Supabase Application With ClickHouse](/blog/building-real-time-applications-with-clickhouse-and-hex-notebook-keeper-engine)\\n4. [Using Materialized Views in ClickHouse](/blog/using-materialized-views-in-clickhouse)\\n5. [Working with Time Series Data in ClickHouse](/blog/working-with-time-series-data-and-functions-ClickHouse)\\n6.\\n[Change Data Capture (CDC) with PostgreSQL and ClickHouse - Part 1](/blog/clickhouse-postgresql-change-data-capture-cdc-part-1)\\n7. [Change Data Capture (CDC) with PostgreSQL and ClickHouse - Part 2](/blog/clickhouse-postgresql-change-data-capture-cdc-part-2)\\n8. [How Cloudflare Processes Hundreds of Millions of Rows per Second with ClickHouse](/blog/how-cloudflare-processes-hundreds-of-millions-of-rows-per-second-with-clickhouse)\\n\\n## Get started with ClickHouse Cloud for free\\n\\nWe'll get you started on a 30 day trial and $300 credits to spend at your own pace. [Create a free account](https://console.clickhouse.cloud/signUp?loc=real-time-use-case-getstarted-footer)\\n\\nStay informed on feature releases, product roadmap, support, and cloud offerings! Loading form...\\n\\n[Star us on Github](https://github.com/ClickHouse/ClickHouse)\\n\\nÂ© 2025 ClickHouse, Inc. HQ in the Bay Area, CA and Amsterdam, NL.\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"clickhouse.com\",\n    \"source_title\": \"Real-time Analytics with ClickHouse\",\n    \"source_link\": \"https://clickhouse.com/use-cases/real-time-analytics\"\n  },\n  {\n    \"title\": \"How to optimize ClickHouseÂ® for high-throughput streaming\",\n    \"url\": \"https://www.tinybird.co/blog/clickhouse-streaming-analytics\",\n    \"snippet\": \"[Back to Blog](/blog)\\n\\nShare this article:\\n\\n[](http://www.facebook.com/share.php?u=https://www.tinybird.co/blog/clickhouse-streaming-analytics) [](https://x.com/intent/tweet?original_referer=https://www.tinybird.co/blog/clickhouse-streaming-analytics&utm_source=x&url=https://www.tinybird.co/blog/clickhouse-streaming-analytics&utm_medium=social&utm_source=x&source=tweetbutton&text=How%20to%20optimize%20ClickHouse%C2%AE%20for%20high-throughput%20streaming) [](http://www.linkedin.com/shareArticle?mini=true&url=https://www.tinybird.co/blog/clickhouse-streaming-analytics&utm_medium=social&utm_source=linkedin) [](https://news.ycombinator.com/submitlink?u=https://www.tinybird.co/blog/clickhouse-streaming-analytics&t=How%20to%20optimize%20ClickHouse%C2%AE%20for%20high-throughput%20streaming)\\n\\nTinybird is not affiliated with, associated with, or sponsored by ClickHouse, Inc. ClickHouseÂ® is a registered trademark of ClickHouse, Inc.\\n\\n[Back](/blog)\\n\\nOct 17, 2025\\n\\n# How to optimize ClickHouse\\n<sup>Â®</sup> for high-throughput streaming\\n\\nLearn how to optimize ClickHouse <sup>Â®</sup> for million-row-per-second streaming analytics with expert tips on table design, Kafka ingestion, and query performance tuning. [AI Resources](/blog/categories/ai-resources)\\n\\n[Cameron Archer Tech Writer](/blog/authors/cameron-archer)\\n\\nStreaming analytics systems process millions of events per second, but most databases struggle to ingest data fast enough while still returning query results in milliseconds. ClickHouse <sup>Â®</sup> is built differently: its columnar storage, vectorized execution, and merge tree architecture can handle both high-velocity writes and sub-second analytical queries on billions of rows. This guide covers how to optimize ClickHouse <sup>Â®</sup> for high-throughput streaming workloads, from schema design and ingestion tuning to query optimization and infrastructure decisions.\\nYou'll learn the specific settings, patterns, and tradeoffs that determine whether your system handles thousands or millions of events per second. Unlike complex stream processors like Apache Flink, which [most teams don't actually need](https://www.tinybird.co/blog-posts/flink-is-95-problem) , ClickHouse <sup>Â®</sup> provides a simpler approach using familiar SQL. ## Why ClickHouse <sup>Â®</sup> excels at high-throughput streaming analytics\\n\\nClickHouse <sup>Â®</sup> is a columnar database designed for analytical processing, which makes it particularly well-suited for streaming analytics workloads. The database stores data by column rather than by row, which means queries that aggregate or filter specific fields can skip reading unnecessary data entirely. This columnar approach delivers compression ratios of [10x or more](https://clickhouse.com/engineering-resources/database-compression) compared to row-oriented databases.\\nWhen you're ingesting millions of events per second, that compression directly reduces both storage costs and the amount of data that queries need to scan. ClickHouse <sup>Â®</sup> uses [vectorized query execution](https://clickhouse.com/docs/en/introduction/performance.html) , processing data in batches rather than one row at a time. Modern CPUs can apply the same operation to multiple values simultaneously through SIMD instructions, which is why [ClickHouse <sup>Â®</sup> queries](https://www.tinybird.co/blog-posts/clickhouse-query-examples) can return results in under 100 milliseconds even when scanning billions of rows. The database scales both vertically and horizontally. You can add more CPU cores and memory to a single server, or you can distribute data across multiple shards with replication for fault tolerance.\\n## Core architecture for real-time ingestion and queries\\n\\nA typical [streaming analytics setup](https://www.tinybird.co/blog-posts/real-time-streaming-data-architectures-that-scale) with ClickHouse <sup>Â®</sup> involves four main components that work together. Each component handles a specific part of the data flow, from ingestion to serving queries. ### 1\\\\. Streaming sources to buffer tables\\n\\nData arrives from sources like Apache Kafka, webhooks, or HTTP APIs. ClickHouse <sup>Â®</sup> can consume directly from Kafka using the Kafka table engine, which creates a consumer that reads messages and writes them to a target table. Buffer tables accumulate incoming data in memory before flushing to disk. This reduces the number of small data parts created during high-velocity ingestion, which otherwise slows down background merges and queries. You can also batch inserts on the client side, which gives you more control over batch sizes and error handling.\\n ... \\nEach materialized view writes to its own MergeTree table that you can query independently. The tradeoff is that backfilling becomes more complex. If you need to recalculate historical aggregates, you'll need to rebuild the materialized view or manually insert corrected data. ### 3\\\\. Denormalised analytics tables\\n\\nClickHouse <sup>Â®</sup> performs best when related data lives in the same table rather than split across multiple tables with joins. This means duplicating user attributes into each event row instead of storing them separately. For example, instead of joining an events table with a users table, you might embed user\\\\_name, user\\\\_tier, and user\\\\_region directly into each event. This increases storage slightly but eliminates expensive join operations. The tradeoff is write amplification. When a user attribute changes, you may need to update many rows. For streaming analytics where reads vastly outnumber writes, this tradeoff usually makes sense. ### 4\\\\.\\n ... \\nInstead, it creates granules (blocks of rows) and stores min/max values for each granule, letting queries skip entire granules that don't match filters. ### 2\\\\. Partition by time and cardinality\\n\\nPartitioning splits a table into separate directories based on a partition key, typically a time-based column like date or hour. This makes dropping old data fast and can speed up queries that filter by the partition key. Too many partitions hurt performance though. Each partition creates separate files on disk, and ClickHouse <sup>Â®</sup> tracks metadata for all of them. A good guideline is keeping partitions under a few thousand. For most streaming workloads, daily or hourly partitions work well. Late-arriving data can still be inserted after a partition is finalized, but it may trigger additional merges. If late arrivals are common, consider using longer partition intervals. ### 3\\\\.\\nApply codecs and compression wisely\\n\\nClickHouse <sup>Â®</sup> supports several compression codecs that trade CPU time for storage savings. LZ4 is the default and balances speed with compression. ZSTD provides better compression ratios but uses more CPU during both compression and decompression. Per-column codecs can optimize specific data types:\\n\\n* **Delta and DoubleDelta:** Work well for timestamps and monotonically increasing values\\n* **Gorilla:** Designed for floating-point time series data\\n* **LZ4 vs ZSTD:** LZ4 for hot data queried frequently, ZSTD for cold data rarely accessed\\n\\nHigher compression uses more CPU during writes and reads. For data queried often, LZ4 minimizes query latency. For data rarely accessed, ZSTD significantly reduces storage costs. ### 4\\\\. Avoid costly mutations and TTL clashes\\n\\n`ALTER TABLE` operations that modify or delete existing data (mutations) are expensive because they rewrite entire data parts.\\nFrequent mutations slow ingestion by competing for disk I/O and CPU with background merges. For workloads that need updates or deletes, ReplacingMergeTree with a version column offers an alternative. This engine automatically deduplicates rows with the same `ORDER BY` key during merges, keeping only the row with the highest version number. TTL (Time To Live) rules automatically delete old data based on a time column. However, TTL operations can trigger large numbers of merges if not configured carefully. Align TTL intervals with partition boundaries to minimize merge overhead. ## Optimizing merge tree settings for streaming workloads\\n\\nClickHouse <sup>Â®</sup> continuously merges small data parts into larger ones in the background. Tuning these merge settings significantly impacts both ingestion throughput and query performance. ### Adjusting `max_partitions_to_read`\\n\\nThe `max_partitions_to_read` setting limits how many partitions a single query can scan.\\n ... \\n## Minimising query latency on hot data\\n\\nEven with optimized ingestion, queries can be slow if they scan too much data or don't use ClickHouse <sup>Â®</sup> 's indexing features. For visualizing performance metrics, you can [connect Grafana to ClickHouse <sup>Â®</sup>](https://www.tinybird.co/blog-posts/clickhouse-grafana-example) to monitor query latency and throughput in real-time. ### Bloom filters and skip indexes\\n\\nSkip indexes (also called data skipping indexes) let ClickHouse <sup>Â®</sup> skip entire granules of data that don't match a query's `WHERE` clause. Bloom filters ( `tokenbf_v1` ) are particularly useful for high-cardinality string columns like user IDs or session IDs. To add a Bloom filter index, use `ALTER TABLE ADD INDEX` with the `tokenbf_v1` type. The index is built asynchronously during background merges. Choose indexes based on your most common query patterns. If you frequently filter by `user_id` , add a Bloom filter on that column.\\n ... \\n### Balancing `max_threads` and concurrency\\n\\nThe `max_threads` setting controls how many CPU cores ClickHouse <sup>Â®</sup> uses for a single query. More threads can speed up large queries but increase CPU usage and can starve other queries of resources. For streaming analytics workloads with many concurrent queries, you may want to limit `max_threads` to prevent any single query from monopolizing the CPU. A value of 4-8 threads per query is often a good starting point. The `max_concurrent_queries` setting limits the total number of queries that can run simultaneously. This prevents overload during traffic spikes and ensures each query gets a reasonable share of CPU and memory. ## Next steps to ship streaming analytics features with Tinybird\\n\\n[Tinybird](https://tinybird.co) is a [managed ClickHouse <sup>Â®</sup> platform](https://www.tinybird.co/product/managed-clickhouse) designed for developers who want to integrate real-time analytics into applications without managing infrastructure.\\nThe platform handles ingestion, storage, query optimization, and API creation. You can get started with Tinybird by creating a free account at <https://cloud.tinybird.co/signup> . The free tier includes generous limits for development and testing, with no credit card required. Tinybird provides rapid ingestion from Kafka, webhooks, and other streaming sources, along with real-time SQL pipes that transform and aggregate data as it arrives. You can deploy parameterized API endpoints with built-in authentication, rate limiting, and row-level security. The platform includes a CLI for local development and CI/CD integration, allowing you to define data pipelines as code, test them locally, and deploy to production with a single command. ## FAQs about streaming analytics in ClickHouse <sup>Â®</sup>\\n\\n### How do I secure per-tenant analytics APIs? Row-level security (RLS) filters data based on the authenticated user or API token, ensuring each tenant only sees their own data.\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"tinybird.co\",\n    \"source_title\": \"How to optimize ClickHouseÂ® for high-throughput streaming\",\n    \"source_link\": \"https://www.tinybird.co/blog/clickhouse-streaming-analytics\"\n  },\n  {\n    \"title\": \"How we use ClickHouse as a real-time stream processing engine | Mux\",\n    \"url\": \"https://www.mux.com/blog/how-we-use-clickhouse-as-a-real-time-stream-processing-engine\",\n    \"snippet\": \"[Skip to content]()\\n\\n[Mux Logo Toggle Mux Brand Popover](/)\\n\\n[Blog Blog](/blog)\\n\\nSearch\\n\\nCopied Share Copied Share\\n\\n[Talk to us Talk to us](/sales-contact)\\n\\n[Log in Log in](https://dashboard.mux.com/login)\\n\\nPublished on December 18, 2024 (about 1 year ago)\\n\\n# How we use ClickHouse as a real-time stream processing engine\\n\\n[](/team/faith-szeto) [](/team/josh-wien)\\n\\nBy [Faith](/team/faith-szeto) and [Josh](/team/josh-wien) â€¢ 18 min read â€¢ [Engineering](/blog/category/engineering)\\n\\n* * *\\n\\nAt Mux, building video at scale comes with many interesting challenges. Weâ€™ve built systems that stream millions of minutes every hour, but the fun doesnâ€™t stop there. Every second of video that gets uploaded, watched, or deleted must be pulled from sources like CDN logs and tracked in a way that can be used by customer dashboards, billing & analytics, and our [Delivery Usage API](https://docs.mux.com/api-reference) . Thatâ€™sâ€¦ a lot of data.\\nFor a long time, we had relied on our trusty old usage pipeline to accomplish this. But as Mux scaled, the existing system became limiting in terms of product and performance. To support new features (e.g. high usage alerting), we needed to achieve fresher data, higher throughput, and faster queries in our pipeline. In this post, we will cover:\\n\\n* The limitations of our existing pipeline\\n* The changes we made to improve performance and scale\\n* Reflections and future plans\\n\\n## [How our architecture looked in the past]()\\n\\nHere is a representation of the former pipeline:\\n\\n### [**CDN log processing**]()\\n\\nWhen a view occurs, the activity is logged within one of our CDNs. Log Importer pulls CDN logs from cloud bucket stores, [performing exactly-once processing and enrichment](/blog/processing-cdn-logs-exactly-once-with-kafka-transactions) . Enriched log messages are sent to Kafka where they are further processed and aggregated by the Media Usage Processor (MUP) Flink application.\\nAggregate rows are written to Postgres, which serves read queries by downstream consumers. ### [**Internal event stream processing**]()\\n\\nIn various regions, internal CDC events are queued, transformed and then mirrored into a single-region Kafka cluster. Asset Events Flink further transforms and enriches the events, then updates Postgres with the finalized results. ### [**How we were using Flink**]()\\n\\n[Apache Flink](https://flink.apache.org/) is a popular stream processing engine which can perform stateful computations with exactly-once guarantees. Common use cases include anomaly detection and real-time analytics. Our MUP Flink application made use of [Window functions](https://flink.apache.org/2015/12/04/introducing-stream-windows-in-apache-flink/) in order to collect various aggregates like â€œseconds deliveredâ€ for a given video. For a given entity (e.g. video asset or environment) it would keep track of hourly aggregates, then finalize and write them to Postgres.\\nBefore finalizing any row, it had to delay a certain amount of time (determined by [maxOutOfOrderness](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/built_in/) , affectionately dubbed â€œMOOâ€ ðŸ®) to be reasonably sure no late records would arrive. Since we had no guarantees about ordering of CDN log records, the MOO value had to be at least an hour in order to ensure we finalized hourly windows only after all records had been processed. Asset Events Flink was simpler - its main function was to perform stateless transformations on incoming events before writing them to Postgres. Maintaining a distributed Flink deployment for this felt like overkill. ### [**Challenges with our Flink architecture**]()\\n\\nThe old system served us well for a long time, but our needs changed as we expanded our user-base and feature set.\\n* During popular live events, like the US presidential election, traffic surges could cause MUP Flink to become a throughput bottleneck unless scaled. With careful tuning and verification this could be fully automated, but at the moment scaling still involved some manual intervention. * While our user base continued to grow, we were also adding new features and breakdowns to our billing dashboard. Increasing user traffic and aggregate complexity led to very slow queries (>1s). * Operational challenges: Our Flink deployments were managed using the [Flink Kubernetes Operator](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.6/) and implemented with a different language (Java) than the rest of our stack (Go). Coupled with Flinkâ€™s steep learning curve, this meant we had a tough time debugging or deploying changes with high confidence. The technology being an outlier from our usual stack meant there was little support in terms of tooling, which made ops tedious.\\n* The performance issues with our self-hosted Postgres repeatedly required delicate sharding and vertical scaling mitigations. New features were also blocked:\\n\\n* Alerts: The ability to alert on high usage was a popular feature request among customers. However, our current aggregates would be stale by multiple hours by the time they were available for any alerting. * Better visualization and analytics: We also wanted to enhance our billing dashboards with more detail and dimensions. But the required additional load on our Postgres was a complete no-go at this point. ### [Time to rethink our approach]()\\n\\nAs growing pains and blocked features grew more pressing, we decided it was time for a rearchitecture. We needed to overcome these key issues:\\n\\n1. Poor aggregate query performance\\n2. Poor data freshness\\n3. Operational challenges\\n\\nAnd, whatever solution we came up with needed to meet the following functional requirements:\\n\\n1. Can power high usage alerting\\n2.\\nCan provide detailed usage breakdowns with hourly granularity for the past 90 days\\n3. Continue to support billing & invoicing jobs\\n\\n## [Laying out the options]()\\n\\nAfter careful research and discussion, we agreed that we had three options:\\n\\n### [**Option 1a: Have ClickHouse serve read queries**]()\\n\\nWe could keep our existing setup, but swap in [ClickHouse](https://clickhouse.com/docs/en/intro) , a database optimized for fast analytics queries, as the read-only replica for Postgres. This would require the least work, but only addressed our poor aggregate query performance, and would exacerbate operational challenges. ### [**Option 1b: Have Flink write to ClickHouse**]()\\n\\nInstead of keeping both Postgres and ClickHouse, we could replace Postgres with ClickHouse entirely. This would be pretty similar to the previous option, but without impacting data freshness issues or ops.\\n ... \\nSome organizations dedicate entire teams to operating Flink clusters. While we could cultivate team expertise, none of us were excited about working with Java. In all honesty, this really biased us against this option. **Pros**\\n\\n* Would reduce data staleness\\n* Might improve read query performance\\n* Stream processing component can remain horizontally scalable\\n\\n**Cons**\\n\\n* Operability burden\\n* Data freshness still not ideal for our use cases\\n* Aggregate queries are unlikely to be improved to the level we require\\n* Working with Javaâ€¦\\n\\n### [**Option 3: Use ClickHouse as our stream-processing engine _and_ persistent data store, replacing both Postgres and Flink**]()\\n\\nWhile we donâ€™t have a team of Flink experts, we _do_ have a team of ClickHouse experts. [ClickHouse](https://clickhouse.com/docs/en/intro) is an OLAP column-oriented database that uses a SQL-like query language. It is useful for â€œcomplex calculations over massive datasetsâ€, like real-time analytics over huge data streams.\\n ... \\nFor example, on an incoming record we had to classify its type, determine if and how much we should bill on, and extract nested objects for processing. Explaining how we achieved all of this stream processing logic in our schema would be difficult without a good understanding of ClickHouse, so we'll save the gory details for a future post. The basic gist is this: we used Kafka Table Engines to ingest from Kafka, then sent the data through a series of cascading [Materialized Views](https://clickhouse.com/docs/en/materialized-view) and [Null Tables](https://clickhouse.com/docs/en/engines/table-engines/special/null) , which allowed us to pre-aggregate and transform the data several times before dumping it into rollup tables, which were optimized for aggregate queries. ### [**Additional considerations**]()\\n\\nIn some high volume pipelines, performance can be prioritized over accuracy. The critical nature of our data prevented us from making this tradeoff.\\n ... \\nEven small amounts of lost data or changes to usage categorization will add up to substantial billing inaccuracies at our scale, and we needed to prove that the new system would be just as reliable as the old. We started by copying over all of the existing data in Postgres to ClickHouse and feeding in new data via Kafka, which was made easier thanks to ClickHouseâ€™s Postgres table engine and our aggregating schema design, but we still needed to verify that there were no dropped or duplicated usage events. Working closely with our Analytics team, we did extensive comparisons in our data warehouse between the Postgres and ClickHouse datasets. Then, going a step further to validate the data we would return to our customers via the Usage API or invoices, we also set up a query shadowing system, where any read query run against the Postgres database would be followed by sending the equivalent query to ClickHouse.\\n ... \\nBecause both of these issues only occurred at scale, we would have missed these without our integrity checks. Instead, we caught them quickly and were able to fix them well ahead of release, by which time there was little room left for doubt. When it finally came time to switch over to the new usage pipeline, the query shadowing system again made this easy and safe. Simply flipping a configuration made clients of the usage databases return the results from ClickHouse instead of Postgres, while still logging the diffs for post-release monitoring. We kept the old system running so that we could cut over back to Postgres just as easily in case an issue arose. Thanks to our careful approach to validation beforehand, however, we never had to, and the old usage pipeline was decommissioned a month later. Weâ€™ll miss you. Ensuring data integrity is an important part of any data migration, even more so when working with business-critical data.\\n ... \\n## [What this approach unlocks for our future]()\\n\\nWeâ€™re pleased with how the new architecture has opened up a whole suite of new features that had not been feasible to build on top of the old system. The improved aggregation and query performance allows us to access the data more often and in new ways, enabling us to provide more granular billing representations to internal teams and our customers. Internal teams will soon gain the ability to recognize revenue with much greater fidelity than before, saving hours of work each month, and customers will be able to dive deeper into their spend through new dashboard visualizations and APIs. On top of that, the increased freshness of the data will no doubt benefit these features, and it also unblocks billing alerting, addressing a frequently requested feature and helping customers to monitor and control their usage without having to watch or track it themselves.\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"mux.com\",\n    \"source_title\": \"How we use ClickHouse as a real-time stream processing engine | Mux\",\n    \"source_link\": \"https://www.mux.com/blog/how-we-use-clickhouse-as-a-real-time-stream-processing-engine\"\n  },\n  {\n    \"title\": \"Guide for Query optimization | ClickHouse Docs\",\n    \"url\": \"https://clickhouse.com/docs/optimize/query-optimization\",\n    \"snippet\": \"[Skip to main content]()\\n\\n[Skip to main content]()\\n\\n* [](/docs/)\\n* [Performance and optimizations](/docs/operations/overview)\\n* Query optimization\\n\\n[Edit this page](https://github.com/ClickHouse/clickhouse-docs/blob/main/docs/guides/best-practices/query-optimization.md)\\n\\n# A simple guide for query optimization\\n\\nThis section aims to illustrate through common scenarios how to use different performance and optimization techniques, such as [analyzer](/docs/operations/analyzer) , [query profiling](/docs/operations/optimizing-performance/sampling-query-profiler) or [avoid nullable Columns](/docs/optimize/avoid-nullable-columns) , in order to improve your ClickHouse query performances. ## Understand query performance [â€‹]( \\\"Direct link to Understand query performance\\\")\\n\\nThe best moment to think about performance optimization is when you're setting up your [data schema](/docs/data-modeling/schema-design) before ingesting data into ClickHouse for the first time.\\nBut let's be honest; it is difficult to predict how much your data will grow or what types of queries will be executed. If you have an existing deployment with a few queries that you want to improve, the first step is understanding how those queries perform and why some execute in a few milliseconds while others take longer. ClickHouse has a rich set of tools to help you understand how your query is getting executed and the resources consumed to perform the execution. In this section, we will look at those tools and how to use them. ## General considerations [â€‹]( \\\"Direct link to General considerations\\\")\\n\\nTo understand query performance, let's look at what happens in ClickHouse when a query is executed. The following part is deliberately simplified and takes some shortcuts; the idea here is not to drown you with details but to get you up to speed with the basic concepts. For more information you can read about [query analyzer](/docs/operations/analyzer) .\\nFrom a very high-level standpoint, when ClickHouse executes a query, the following happens:\\n\\n* **Query parsing and analysis**\\n\\nThe query is parsed and analyzed, and a generic query execution plan is created. * **Query optimization**\\n\\nThe query execution plan is optimized, unnecessary data is pruned, and a query pipeline is built from the query plan. * **Query pipeline execution**\\n\\nThe data is read and processed in parallel. This is the stage where ClickHouse actually executes the query operations such as filtering, aggregations, and sorting. * **Final processing**\\n\\nThe results are merged, sorted, and formatted into a final result before being sent to the client. In reality, many [optimizations](/docs/concepts/why-clickhouse-is-so-fast) are taking place, and we will discuss them a bit more in this guide, but for now, those main concepts give us a good understanding of what is happening behind the scenes when ClickHouse executes a query.\\nWith this high-level understanding, let's examine the tooling ClickHouse provides and how we can use it to track the metrics that affect query performance. ## Dataset [â€‹]( \\\"Direct link to Dataset\\\")\\n\\nWe'll use a real example to illustrate how we approach query performances. Let's use the NYC Taxi dataset, which contains taxi ride data in NYC. First, we start by ingesting the NYC taxi dataset with no optimization. Below is the command to create the table and insert data from an S3 bucket. Note that we infer the schema from the data voluntarily, which is not optimized.\\n```\\n-- Create table with inferred schema\\nCREATE TABLE trips_small_inferred\\nORDER BY () EMPTY\\nAS SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/clickhouse-academy/nyc_taxi_2009-2010.parquet');\\n\\n-- Insert data into table with inferred schema\\nINSERT INTO trips_small_inferred\\nSELECT *\\nFROM s3Cluster\\n('default','https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/clickhouse-academy/nyc_taxi_2009-2010.parquet');\\n```\\n\\nLet's have a look to the table schema automatically inferred from the data.\\n ... \\n[query logs](/docs/operations/system-tables/query_log) . This data is stored in the table `system.query_log` . For each executed query, ClickHouse logs statistics such as query execution time, number of rows read, and resource usage, such as CPU, memory usage, or filesystem cache hits. Therefore, the query log is a good place to start when investigating slow queries. You can easily spot the queries that take a long time to execute and display the resource usage information for each one. Let's find the top five long-running queries on our NYC taxi dataset.\\n ... \\n|Name |Elapsed |Rows processed |Peak memory |\\n| --- | --- | --- | --- |\\n|Query 1 |1\\\\.699 sec |329\\\\.04 million |440\\\\.24 MiB |\\n|Query 2 |1\\\\.419 sec |329\\\\.04 million |546\\\\.75 MiB |\\n|Query 3 |1\\\\.414 sec |329\\\\.04 million |451\\\\.53 MiB |\\n\\n\\nLet's understand a bit better what the queries achieve. * Query 1 calculates the distance distribution in rides with an average speed of over 30 miles per hour. * Query 2 finds the number and average cost of rides per week. * Query 3 calculates the average time of each trip in the dataset. None of these queries are doing very complex processing, except the first query that calculates the trip time on the fly every time the query executes. However, each of these queries takes more than one second to execute, which, in the ClickHouse world, is a very long time. We can also note the memory usage of these queries; more or less 400 Mb for each query is quite a lot of memory. Also, each query appears to read the same number of rows (i.e., 329.04 million).\\n ... \\nFinally, the result is sorted and outputted. Here, we can note that no primary keys are used, which makes sense as we didn't define any when we created the table. As a result, ClickHouse is doing a full scan of the table for the query. **Explain Pipeline**\\n\\nEXPLAIN Pipeline shows the concrete execution strategy for the query. There, you can see how ClickHouse actually executed the generic query plan we looked at previously. ```\\nEXPLAIN PIPELINE\\nWITH\\n    dateDiff('s', pickup_datetime, dropoff_datetime) AS trip_time,\\n    (trip_distance / trip_time) * 3600 AS speed_mph\\nSELECT quantiles(0.5, 0.75, 0.9, 0.99)(trip_distance)\\nFROM nyc_taxi.trips_small_inferred\\nWHERE speed_mph > 30\\n\\nQuery id: c7e11e7b-d970-4e35-936c-ecfc24e3b879\\n\\n    â”Œâ”€explainâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\\n 1. â”‚ (Expression)                                                                        â”‚\\n 2.\\n ... \\nDepending on how you ingested your data, you might have leveraged ClickHouse [capabilities](/docs/interfaces/schema-inference) to infer the table schema based on the ingested data. While this is very practical to get started, if you want to optimize your query performance, you'll need to review the data schema to best fit your use case. ### Nullable [â€‹]( \\\"Direct link to Nullable\\\")\\n\\nAs described in the [best practices documentation](/docs/best-practices/select-data-types) , avoid nullable columns wherever possible. It is tempting to use them often, as they make the data ingestion mechanism more flexible, but they negatively affect performance as an additional column has to be processed every time. Running an SQL query that counts the rows with a NULL value can easily reveal the columns in your tables that actually need a Nullable value.\\n ... \\n|Name |Run 1 - Elapsed |Elapsed |Rows processed |Peak memory |\\n| --- | --- | --- | --- | --- |\\n|Query 1 |1\\\\.699 sec |1\\\\.353 sec |329\\\\.04 million |337\\\\.12 MiB |\\n|Query 2 |1\\\\.419 sec |1\\\\.171 sec |329\\\\.04 million |531\\\\.09 MiB |\\n|Query 3 |1\\\\.414 sec |1\\\\.188 sec |329\\\\.04 million |265\\\\.05 MiB |\\n\\n\\nWe notice some improvements in both query time and memory usage. Thanks to the optimization in the data schema, we reduce the total volume of data that represents our data, leading to improved memory consumption and reduced processing time. Let's check the size of the tables to see the difference.\\n ... \\n## The importance of primary keys [â€‹]( \\\"Direct link to The importance of primary keys\\\")\\n\\nPrimary keys in ClickHouse work differently than in most traditional database systems. In those systems, primary keys enforce uniqueness and data integrity. Any attempt to insert duplicate primary key values is rejected, and a B-tree or hash-based index is usually created for fast lookup. In ClickHouse, the primary key's [objective](/docs/guides/best-practices/sparse-primary-indexes) is different; it does not enforce uniqueness or help with data integrity. Instead, it is designed to optimize query performance. The primary key defines the order in which the data is stored on disk and is implemented as a sparse index that stores pointers to the first row of each granule. > Granules in ClickHouse are the smallest units of data read during query execution. They contain up to a fixed number of rows, determined by index\\\\_granularity, with a default value of 8192 rows.\\nGranules are stored contiguously and sorted by the primary key. > \\n> \\n\\nSelecting a good set of primary keys is important for performance, and it's actually common to store the same data in different tables and use different sets of primary keys to speed up a specific set of queries. Other options supported by ClickHouse, such as Projection or Materialized view, allow you to use a different set of primary keys on the same data. The second part of this blog series will cover this in more detail. ### Choose primary keys [â€‹]( \\\"Direct link to Choose primary keys\\\")\\n\\nChoosing the correct set of primary keys is a complex topic, and it might require trade-offs and experiments to find the best combination. For now, we're going to follow these simple practices:\\n\\n* Use fields that are used to filter in most queries\\n* Choose columns with lower cardinality first\\n* Consider a time-based component in your primary key, as filtering by time on a timestamp dataset is pretty common.\\n ... \\nâ”‚                 Condition: and((pickup_datetime in (-Inf, 1238543999]), (pickup_datetime in [1230768000, +Inf))) â”‚\\n13. â”‚                 Parts: 9/9                                                                                       â”‚\\n14. â”‚                 Granules: 5061/40167                                                                             â”‚\\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\\n```\\n\\nThanks to the primary key, only a subset of the table granules has been selected. This alone greatly improves the query performance since ClickHouse has to process significantly less data. ## Next steps [â€‹]( \\\"Direct link to Next steps\\\")\\n\\nHopefully this guide gets a good understanding on how to investigate slow queries with ClickHouse and how to make them faster.\\n ... \\n[Previous Performance and optimizations overview](/docs/operations/overview) [Next Primary indexes](/docs/guides/best-practices/sparse-primary-indexes)\\n\\nÂ© 2016â€“2025 ClickHouse, Inc.\\n\\n[Trademark](https://clickhouse.com/legal/trademark-policy) Â· [Privacy](https://clickhouse.com/legal/privacy-policy) Â· [Security](https://trust.clickhouse.com/) Â· [Terms of Service](https://clickhouse.com/legal/agreements/terms-of-service)\\n\\nÂ© 2016â€“2025 ClickHouse, Inc.\\n\\n[Trademark](https://clickhouse.com/legal/trademark-policy) Â· [Privacy](https://clickhouse.com/legal/privacy-policy) Â· [Security](https://trust.clickhouse.com/) Â· [Terms of Service](https://clickhouse.com/legal/agreements/terms-of-service)\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"clickhouse.com\",\n    \"source_title\": \"Guide for Query optimization | ClickHouse Docs\",\n    \"source_link\": \"https://clickhouse.com/docs/optimize/query-optimization\"\n  },\n  {\n    \"title\": \"Architecture Overview | ClickHouse Docs\",\n    \"url\": \"https://clickhouse.com/docs/academic_overview\",\n    \"snippet\": \"](https://clickhouse.com/integrations)\\n* Use cases\\n  \\n    + [Real-time analytics](https://clickhouse.com/use-cases/real-time-analytics)\\n    + [Machine Learning & Generative AI](https://clickhouse.com/use-cases/machine-learning-and-data-science)\\n    + [Business Intelligence](https://clickhouse.com/use-cases/data-warehousing)\\n    + [Logs, Events, Traces](https://clickhouse.com/use-cases/observability)\\n    + [All use cases](https://clickhouse.com/use-cases)\\n  \\n  [All use cases](https://clickhouse.com/use-cases)\\n* [Documentation](https://clickhouse.com/docs)\\n* Resources\\n  \\n    + [User stories](https://clickhouse.com/user-stories)\\n    + [Blog](https://clickhouse.com/blog)\\n    + [Events](https://clickhouse.com/company/events)\\n    + [Learning and certification](https://clickhouse.com/learn)\\n    + Comparison\\n        \\n        - [BigQuery](https://clickhouse.com/comparison/bigquery)\\n        - [PostgreSQL](https://clickhouse.com/comparison/postgresql)\\n        -\\n ... \\nanalyzer](/docs/guides/developer/understanding-query-execution-with-the-analyzer)\\n    + [Using JOINs in ClickHouse](/docs/guides/joining-tables)\\n    + [Stored procedures & query parameters](/docs/guides/developer/stored-procedures-and-prepared-statements)\\n* [Performance and optimizations](/docs/operations/overview)\\n  \\n    + [Query optimization](/docs/optimize/query-optimization)\\n    + [Primary indexes](/docs/guides/best-practices/sparse-primary-indexes)\\n    + [Query parallelism](/docs/optimize/query-parallelism)\\n    + [Partitioning key](/docs/optimize/partitioning-key)\\n    + [Data skipping indexes](/docs/optimize/skipping-indexes)\\n    + [PREWHERE optimization](/docs/optimize/prewhere)\\n    + [Bulk inserts](/docs/optimize/bulk-inserts)\\n    + [Asynchronous Inserts](/docs/optimize/asynchronous-inserts)\\n    + [Avoid mutations](/docs/optimize/avoid-mutations)\\n    + [Avoid nullable Columns](/docs/optimize/avoid-nullable-columns)\\n    + [Avoid optimize\\n ... \\nRecurring queries additionally provide an opportunity to adapt the physical database layout to the workload. As a result, databases should offer pruning techniques that allow optimizing frequent queries. Depending on the query priority, databases must further grant equal or prioritized access to shared system resources such as CPU, memory, disk and network I/O, even if a large number of queries run simultaneously. 3. **Diverse landscapes of data stores, storage locations, and formats** . To integrate with existing data architectures, modern analytical databases should exhibit a high degree of openness to read and write external data in any system, location, or format. 4. **A convenient query language with support for performance introspection** . Real-world usage of OLAP databases poses additional \\\"soft\\\" requirements.\\nFor example, instead of a niche programming language, users often prefer to interface with databases in an expressive SQL dialect with nested data types and a broad range of regular, aggregation, and window functions. Analytical databases should also provide sophisticated tooling to introspect the performance of the system or individual queries. 5. **Industry-grade robustness and versatile deployment** . As commodity hardware is unreliable, databases must provide data replication for robustness against node failures. Also, databases should run on any hardware, from old laptops to powerful servers. Finally, to avoid the overhead of garbage collection in JVM-based programs and enable bare-metal performance (e.g. SIMD), databases are ideally deployed as native binaries for the target platform. Figure 1: ClickHouse timeline. ## 2 ARCHITECTURE [â€‹]( \\\"Direct link to 2 ARCHITECTURE\\\")\\n\\nFigure 2: The high-level architecture of the ClickHouse database engine.\\nAs shown by [Figure 2,]() the ClickHouse engine is split into three main layers: the query processing layer (described in Section [4)]() , the storage layer (Section [3)]() , and the integration layer (Section [5)]() . Besides these, an access layer manages user sessions and communication with applications via different protocols. There are orthogonal components for threading, caching, role-based access control, backups, and continuous monitoring. ClickHouse is built in C++ as a single, statically-linked binary without dependencies. Query processing follows the traditional paradigm of parsing incoming queries, building and optimizing logical and physical query plans, and execution. ClickHouse uses a vectorized execution model similar to MonetDB/X100 [[11]]() , in combination with opportunistic code compilation [[53]]() . Queries can be written in a feature-rich SQL dialect, PRQL [[76]]() , or Kusto's KQL [[50]]() .\\nThe storage layer consists of different table engines that encapsulate the format and location of table data. Table engines fall into three categories: The first category is the [MergeTree](/docs/concepts/glossary) A MergeTree in ClickHouse is a table engine designed for high data ingest rates and large data volumes. It is the core storage engine in ClickHouse, providing features such as columnar storage, custom partitioning, sparse primary indexes, and support for background data merges. \\\\* family of table engines which represent the primary persistence format in ClickHouse. Based on the idea of LSM trees [[60]]() , tables are split into horizontal, sorted [parts](/docs/concepts/glossary) A physical file (or directory) on disk that stores a portion of the table's data. This is different from a partition, which is a logical division of a table's data that is created using a partition key. , which are continuously merged by a background process.\\n ... \\nAs an optimization, the columns of a small part (smaller than 10 MB by default) are stored consecutively in a single file to increase the spatial locality for reads and writes. The rows of a part are further logically divided into groups of 8192 records, called granules. A [granule](/docs/concepts/glossary) A granule is a batch of rows in an uncompressed block. When reading data, ClickHouse accesses granules, but not individual rows, which enables faster data processing in analytical workloads. A granule contains 8192 rows by default. The primary index contains one entry per granule. represents the smallest indivisible data unit processed by the scan and index lookup operators in ClickHouse. Reads and writes of on-disk data are, however, not performed at the [granule](/docs/concepts/glossary) A granule is a batch of rows in an uncompressed block. When reading data, ClickHouse accesses granules, but not individual rows, which enables faster data processing in analytical workloads.\\n ... \\nEach block contains columnar data which is processed together to enhance performance during query execution. By processing data in blocks, ClickHouse utilizes CPU cores efficiently by minimizing cache misses and facilitating vectorized execution. ClickHouse uses various compression algorithms, such as LZ4, ZSTD, and Delta, to compress data in blocks. values. These indexes are best used with data with a small local cardinality, i.e. \\\"clumped together\\\" values. 3. Bloom filter indices [[9]]() build for row, token, or n-gram values with a configurable false positive rate. These indices support text search [[73]]() , but unlike min-max and set indices, they cannot be used for range or negative predicates. ### 3\\\\.3 Merge-time Data Transformation [â€‹]( \\\"Direct link to 3-3-merge-time-data-transformation\\\")\\n\\nBusiness intelligence and observability use cases often need to handle data generated at constantly high rates or in bursts.\\n ... \\n### 4\\\\.3 Multi-Node Parallelization [â€‹]( \\\"Direct link to 4.3 Multi-Node Parallelization\\\")\\n\\nIf the source table of a query is sharded, the query optimizer on the node that received the query (initiator node) tries to perform as much work as possible on other nodes. Results from other nodes can be integrated into different points of the query plan. Depending on the query, remote nodes may either 1. stream raw source table columns to the initiator node, 2. filter the source columns and send the surviving rows, 3. execute filter and aggregation steps and send local result groups with partial aggregation states, or 4. run the entire query including filters, aggregation, and sorting. Node 2 ... N in [Figure 8]() show plan fragments executed on other nodes holding shards of the hits table. These nodes filter and group the local data and send the result to the initiator node. The GroupStateMerge operator on node 1 merges the local and remote results before the results groups are finally sorted.\\n ... \\nMore recently, analytical execution kernels (e.g. Photon [[5]]() and Velox [[62]]() ) offer co-modified data processing for use in different analytical, streaming, and machine learning applications. The most similar databases to ClickHouse, in terms of goals and design principles, are Druid [[78]]() and Pinot [[34]]() . Both systems target real-time analytics with high data ingestion rates. Like ClickHouse, tables are split into horizontal [parts](/docs/concepts/glossary) A physical file (or directory) on disk that stores a portion of the table's data. This is different from a partition, which is a logical division of a table's data that is created using a partition key. called segments. While ClickHouse continuously merges smaller [parts](/docs/concepts/glossary) A physical file (or directory) on disk that stores a portion of the table's data. This is different from a partition, which is a logical division of a table's data that is created using a partition key.\\n ... \\nThe query processing layer optimizes queries using a wealth of techniques, and parallelizes execution across all server and [cluster](/docs/concepts/glossary) A collection of nodes (servers) that work together to store and process data. resources. Integration table engines and functions provide a convenient way to interact with other data management systems and data formats seamlessly. Through benchmarks, we demonstrate that ClickHouse is amongst the fastest analytical databases on the market, and we showed significant improvements in the performance of typical queries in real-world deployments of ClickHouse throughout the years. All features and enhancements planned for 2024 can be found on the public roadmap [[18]]() . Planned improvements include support for user transactions, PromQL [[69]]() as an alternative query language, a new datatype for semi-structured data (e.g.\\n ... \\nIn Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data (Athens, Greece) (SIGMOD '11). Association for Computing Machinery, New York, NY, USA, 37â€“48. <https://doi.org/10.1145/1989323.1989328>\\n* [8] Daniel Gomez Blanco. 2023. Practical OpenTelemetry. Springer Nature. * [9] Burton H. Bloom. 1970. Space/Time Trade-Ofs in Hash Coding with Allowable Errors. Commun. ACM 13, 7 (1970), 422â€“426. [https://doi.org/10.1145/362686. ](https://doi.org/10.1145/362686.362692) [362692](https://doi.org/10.1145/362686.362692)\\n* [10] Peter Boncz, Thomas Neumann, and Orri Erling. 2014. TPC-H Analyzed: Hidden Messages and Lessons Learned from an Infuential Benchmark. In Performance Characterization and Benchmarking. 61â€“76. [https://doi.org/10.1007/978-3-319-](https://doi.org/10.1007/978-3-319-04936-6_5) [04936-6\\\\_5](https://doi.org/10.1007/978-3-319-04936-6_5)\\n* [11] Peter Boncz, Marcin Zukowski, and Niels Nes. 2005. MonetDB/X100: Hyper-Pipelining Query Execution. In CIDR.\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"clickhouse.com\",\n    \"source_title\": \"Architecture Overview | ClickHouse Docs\",\n    \"source_link\": \"https://clickhouse.com/docs/academic_overview\"\n  }\n]"
              }
            },
            {
              "id": "01KDHRMDJTW3B2MD242VDFDMR8",
              "title": "Create a general assistant report based on search results",
              "description": "Analyze the search results obtained from the deep search and create a comprehensive report on stream analytics using ClickHouse.",
              "status": "completed",
              "agentId": "general-assistant",
              "agentType": "chat",
              "dependencies": [
                "01KDHRMDJSQH06D04CKFGS2W0M"
              ],
              "input": {
                "body": {
                  "userPrompt": {
                    "__fromDependency": true,
                    "source": "previous-output"
                  }
                },
                "extra_body": {}
              },
              "createdAt": "2025-12-28T05:59:20.922Z",
              "completedAt": "2025-12-28T06:01:00.554Z",
              "output": "# Comprehensive Business and Data Analysis of ClickHouse Real-Time Analytics\n\n## 1. Executive Summary\nThe analysis of ClickHouse real-time analytics and related resources reveals a powerful database solution designed to handle high-throughput workloads while maintaining swift query performance. ClickHouse's architecture is optimized for real-time data ingestion and analysis, making it suitable for various applications, such as e-commerce optimization, fraud detection, supply chain analytics, and more. Key findings highlight the efficiency of ClickHouse in managing billions of rows and the distinct advantages its column-oriented storage architecture offers over traditional databases. \n\nPrimary recommendations include enhancing usage of built-in query optimization techniques, leveraging partitioning and indexing functionalities for better performance, and adequately training teams on the ClickHouse ecosystem. Critical risks affecting this architecture mainly revolve around managing data latency during peak loads and the potential complexity involved in maintaining high availability without impacting performance.\n\n## 2. Data Overview\nThe data analyzed for ClickHouse includes a comprehensive set of case studies, technical documents, and user experiences related to real-time analytics solutions. Below is an overview of the datasets:\n\n- **Data Sources Analyzed**: \n  - ClickHouse official website (use cases, documentation) \n  - Blogs (external and internal) detailing optimizations and user success stories\n  - Case studies across different industries using ClickHouse.\n\n- **Data Volume and Scope**: The dataset comprises multiple articles totaling thousands of words, with each source detailing specific implementations, use cases, and optimization strategies within ClickHouse.\n\n- **Data Quality Assessment**: The quality of the data appears high, with consistent formatting and relevant content providing in-depth insights into architecture, optimization techniques, and real-world applications.\n\n- **Key Entities and Relationships Identified**: \n  - ClickHouse functionalities (data ingestion, SQL querying, user dashboards)\n  - Interrelations with integration tools (Grafana, Tableau)\n  - User case studies showcasing performance improvements in analytics\n\n## 3. Detailed Analysis\n### System Architecture\nClickHouse employs a three-layer architecture consisting of:\n- **Query Processing Layer**: Handles incoming queries, optimizes plans, and executes them using a vectorized execution model for high performance.\n- **Storage Layer**: Utilizes column-oriented storage with MergeTree engines to facilitate high throughput and massive datasets; supports various compression algorithms to optimize storage.\n- **Integration Layer**: Facilitates connections with diverse data sources and formats, enhancing the usability of ClickHouse across various ecosystems.\n\nThis architecture supports efficient data handling for analytical queries, maximizing CPU core utilization while minimizing latency through compression and batch processing.\n\n### Business Processes\nClickHouse is utilized across many verticals, driving improvements in:\n- **Data Analytics**: Enabling near-instant answers to complex queries over large datasets.\n- **Operational Efficiency**: Streamlining processes from data ingestion to transformation and querying, creating responsive dashboards for user interaction.\n- **Real-Time Insights**: Supporting functions such as fraud detection, proactive maintenance, and intelligent automation through immediate data availability.\n\nApplication users report significant improvements in processing speed, enabling timely business decisions and enhanced operational capabilities.\n\n### Data Relationships\nThe interactions between data ingestion sources (like Kafka), ClickHouse tables, and querying interfaces are robust:\n- **Ingestion Sources**: ClickHouse can directly ingest from static or streaming data sources, optimizing how data flows from initial input to output queries.\n- **Transformations**: The database allows for the application of functions during ingestion and querying to create optimized datasets for analysis.\n- **User Output**: Seamless integration with BI tools provides a link between relational data storage and user-facing applications, enhancing usability.\n\n### Usage Patterns\nUse of ClickHouse typically exhibits:\n- **High Query Concurrency**: Enabling interactive applications with minimal performance degradation.\n- **Diverse Workloads**: Applications span various industries, implying flexibility and adaptability of ClickHouse for numerous scenarios.\n- **Engagement with BI Tools**: Extensive ecosystem integration supports tools like Grafana, optimizing visualization and reporting.\n\n### Organizational Context\nClickHouse's tenant structure allows for easy scaling and efficient management of resources. Users have reported positive experiences transitioning from traditional systems, highlighting advantages in scalability and performance.\n\n## 4. Key Insights\n- ClickHouse can efficiently process millions of rows per second, allowing businesses to extract real-time insights from large datasets.\n- Its columnar storage optimized for read-heavy activities leads to substantially reduced query times compared to traditional row-oriented databases.\n- Real-time dashboards built atop ClickHouse facilitate immediate business decisions, bolstered by responsiveness and operational insights.\n- The capacity to integrate with prominent BI tools maximizes data utilization and strengthens analytical capabilities across organizations.\n\n## 5. Recommendations\n### High Priority\n1. **Implement Partitioning Strategies**:\n   - **What to do**: Introduce partitioning based on time or cardinality to optimize data retrieval and manageability.\n   - **Why it's essential**: This can significantly speed up data queries and simplify the process of archiving old data.\n   - **Expected Impact**: Faster queries, reduced maintenance overhead.\n\n2. **Optimize Indexing**:\n   - **What to do**: Utilize skip indexes and Bloom filters for frequently queried attributes.\n   - **Why it's essential**: Reduces the data scanned during query execution, leading to quicker responses.\n   - **Expected Impact**: Enhanced performance and lower latency.\n\n### Medium Priority\n1. **Regular Training for Teams**:\n   - **What to do**: Conduct training sessions on ClickHouse optimizations and best practices for all relevant team members.\n   - **Why it's essential**: Equipping teams ensures they can exploit the full capabilities of ClickHouse.\n   - **Expected Impact**: Reduced misconfigurations and improved database handling.\n\n2. **Leverage Data Compression Techniques**:\n   - **What to do**: Implement appropriate compression codecs based on data access patterns (e.g., LZ4 for frequently accessed and ZSTD for rarely accessed data).\n   - **Why it's essential**: Managing data storage cost and optimizing retrieval speeds.\n   - **Expected Impact**: Lower storage costs and better CPU utilization.\n\n### Low Priority\n1. **Explore New Use Cases**:\n   - **What to do**: Investigate new analytical applications that can leverage ClickHouse's capabilities.\n   - **Why it's essential**: Continuous exploration ensures the database adapts and evolves with the organizationâ€™s needs.\n   - **Expected Impact**: Potential market advantages and enhanced business intelligence.\n\n## 6. Risks and Concerns\n### Critical Risks\n1. **Management of Latency During Peak Loads**:\n   - **Description**: High concurrency during peak events may lead to latency or errors.\n   - **Likelihood & Severity**: High likelihood in high-traffic scenarios, potential severe business impact.\n   - **Mitigation Strategy**: Implement load balancing and optimize resource allocation during high usage times.\n\n### Moderate Risks\n1. **Data Corruption or Loss During Migration**:\n   - **Description**: Transitioning from other systems may introduce data loss.\n   - **Likelihood & Severity**: Moderate, depending on current systems and processes.\n   - **Mitigation Strategy**: Develop a comprehensive data backup and migration strategy, including validation post-migration.\n\n### Minor Concerns\n1. **Limited Documentation on Advanced Features**:\n   - **Description**: Users may struggle with accessing and leveraging advanced capabilities.\n   - **Likelihood & Severity**: Relatively low; most users will adapt over time.\n   - **Mitigation Strategy**: Promote community forums and encourage sharing of best practices within teams.\n\n## 7. Things to Consider\n- **Future Scalability**: The growing amounts of data require ongoing evaluations of architecture capacity and potential for scale.\n- **Continuous Optimization**: Implementing strategies for regular monitoring and optimizing query performance and system efficiency.\n- **Best Practices**: Stay updated on the latest developments and best practices for ClickHouse to ensure competitive advantages.\n- **Integration Opportunities**: Evaluate potential integrations with emerging data sources and analytics tools.\n\n## 8. Conclusion\nClickHouse represents a robust solution for organizations demanding real-time analytics capabilities across varying workloads. The ability to process vast datasets quickly and integrate smoothly with various tools and technologies offers significant advantages. Nevertheless, proactive management of growth, continuous optimization efforts, and team training on best practices will be vital in harnessing the full potential of ClickHouse for future organizational growth and efficiency.",
              "tokenUsage": {
                "prompt_tokens": 16581,
                "completion_tokens": 1695,
                "total_tokens": 18276,
                "pricing": {
                  "input_price_per_1m": 0.15,
                  "output_price_per_1m": 0.6,
                  "input_cost": 0.0024871499999999996,
                  "output_cost": 0.001017,
                  "total_cost": 0.0035041499999999993,
                  "model_id": "gpt-4o-mini"
                }
              },
              "duration": 22065,
              "cost": 0.0035041499999999993,
              "responseFormat": "string",
              "chainMetadata": {
                "input": "[\n  {\n    \"title\": \"Real-time Analytics with ClickHouse\",\n    \"url\": \"https://clickhouse.com/use-cases/real-time-analytics\",\n    \"snippet\": \"* [Use cases](/use-cases)\\n* /\\n* Real-time analytics\\n\\n# Real-time analytics with ClickHouse\\n\\nIngest millions of rows per second. Handle the most heavily concurrent workloads. All without compromising query speed. Create user-facing dashboards and applications with instant responsiveness\\n\\nDrive well-informed business decisions with the most up-to-date data\\n\\nRun SQL-based analytics effortlessly on your most diverse, high-cardinality datasets\\n\\nSeamlessly integrate static or streaming data from a wide range of sources\\n\\nUse your favorite visualization tools like Grafana, Tableau, Superset, and more\\n\\n[Get started today](https://console.clickhouse.cloud/signUp?loc=use-case-real-time-analytics) [Contact sales](/company/contact?loc=use-case-real-time-analytics)\\n\\n## The real-time database that truly shines at scale. Count on blazing performance when low latency really matters. Discover why companies are choosing ClickHouse for their real-time analytics applications.\\n01\\n\\nBest-in-class query performance that your mission-critical and time-sensitive applications can depend on. 02\\n\\nBuilt for continuous ingest with native integrations and managed pipelines for your favorite streaming sources. 03\\n\\nHigh query concurrency so you can build interactive apps your users love without worrying about responsiveness at scale. 04\\n\\nUse the tools you love through our extensive ecosystem of integrations, including popular products like Tableau, Metabase, DBT, and many more. > \\\"This year we actually exceeded a thousand active replicas. That's **processing hundreds of millions of inserted rows every second** , which actually corresponds to quite a significantly larger number of events because we've been using a lot of sampling.\\\" > \\n>\\n\\n[> \\\"We have multiple clusters deployed on our hardware with hundreds of hosts. Our main cluster is now geo-replicated, and we designate some replicas for read-heavy operations and others for write-heavy operations.\\nMany of our tables are quite large, **with trillions and trillions of rows, as well as tens of columns. ** \\\" > >](/blog/how-clickhouse-powers-ahrefs-the-worlds-most-active-web-crawler)\\n\\n[> \\\"Weâ€™ve had a positive experience with ClickHouse. It allowed us to scale LangSmith to production workloads and provide a service where users can log all of their data. We couldnâ€™t have accomplished this without ClickHouse.\\\" > >](/blog/langchain-why-we-choose-clickhouse-to-power-langchain)\\n\\n[> \\\"In the post-evaluation of each database against our criteria (with metrics ranging from query performance to cost), **ClickHouse emerged as the unrivaled frontrunner. ** It excelled across the board, even astonishingly so in certain domains, and proved more cost-efficient.\\\"\\n ... \\n[](/user-stories?loc=gitlab) [](/user-stories?loc=mux) [](/user-stories?loc=netapp) [](/user-stories?loc=servicenow) [](/user-stories?loc=posthog) [](/blog/weights-and-biases-scale-ai-development?loc=carousel) [](/user-stories?loc=deutsche-bank&search=deutsche) [](/blog/behind-the-scenes-how-clickhouse-helps-vimeo-power-video-analytics-at-scale?loc=carousel) [](/videos/funnel-analytics-and-distributed-queries-in-clickhouse) [](/blog/unlocking-the-power-of-onchain-analytics-how-nansen-transformed-their-data-infrastructure-with-clickhouse-cloud) [](/blog/poolside-using-clickhouse-to-build-next-gen-ai-for-software-development) [](/user-stories?loc=sony&search=sony) [](/blog/hifis-migration-from-bigquery-to-clickhouse?loc=carousel) [](/user-stories?loc=vercel) [](/videos/ramp?loc=carousel) [](/blog/transforming-ad-tech-how-cognitiv-uses-clickhouse-to-build-better-machine-learning-models?loc=carousel) [](/user-stories?loc=statsig) [](/user-stories?loc=didi&search=didi)\\n[](/blog/prefect-event-driven-workflow-orchestration-powered-by-clickhouse?loc=prefect) [](/user-stories?loc=cursor) [](/blog/how-trip.com-migrated-from-elasticsearch-and-built-a-50pb-logging-solution-with-clickhouse) [](/videos/sierraai?loc=sierra) [](/blog/seeing-the-big-picture-shopees-journey-to-distributed-tracing-with-clickhouse?loc=carousel) [](/blog/clickhouse-replaces-postgres-to-power-real-time-analytics-in-common-room-customer-portal?loc=carousel) [](/blog/tekion-adopts-clickhouse-cloud-to-power-application-performance-and-metrics-monitoring?loc=carousel) [](/user-stories?loc=whatnot) [](/videos/scaling-graphite-with-clickhouse?loc=carousel) [](/blog/why-astronomer-chose-clickhouse-to-power-its-new-data-observability-platform-astro-observe) [](/blog/corsearch-replaces-mysql-with-clickhouse-for-content-and-brand-protection?loc=carousel) [](/blog/upollos-journey-from-bigquery-to-clickhouse?loc=carousel) [](/blog/amp-clickhouse-oss-to-clickhouse-cloud?loc=carousel)\\n[](/blog/electrum-driving-net-zero-carbon-future-with-clickhouse) [](/user-stories?loc=spoton) [](/blog/how-memorial-sloan-kettering-cancer-center-is-using-clickhouse-to-accelerate-cancer-research) [](/blog/migrating-to-clickhouse-cloud-in-just-three-months-with-no-downtime-a-120-terabyte-data-journey-and-its-benefits) [](/videos/vantage) [](/blog/how-solarwinds-uses-clickhouse-for-realtime-observability-at-scale) [](/blog/longbridge-technology-simplifies-their-architecture-and-achieves-10x-performance-boost-with-clickhouse) [](/blog/how-harvey-uses-clickhouse-for-proactive-threat-detection) [](/user-stories?loc=royal-bank&search=royal+bank) [](/blog/observing-in-style-how-poizon-rebuilt-its-data-platform-with-clickhouse-enterprise-edition) [](/blog/how-real-estate-analytics-made-its-data-pipeline-50x-faster-with-clickhouse) [](/blog/how-anthropic-is-using-clickhouse-to-scale-observability-for-ai-era)\\n ... \\n[](/user-stories?loc=servicenow) [](/user-stories?loc=posthog) [](/blog/weights-and-biases-scale-ai-development?loc=carousel) [](/user-stories?loc=deutsche-bank&search=deutsche) [](/blog/behind-the-scenes-how-clickhouse-helps-vimeo-power-video-analytics-at-scale?loc=carousel) [](/videos/funnel-analytics-and-distributed-queries-in-clickhouse) [](/blog/unlocking-the-power-of-onchain-analytics-how-nansen-transformed-their-data-infrastructure-with-clickhouse-cloud) [](/blog/poolside-using-clickhouse-to-build-next-gen-ai-for-software-development) [](/user-stories?loc=sony&search=sony) [](/blog/hifis-migration-from-bigquery-to-clickhouse?loc=carousel) [](/user-stories?loc=vercel) [](/videos/ramp?loc=carousel) [](/blog/transforming-ad-tech-how-cognitiv-uses-clickhouse-to-build-better-machine-learning-models?loc=carousel) [](/user-stories?loc=statsig) [](/user-stories?loc=didi&search=didi) [](/blog/prefect-event-driven-workflow-orchestration-powered-by-clickhouse?loc=prefect)\\n ... \\n[](/blog/how-memorial-sloan-kettering-cancer-center-is-using-clickhouse-to-accelerate-cancer-research) [](/blog/migrating-to-clickhouse-cloud-in-just-three-months-with-no-downtime-a-120-terabyte-data-journey-and-its-benefits) [](/videos/vantage) [](/blog/how-solarwinds-uses-clickhouse-for-realtime-observability-at-scale) [](/blog/longbridge-technology-simplifies-their-architecture-and-achieves-10x-performance-boost-with-clickhouse) [](/blog/how-harvey-uses-clickhouse-for-proactive-threat-detection) [](/user-stories?loc=royal-bank&search=royal+bank) [](/blog/observing-in-style-how-poizon-rebuilt-its-data-platform-with-clickhouse-enterprise-edition) [](/blog/how-real-estate-analytics-made-its-data-pipeline-50x-faster-with-clickhouse) [](/blog/how-anthropic-is-using-clickhouse-to-scale-observability-for-ai-era) [](/blog/scaling-observabilty-for-thousands-of-gpus-at-character-ai)\\n\\n## Real-time applications and dashboards\\n\\nClickHouse is used across industries to power systems and\\napplications where real-time analysis, evaluation, and querying are critical. User-facing dashboards and apps\\n\\nE-commerce optimization\\n\\nRetail analytics\\n\\nSupply chain optimization\\n\\nFraud detection\\n\\nThreat prevention\\n\\nProactive maintenance\\n\\nIntelligent automation\\n\\nUser behavior analytics\\n\\nTrend evaluation and monitoring\\n\\nGaming analytics\\n\\nAdvertising and marketing analysis\\n\\n## ClickHouse for real-time analytics\\n\\nWhether youâ€™re building user-facing dashboards, instantly responsive applications, or analyzing data on the fly, ClickHouse has you covered. Weâ€™re built to ensure that even the most sophisticated data analysis can be done intuitively, using simple SQL. Our parallelized query execution engine, best-in-class compression rates, and column-oriented design deliver unparalleled performance at scale so that you can focus on insights and forget worrying about infrastructure.\\n### Data ingestion\\n\\n### Data transformations and queries\\n\\n### Applications, dashboards, and more\\n\\n### Unlock real-time insights and user experiences\\n\\nWhether youâ€™re performing live analysis to drive business outcomes or building interactive user-facing applications, ClickHouse delivers the unparalleled speed-to-insight you can depend on. ### Simplify your SQL\\n\\nClickHouse supports an extensive library of domain-specific functions that transform even the most complex queries into simple SQL statements. With ClickHouse, real-time data exploration is easy and powerful. ### Manage data efficiently\\n\\nWith highly optimized compression techniques and our fully parallelized query pipeline, ClickHouse maximizes CPU efficiency and provides the power to quickly process huge volumes of compressed data. ### Scale effortlessly\\n\\nClickHouse scales both vertically and horizontally to meet the demands of any workload - read and write - to systems with hundreds of cores and petabytes of storage.\\nClickHouse provides the scalability you need to effortlessly handle increasing data volumes and workloads over time. ## Supporting references\\n\\nFor detailed guides about how to get started with ClickHouse for real-time analytics workloads, follow along in our blog:\\n\\n1. [Asynchronous Data Inserts in ClickHouse](/blog/asynchronous-data-inserts-in-clickhouse)\\n2. [Real-time event streaming with ClickHouse, Confluent Cloud and ClickPipes](/blog/real-time-event-streaming-with-confluent-cloud-clickhouse-and-clickpipes)\\n3. [Adding Real-Time Analytics to a Supabase Application With ClickHouse](/blog/building-real-time-applications-with-clickhouse-and-hex-notebook-keeper-engine)\\n4. [Using Materialized Views in ClickHouse](/blog/using-materialized-views-in-clickhouse)\\n5. [Working with Time Series Data in ClickHouse](/blog/working-with-time-series-data-and-functions-ClickHouse)\\n6.\\n[Change Data Capture (CDC) with PostgreSQL and ClickHouse - Part 1](/blog/clickhouse-postgresql-change-data-capture-cdc-part-1)\\n7. [Change Data Capture (CDC) with PostgreSQL and ClickHouse - Part 2](/blog/clickhouse-postgresql-change-data-capture-cdc-part-2)\\n8. [How Cloudflare Processes Hundreds of Millions of Rows per Second with ClickHouse](/blog/how-cloudflare-processes-hundreds-of-millions-of-rows-per-second-with-clickhouse)\\n\\n## Get started with ClickHouse Cloud for free\\n\\nWe'll get you started on a 30 day trial and $300 credits to spend at your own pace. [Create a free account](https://console.clickhouse.cloud/signUp?loc=real-time-use-case-getstarted-footer)\\n\\nStay informed on feature releases, product roadmap, support, and cloud offerings! Loading form...\\n\\n[Star us on Github](https://github.com/ClickHouse/ClickHouse)\\n\\nÂ© 2025 ClickHouse, Inc. HQ in the Bay Area, CA and Amsterdam, NL.\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"clickhouse.com\",\n    \"source_title\": \"Real-time Analytics with ClickHouse\",\n    \"source_link\": \"https://clickhouse.com/use-cases/real-time-analytics\"\n  },\n  {\n    \"title\": \"How to optimize ClickHouseÂ® for high-throughput streaming\",\n    \"url\": \"https://www.tinybird.co/blog/clickhouse-streaming-analytics\",\n    \"snippet\": \"[Back to Blog](/blog)\\n\\nShare this article:\\n\\n[](http://www.facebook.com/share.php?u=https://www.tinybird.co/blog/clickhouse-streaming-analytics) [](https://x.com/intent/tweet?original_referer=https://www.tinybird.co/blog/clickhouse-streaming-analytics&utm_source=x&url=https://www.tinybird.co/blog/clickhouse-streaming-analytics&utm_medium=social&utm_source=x&source=tweetbutton&text=How%20to%20optimize%20ClickHouse%C2%AE%20for%20high-throughput%20streaming) [](http://www.linkedin.com/shareArticle?mini=true&url=https://www.tinybird.co/blog/clickhouse-streaming-analytics&utm_medium=social&utm_source=linkedin) [](https://news.ycombinator.com/submitlink?u=https://www.tinybird.co/blog/clickhouse-streaming-analytics&t=How%20to%20optimize%20ClickHouse%C2%AE%20for%20high-throughput%20streaming)\\n\\nTinybird is not affiliated with, associated with, or sponsored by ClickHouse, Inc. ClickHouseÂ® is a registered trademark of ClickHouse, Inc.\\n\\n[Back](/blog)\\n\\nOct 17, 2025\\n\\n# How to optimize ClickHouse\\n<sup>Â®</sup> for high-throughput streaming\\n\\nLearn how to optimize ClickHouse <sup>Â®</sup> for million-row-per-second streaming analytics with expert tips on table design, Kafka ingestion, and query performance tuning. [AI Resources](/blog/categories/ai-resources)\\n\\n[Cameron Archer Tech Writer](/blog/authors/cameron-archer)\\n\\nStreaming analytics systems process millions of events per second, but most databases struggle to ingest data fast enough while still returning query results in milliseconds. ClickHouse <sup>Â®</sup> is built differently: its columnar storage, vectorized execution, and merge tree architecture can handle both high-velocity writes and sub-second analytical queries on billions of rows. This guide covers how to optimize ClickHouse <sup>Â®</sup> for high-throughput streaming workloads, from schema design and ingestion tuning to query optimization and infrastructure decisions.\\nYou'll learn the specific settings, patterns, and tradeoffs that determine whether your system handles thousands or millions of events per second. Unlike complex stream processors like Apache Flink, which [most teams don't actually need](https://www.tinybird.co/blog-posts/flink-is-95-problem) , ClickHouse <sup>Â®</sup> provides a simpler approach using familiar SQL. ## Why ClickHouse <sup>Â®</sup> excels at high-throughput streaming analytics\\n\\nClickHouse <sup>Â®</sup> is a columnar database designed for analytical processing, which makes it particularly well-suited for streaming analytics workloads. The database stores data by column rather than by row, which means queries that aggregate or filter specific fields can skip reading unnecessary data entirely. This columnar approach delivers compression ratios of [10x or more](https://clickhouse.com/engineering-resources/database-compression) compared to row-oriented databases.\\nWhen you're ingesting millions of events per second, that compression directly reduces both storage costs and the amount of data that queries need to scan. ClickHouse <sup>Â®</sup> uses [vectorized query execution](https://clickhouse.com/docs/en/introduction/performance.html) , processing data in batches rather than one row at a time. Modern CPUs can apply the same operation to multiple values simultaneously through SIMD instructions, which is why [ClickHouse <sup>Â®</sup> queries](https://www.tinybird.co/blog-posts/clickhouse-query-examples) can return results in under 100 milliseconds even when scanning billions of rows. The database scales both vertically and horizontally. You can add more CPU cores and memory to a single server, or you can distribute data across multiple shards with replication for fault tolerance.\\n## Core architecture for real-time ingestion and queries\\n\\nA typical [streaming analytics setup](https://www.tinybird.co/blog-posts/real-time-streaming-data-architectures-that-scale) with ClickHouse <sup>Â®</sup> involves four main components that work together. Each component handles a specific part of the data flow, from ingestion to serving queries. ### 1\\\\. Streaming sources to buffer tables\\n\\nData arrives from sources like Apache Kafka, webhooks, or HTTP APIs. ClickHouse <sup>Â®</sup> can consume directly from Kafka using the Kafka table engine, which creates a consumer that reads messages and writes them to a target table. Buffer tables accumulate incoming data in memory before flushing to disk. This reduces the number of small data parts created during high-velocity ingestion, which otherwise slows down background merges and queries. You can also batch inserts on the client side, which gives you more control over batch sizes and error handling.\\n ... \\nEach materialized view writes to its own MergeTree table that you can query independently. The tradeoff is that backfilling becomes more complex. If you need to recalculate historical aggregates, you'll need to rebuild the materialized view or manually insert corrected data. ### 3\\\\. Denormalised analytics tables\\n\\nClickHouse <sup>Â®</sup> performs best when related data lives in the same table rather than split across multiple tables with joins. This means duplicating user attributes into each event row instead of storing them separately. For example, instead of joining an events table with a users table, you might embed user\\\\_name, user\\\\_tier, and user\\\\_region directly into each event. This increases storage slightly but eliminates expensive join operations. The tradeoff is write amplification. When a user attribute changes, you may need to update many rows. For streaming analytics where reads vastly outnumber writes, this tradeoff usually makes sense. ### 4\\\\.\\n ... \\nInstead, it creates granules (blocks of rows) and stores min/max values for each granule, letting queries skip entire granules that don't match filters. ### 2\\\\. Partition by time and cardinality\\n\\nPartitioning splits a table into separate directories based on a partition key, typically a time-based column like date or hour. This makes dropping old data fast and can speed up queries that filter by the partition key. Too many partitions hurt performance though. Each partition creates separate files on disk, and ClickHouse <sup>Â®</sup> tracks metadata for all of them. A good guideline is keeping partitions under a few thousand. For most streaming workloads, daily or hourly partitions work well. Late-arriving data can still be inserted after a partition is finalized, but it may trigger additional merges. If late arrivals are common, consider using longer partition intervals. ### 3\\\\.\\nApply codecs and compression wisely\\n\\nClickHouse <sup>Â®</sup> supports several compression codecs that trade CPU time for storage savings. LZ4 is the default and balances speed with compression. ZSTD provides better compression ratios but uses more CPU during both compression and decompression. Per-column codecs can optimize specific data types:\\n\\n* **Delta and DoubleDelta:** Work well for timestamps and monotonically increasing values\\n* **Gorilla:** Designed for floating-point time series data\\n* **LZ4 vs ZSTD:** LZ4 for hot data queried frequently, ZSTD for cold data rarely accessed\\n\\nHigher compression uses more CPU during writes and reads. For data queried often, LZ4 minimizes query latency. For data rarely accessed, ZSTD significantly reduces storage costs. ### 4\\\\. Avoid costly mutations and TTL clashes\\n\\n`ALTER TABLE` operations that modify or delete existing data (mutations) are expensive because they rewrite entire data parts.\\nFrequent mutations slow ingestion by competing for disk I/O and CPU with background merges. For workloads that need updates or deletes, ReplacingMergeTree with a version column offers an alternative. This engine automatically deduplicates rows with the same `ORDER BY` key during merges, keeping only the row with the highest version number. TTL (Time To Live) rules automatically delete old data based on a time column. However, TTL operations can trigger large numbers of merges if not configured carefully. Align TTL intervals with partition boundaries to minimize merge overhead. ## Optimizing merge tree settings for streaming workloads\\n\\nClickHouse <sup>Â®</sup> continuously merges small data parts into larger ones in the background. Tuning these merge settings significantly impacts both ingestion throughput and query performance. ### Adjusting `max_partitions_to_read`\\n\\nThe `max_partitions_to_read` setting limits how many partitions a single query can scan.\\n ... \\n## Minimising query latency on hot data\\n\\nEven with optimized ingestion, queries can be slow if they scan too much data or don't use ClickHouse <sup>Â®</sup> 's indexing features. For visualizing performance metrics, you can [connect Grafana to ClickHouse <sup>Â®</sup>](https://www.tinybird.co/blog-posts/clickhouse-grafana-example) to monitor query latency and throughput in real-time. ### Bloom filters and skip indexes\\n\\nSkip indexes (also called data skipping indexes) let ClickHouse <sup>Â®</sup> skip entire granules of data that don't match a query's `WHERE` clause. Bloom filters ( `tokenbf_v1` ) are particularly useful for high-cardinality string columns like user IDs or session IDs. To add a Bloom filter index, use `ALTER TABLE ADD INDEX` with the `tokenbf_v1` type. The index is built asynchronously during background merges. Choose indexes based on your most common query patterns. If you frequently filter by `user_id` , add a Bloom filter on that column.\\n ... \\n### Balancing `max_threads` and concurrency\\n\\nThe `max_threads` setting controls how many CPU cores ClickHouse <sup>Â®</sup> uses for a single query. More threads can speed up large queries but increase CPU usage and can starve other queries of resources. For streaming analytics workloads with many concurrent queries, you may want to limit `max_threads` to prevent any single query from monopolizing the CPU. A value of 4-8 threads per query is often a good starting point. The `max_concurrent_queries` setting limits the total number of queries that can run simultaneously. This prevents overload during traffic spikes and ensures each query gets a reasonable share of CPU and memory. ## Next steps to ship streaming analytics features with Tinybird\\n\\n[Tinybird](https://tinybird.co) is a [managed ClickHouse <sup>Â®</sup> platform](https://www.tinybird.co/product/managed-clickhouse) designed for developers who want to integrate real-time analytics into applications without managing infrastructure.\\nThe platform handles ingestion, storage, query optimization, and API creation. You can get started with Tinybird by creating a free account at <https://cloud.tinybird.co/signup> . The free tier includes generous limits for development and testing, with no credit card required. Tinybird provides rapid ingestion from Kafka, webhooks, and other streaming sources, along with real-time SQL pipes that transform and aggregate data as it arrives. You can deploy parameterized API endpoints with built-in authentication, rate limiting, and row-level security. The platform includes a CLI for local development and CI/CD integration, allowing you to define data pipelines as code, test them locally, and deploy to production with a single command. ## FAQs about streaming analytics in ClickHouse <sup>Â®</sup>\\n\\n### How do I secure per-tenant analytics APIs? Row-level security (RLS) filters data based on the authenticated user or API token, ensuring each tenant only sees their own data.\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"tinybird.co\",\n    \"source_title\": \"How to optimize ClickHouseÂ® for high-throughput streaming\",\n    \"source_link\": \"https://www.tinybird.co/blog/clickhouse-streaming-analytics\"\n  },\n  {\n    \"title\": \"How we use ClickHouse as a real-time stream processing engine | Mux\",\n    \"url\": \"https://www.mux.com/blog/how-we-use-clickhouse-as-a-real-time-stream-processing-engine\",\n    \"snippet\": \"[Skip to content]()\\n\\n[Mux Logo Toggle Mux Brand Popover](/)\\n\\n[Blog Blog](/blog)\\n\\nSearch\\n\\nCopied Share Copied Share\\n\\n[Talk to us Talk to us](/sales-contact)\\n\\n[Log in Log in](https://dashboard.mux.com/login)\\n\\nPublished on December 18, 2024 (about 1 year ago)\\n\\n# How we use ClickHouse as a real-time stream processing engine\\n\\n[](/team/faith-szeto) [](/team/josh-wien)\\n\\nBy [Faith](/team/faith-szeto) and [Josh](/team/josh-wien) â€¢ 18 min read â€¢ [Engineering](/blog/category/engineering)\\n\\n* * *\\n\\nAt Mux, building video at scale comes with many interesting challenges. Weâ€™ve built systems that stream millions of minutes every hour, but the fun doesnâ€™t stop there. Every second of video that gets uploaded, watched, or deleted must be pulled from sources like CDN logs and tracked in a way that can be used by customer dashboards, billing & analytics, and our [Delivery Usage API](https://docs.mux.com/api-reference) . Thatâ€™sâ€¦ a lot of data.\\nFor a long time, we had relied on our trusty old usage pipeline to accomplish this. But as Mux scaled, the existing system became limiting in terms of product and performance. To support new features (e.g. high usage alerting), we needed to achieve fresher data, higher throughput, and faster queries in our pipeline. In this post, we will cover:\\n\\n* The limitations of our existing pipeline\\n* The changes we made to improve performance and scale\\n* Reflections and future plans\\n\\n## [How our architecture looked in the past]()\\n\\nHere is a representation of the former pipeline:\\n\\n### [**CDN log processing**]()\\n\\nWhen a view occurs, the activity is logged within one of our CDNs. Log Importer pulls CDN logs from cloud bucket stores, [performing exactly-once processing and enrichment](/blog/processing-cdn-logs-exactly-once-with-kafka-transactions) . Enriched log messages are sent to Kafka where they are further processed and aggregated by the Media Usage Processor (MUP) Flink application.\\nAggregate rows are written to Postgres, which serves read queries by downstream consumers. ### [**Internal event stream processing**]()\\n\\nIn various regions, internal CDC events are queued, transformed and then mirrored into a single-region Kafka cluster. Asset Events Flink further transforms and enriches the events, then updates Postgres with the finalized results. ### [**How we were using Flink**]()\\n\\n[Apache Flink](https://flink.apache.org/) is a popular stream processing engine which can perform stateful computations with exactly-once guarantees. Common use cases include anomaly detection and real-time analytics. Our MUP Flink application made use of [Window functions](https://flink.apache.org/2015/12/04/introducing-stream-windows-in-apache-flink/) in order to collect various aggregates like â€œseconds deliveredâ€ for a given video. For a given entity (e.g. video asset or environment) it would keep track of hourly aggregates, then finalize and write them to Postgres.\\nBefore finalizing any row, it had to delay a certain amount of time (determined by [maxOutOfOrderness](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/built_in/) , affectionately dubbed â€œMOOâ€ ðŸ®) to be reasonably sure no late records would arrive. Since we had no guarantees about ordering of CDN log records, the MOO value had to be at least an hour in order to ensure we finalized hourly windows only after all records had been processed. Asset Events Flink was simpler - its main function was to perform stateless transformations on incoming events before writing them to Postgres. Maintaining a distributed Flink deployment for this felt like overkill. ### [**Challenges with our Flink architecture**]()\\n\\nThe old system served us well for a long time, but our needs changed as we expanded our user-base and feature set.\\n* During popular live events, like the US presidential election, traffic surges could cause MUP Flink to become a throughput bottleneck unless scaled. With careful tuning and verification this could be fully automated, but at the moment scaling still involved some manual intervention. * While our user base continued to grow, we were also adding new features and breakdowns to our billing dashboard. Increasing user traffic and aggregate complexity led to very slow queries (>1s). * Operational challenges: Our Flink deployments were managed using the [Flink Kubernetes Operator](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.6/) and implemented with a different language (Java) than the rest of our stack (Go). Coupled with Flinkâ€™s steep learning curve, this meant we had a tough time debugging or deploying changes with high confidence. The technology being an outlier from our usual stack meant there was little support in terms of tooling, which made ops tedious.\\n* The performance issues with our self-hosted Postgres repeatedly required delicate sharding and vertical scaling mitigations. New features were also blocked:\\n\\n* Alerts: The ability to alert on high usage was a popular feature request among customers. However, our current aggregates would be stale by multiple hours by the time they were available for any alerting. * Better visualization and analytics: We also wanted to enhance our billing dashboards with more detail and dimensions. But the required additional load on our Postgres was a complete no-go at this point. ### [Time to rethink our approach]()\\n\\nAs growing pains and blocked features grew more pressing, we decided it was time for a rearchitecture. We needed to overcome these key issues:\\n\\n1. Poor aggregate query performance\\n2. Poor data freshness\\n3. Operational challenges\\n\\nAnd, whatever solution we came up with needed to meet the following functional requirements:\\n\\n1. Can power high usage alerting\\n2.\\nCan provide detailed usage breakdowns with hourly granularity for the past 90 days\\n3. Continue to support billing & invoicing jobs\\n\\n## [Laying out the options]()\\n\\nAfter careful research and discussion, we agreed that we had three options:\\n\\n### [**Option 1a: Have ClickHouse serve read queries**]()\\n\\nWe could keep our existing setup, but swap in [ClickHouse](https://clickhouse.com/docs/en/intro) , a database optimized for fast analytics queries, as the read-only replica for Postgres. This would require the least work, but only addressed our poor aggregate query performance, and would exacerbate operational challenges. ### [**Option 1b: Have Flink write to ClickHouse**]()\\n\\nInstead of keeping both Postgres and ClickHouse, we could replace Postgres with ClickHouse entirely. This would be pretty similar to the previous option, but without impacting data freshness issues or ops.\\n ... \\nSome organizations dedicate entire teams to operating Flink clusters. While we could cultivate team expertise, none of us were excited about working with Java. In all honesty, this really biased us against this option. **Pros**\\n\\n* Would reduce data staleness\\n* Might improve read query performance\\n* Stream processing component can remain horizontally scalable\\n\\n**Cons**\\n\\n* Operability burden\\n* Data freshness still not ideal for our use cases\\n* Aggregate queries are unlikely to be improved to the level we require\\n* Working with Javaâ€¦\\n\\n### [**Option 3: Use ClickHouse as our stream-processing engine _and_ persistent data store, replacing both Postgres and Flink**]()\\n\\nWhile we donâ€™t have a team of Flink experts, we _do_ have a team of ClickHouse experts. [ClickHouse](https://clickhouse.com/docs/en/intro) is an OLAP column-oriented database that uses a SQL-like query language. It is useful for â€œcomplex calculations over massive datasetsâ€, like real-time analytics over huge data streams.\\n ... \\nFor example, on an incoming record we had to classify its type, determine if and how much we should bill on, and extract nested objects for processing. Explaining how we achieved all of this stream processing logic in our schema would be difficult without a good understanding of ClickHouse, so we'll save the gory details for a future post. The basic gist is this: we used Kafka Table Engines to ingest from Kafka, then sent the data through a series of cascading [Materialized Views](https://clickhouse.com/docs/en/materialized-view) and [Null Tables](https://clickhouse.com/docs/en/engines/table-engines/special/null) , which allowed us to pre-aggregate and transform the data several times before dumping it into rollup tables, which were optimized for aggregate queries. ### [**Additional considerations**]()\\n\\nIn some high volume pipelines, performance can be prioritized over accuracy. The critical nature of our data prevented us from making this tradeoff.\\n ... \\nEven small amounts of lost data or changes to usage categorization will add up to substantial billing inaccuracies at our scale, and we needed to prove that the new system would be just as reliable as the old. We started by copying over all of the existing data in Postgres to ClickHouse and feeding in new data via Kafka, which was made easier thanks to ClickHouseâ€™s Postgres table engine and our aggregating schema design, but we still needed to verify that there were no dropped or duplicated usage events. Working closely with our Analytics team, we did extensive comparisons in our data warehouse between the Postgres and ClickHouse datasets. Then, going a step further to validate the data we would return to our customers via the Usage API or invoices, we also set up a query shadowing system, where any read query run against the Postgres database would be followed by sending the equivalent query to ClickHouse.\\n ... \\nBecause both of these issues only occurred at scale, we would have missed these without our integrity checks. Instead, we caught them quickly and were able to fix them well ahead of release, by which time there was little room left for doubt. When it finally came time to switch over to the new usage pipeline, the query shadowing system again made this easy and safe. Simply flipping a configuration made clients of the usage databases return the results from ClickHouse instead of Postgres, while still logging the diffs for post-release monitoring. We kept the old system running so that we could cut over back to Postgres just as easily in case an issue arose. Thanks to our careful approach to validation beforehand, however, we never had to, and the old usage pipeline was decommissioned a month later. Weâ€™ll miss you. Ensuring data integrity is an important part of any data migration, even more so when working with business-critical data.\\n ... \\n## [What this approach unlocks for our future]()\\n\\nWeâ€™re pleased with how the new architecture has opened up a whole suite of new features that had not been feasible to build on top of the old system. The improved aggregation and query performance allows us to access the data more often and in new ways, enabling us to provide more granular billing representations to internal teams and our customers. Internal teams will soon gain the ability to recognize revenue with much greater fidelity than before, saving hours of work each month, and customers will be able to dive deeper into their spend through new dashboard visualizations and APIs. On top of that, the increased freshness of the data will no doubt benefit these features, and it also unblocks billing alerting, addressing a frequently requested feature and helping customers to monitor and control their usage without having to watch or track it themselves.\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"mux.com\",\n    \"source_title\": \"How we use ClickHouse as a real-time stream processing engine | Mux\",\n    \"source_link\": \"https://www.mux.com/blog/how-we-use-clickhouse-as-a-real-time-stream-processing-engine\"\n  },\n  {\n    \"title\": \"Guide for Query optimization | ClickHouse Docs\",\n    \"url\": \"https://clickhouse.com/docs/optimize/query-optimization\",\n    \"snippet\": \"[Skip to main content]()\\n\\n[Skip to main content]()\\n\\n* [](/docs/)\\n* [Performance and optimizations](/docs/operations/overview)\\n* Query optimization\\n\\n[Edit this page](https://github.com/ClickHouse/clickhouse-docs/blob/main/docs/guides/best-practices/query-optimization.md)\\n\\n# A simple guide for query optimization\\n\\nThis section aims to illustrate through common scenarios how to use different performance and optimization techniques, such as [analyzer](/docs/operations/analyzer) , [query profiling](/docs/operations/optimizing-performance/sampling-query-profiler) or [avoid nullable Columns](/docs/optimize/avoid-nullable-columns) , in order to improve your ClickHouse query performances. ## Understand query performance [â€‹]( \\\"Direct link to Understand query performance\\\")\\n\\nThe best moment to think about performance optimization is when you're setting up your [data schema](/docs/data-modeling/schema-design) before ingesting data into ClickHouse for the first time.\\nBut let's be honest; it is difficult to predict how much your data will grow or what types of queries will be executed. If you have an existing deployment with a few queries that you want to improve, the first step is understanding how those queries perform and why some execute in a few milliseconds while others take longer. ClickHouse has a rich set of tools to help you understand how your query is getting executed and the resources consumed to perform the execution. In this section, we will look at those tools and how to use them. ## General considerations [â€‹]( \\\"Direct link to General considerations\\\")\\n\\nTo understand query performance, let's look at what happens in ClickHouse when a query is executed. The following part is deliberately simplified and takes some shortcuts; the idea here is not to drown you with details but to get you up to speed with the basic concepts. For more information you can read about [query analyzer](/docs/operations/analyzer) .\\nFrom a very high-level standpoint, when ClickHouse executes a query, the following happens:\\n\\n* **Query parsing and analysis**\\n\\nThe query is parsed and analyzed, and a generic query execution plan is created. * **Query optimization**\\n\\nThe query execution plan is optimized, unnecessary data is pruned, and a query pipeline is built from the query plan. * **Query pipeline execution**\\n\\nThe data is read and processed in parallel. This is the stage where ClickHouse actually executes the query operations such as filtering, aggregations, and sorting. * **Final processing**\\n\\nThe results are merged, sorted, and formatted into a final result before being sent to the client. In reality, many [optimizations](/docs/concepts/why-clickhouse-is-so-fast) are taking place, and we will discuss them a bit more in this guide, but for now, those main concepts give us a good understanding of what is happening behind the scenes when ClickHouse executes a query.\\nWith this high-level understanding, let's examine the tooling ClickHouse provides and how we can use it to track the metrics that affect query performance. ## Dataset [â€‹]( \\\"Direct link to Dataset\\\")\\n\\nWe'll use a real example to illustrate how we approach query performances. Let's use the NYC Taxi dataset, which contains taxi ride data in NYC. First, we start by ingesting the NYC taxi dataset with no optimization. Below is the command to create the table and insert data from an S3 bucket. Note that we infer the schema from the data voluntarily, which is not optimized.\\n```\\n-- Create table with inferred schema\\nCREATE TABLE trips_small_inferred\\nORDER BY () EMPTY\\nAS SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/clickhouse-academy/nyc_taxi_2009-2010.parquet');\\n\\n-- Insert data into table with inferred schema\\nINSERT INTO trips_small_inferred\\nSELECT *\\nFROM s3Cluster\\n('default','https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/clickhouse-academy/nyc_taxi_2009-2010.parquet');\\n```\\n\\nLet's have a look to the table schema automatically inferred from the data.\\n ... \\n[query logs](/docs/operations/system-tables/query_log) . This data is stored in the table `system.query_log` . For each executed query, ClickHouse logs statistics such as query execution time, number of rows read, and resource usage, such as CPU, memory usage, or filesystem cache hits. Therefore, the query log is a good place to start when investigating slow queries. You can easily spot the queries that take a long time to execute and display the resource usage information for each one. Let's find the top five long-running queries on our NYC taxi dataset.\\n ... \\n|Name |Elapsed |Rows processed |Peak memory |\\n| --- | --- | --- | --- |\\n|Query 1 |1\\\\.699 sec |329\\\\.04 million |440\\\\.24 MiB |\\n|Query 2 |1\\\\.419 sec |329\\\\.04 million |546\\\\.75 MiB |\\n|Query 3 |1\\\\.414 sec |329\\\\.04 million |451\\\\.53 MiB |\\n\\n\\nLet's understand a bit better what the queries achieve. * Query 1 calculates the distance distribution in rides with an average speed of over 30 miles per hour. * Query 2 finds the number and average cost of rides per week. * Query 3 calculates the average time of each trip in the dataset. None of these queries are doing very complex processing, except the first query that calculates the trip time on the fly every time the query executes. However, each of these queries takes more than one second to execute, which, in the ClickHouse world, is a very long time. We can also note the memory usage of these queries; more or less 400 Mb for each query is quite a lot of memory. Also, each query appears to read the same number of rows (i.e., 329.04 million).\\n ... \\nFinally, the result is sorted and outputted. Here, we can note that no primary keys are used, which makes sense as we didn't define any when we created the table. As a result, ClickHouse is doing a full scan of the table for the query. **Explain Pipeline**\\n\\nEXPLAIN Pipeline shows the concrete execution strategy for the query. There, you can see how ClickHouse actually executed the generic query plan we looked at previously. ```\\nEXPLAIN PIPELINE\\nWITH\\n    dateDiff('s', pickup_datetime, dropoff_datetime) AS trip_time,\\n    (trip_distance / trip_time) * 3600 AS speed_mph\\nSELECT quantiles(0.5, 0.75, 0.9, 0.99)(trip_distance)\\nFROM nyc_taxi.trips_small_inferred\\nWHERE speed_mph > 30\\n\\nQuery id: c7e11e7b-d970-4e35-936c-ecfc24e3b879\\n\\n    â”Œâ”€explainâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\\n 1. â”‚ (Expression)                                                                        â”‚\\n 2.\\n ... \\nDepending on how you ingested your data, you might have leveraged ClickHouse [capabilities](/docs/interfaces/schema-inference) to infer the table schema based on the ingested data. While this is very practical to get started, if you want to optimize your query performance, you'll need to review the data schema to best fit your use case. ### Nullable [â€‹]( \\\"Direct link to Nullable\\\")\\n\\nAs described in the [best practices documentation](/docs/best-practices/select-data-types) , avoid nullable columns wherever possible. It is tempting to use them often, as they make the data ingestion mechanism more flexible, but they negatively affect performance as an additional column has to be processed every time. Running an SQL query that counts the rows with a NULL value can easily reveal the columns in your tables that actually need a Nullable value.\\n ... \\n|Name |Run 1 - Elapsed |Elapsed |Rows processed |Peak memory |\\n| --- | --- | --- | --- | --- |\\n|Query 1 |1\\\\.699 sec |1\\\\.353 sec |329\\\\.04 million |337\\\\.12 MiB |\\n|Query 2 |1\\\\.419 sec |1\\\\.171 sec |329\\\\.04 million |531\\\\.09 MiB |\\n|Query 3 |1\\\\.414 sec |1\\\\.188 sec |329\\\\.04 million |265\\\\.05 MiB |\\n\\n\\nWe notice some improvements in both query time and memory usage. Thanks to the optimization in the data schema, we reduce the total volume of data that represents our data, leading to improved memory consumption and reduced processing time. Let's check the size of the tables to see the difference.\\n ... \\n## The importance of primary keys [â€‹]( \\\"Direct link to The importance of primary keys\\\")\\n\\nPrimary keys in ClickHouse work differently than in most traditional database systems. In those systems, primary keys enforce uniqueness and data integrity. Any attempt to insert duplicate primary key values is rejected, and a B-tree or hash-based index is usually created for fast lookup. In ClickHouse, the primary key's [objective](/docs/guides/best-practices/sparse-primary-indexes) is different; it does not enforce uniqueness or help with data integrity. Instead, it is designed to optimize query performance. The primary key defines the order in which the data is stored on disk and is implemented as a sparse index that stores pointers to the first row of each granule. > Granules in ClickHouse are the smallest units of data read during query execution. They contain up to a fixed number of rows, determined by index\\\\_granularity, with a default value of 8192 rows.\\nGranules are stored contiguously and sorted by the primary key. > \\n> \\n\\nSelecting a good set of primary keys is important for performance, and it's actually common to store the same data in different tables and use different sets of primary keys to speed up a specific set of queries. Other options supported by ClickHouse, such as Projection or Materialized view, allow you to use a different set of primary keys on the same data. The second part of this blog series will cover this in more detail. ### Choose primary keys [â€‹]( \\\"Direct link to Choose primary keys\\\")\\n\\nChoosing the correct set of primary keys is a complex topic, and it might require trade-offs and experiments to find the best combination. For now, we're going to follow these simple practices:\\n\\n* Use fields that are used to filter in most queries\\n* Choose columns with lower cardinality first\\n* Consider a time-based component in your primary key, as filtering by time on a timestamp dataset is pretty common.\\n ... \\nâ”‚                 Condition: and((pickup_datetime in (-Inf, 1238543999]), (pickup_datetime in [1230768000, +Inf))) â”‚\\n13. â”‚                 Parts: 9/9                                                                                       â”‚\\n14. â”‚                 Granules: 5061/40167                                                                             â”‚\\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\\n```\\n\\nThanks to the primary key, only a subset of the table granules has been selected. This alone greatly improves the query performance since ClickHouse has to process significantly less data. ## Next steps [â€‹]( \\\"Direct link to Next steps\\\")\\n\\nHopefully this guide gets a good understanding on how to investigate slow queries with ClickHouse and how to make them faster.\\n ... \\n[Previous Performance and optimizations overview](/docs/operations/overview) [Next Primary indexes](/docs/guides/best-practices/sparse-primary-indexes)\\n\\nÂ© 2016â€“2025 ClickHouse, Inc.\\n\\n[Trademark](https://clickhouse.com/legal/trademark-policy) Â· [Privacy](https://clickhouse.com/legal/privacy-policy) Â· [Security](https://trust.clickhouse.com/) Â· [Terms of Service](https://clickhouse.com/legal/agreements/terms-of-service)\\n\\nÂ© 2016â€“2025 ClickHouse, Inc.\\n\\n[Trademark](https://clickhouse.com/legal/trademark-policy) Â· [Privacy](https://clickhouse.com/legal/privacy-policy) Â· [Security](https://trust.clickhouse.com/) Â· [Terms of Service](https://clickhouse.com/legal/agreements/terms-of-service)\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"clickhouse.com\",\n    \"source_title\": \"Guide for Query optimization | ClickHouse Docs\",\n    \"source_link\": \"https://clickhouse.com/docs/optimize/query-optimization\"\n  },\n  {\n    \"title\": \"Architecture Overview | ClickHouse Docs\",\n    \"url\": \"https://clickhouse.com/docs/academic_overview\",\n    \"snippet\": \"](https://clickhouse.com/integrations)\\n* Use cases\\n  \\n    + [Real-time analytics](https://clickhouse.com/use-cases/real-time-analytics)\\n    + [Machine Learning & Generative AI](https://clickhouse.com/use-cases/machine-learning-and-data-science)\\n    + [Business Intelligence](https://clickhouse.com/use-cases/data-warehousing)\\n    + [Logs, Events, Traces](https://clickhouse.com/use-cases/observability)\\n    + [All use cases](https://clickhouse.com/use-cases)\\n  \\n  [All use cases](https://clickhouse.com/use-cases)\\n* [Documentation](https://clickhouse.com/docs)\\n* Resources\\n  \\n    + [User stories](https://clickhouse.com/user-stories)\\n    + [Blog](https://clickhouse.com/blog)\\n    + [Events](https://clickhouse.com/company/events)\\n    + [Learning and certification](https://clickhouse.com/learn)\\n    + Comparison\\n        \\n        - [BigQuery](https://clickhouse.com/comparison/bigquery)\\n        - [PostgreSQL](https://clickhouse.com/comparison/postgresql)\\n        -\\n ... \\nanalyzer](/docs/guides/developer/understanding-query-execution-with-the-analyzer)\\n    + [Using JOINs in ClickHouse](/docs/guides/joining-tables)\\n    + [Stored procedures & query parameters](/docs/guides/developer/stored-procedures-and-prepared-statements)\\n* [Performance and optimizations](/docs/operations/overview)\\n  \\n    + [Query optimization](/docs/optimize/query-optimization)\\n    + [Primary indexes](/docs/guides/best-practices/sparse-primary-indexes)\\n    + [Query parallelism](/docs/optimize/query-parallelism)\\n    + [Partitioning key](/docs/optimize/partitioning-key)\\n    + [Data skipping indexes](/docs/optimize/skipping-indexes)\\n    + [PREWHERE optimization](/docs/optimize/prewhere)\\n    + [Bulk inserts](/docs/optimize/bulk-inserts)\\n    + [Asynchronous Inserts](/docs/optimize/asynchronous-inserts)\\n    + [Avoid mutations](/docs/optimize/avoid-mutations)\\n    + [Avoid nullable Columns](/docs/optimize/avoid-nullable-columns)\\n    + [Avoid optimize\\n ... \\nRecurring queries additionally provide an opportunity to adapt the physical database layout to the workload. As a result, databases should offer pruning techniques that allow optimizing frequent queries. Depending on the query priority, databases must further grant equal or prioritized access to shared system resources such as CPU, memory, disk and network I/O, even if a large number of queries run simultaneously. 3. **Diverse landscapes of data stores, storage locations, and formats** . To integrate with existing data architectures, modern analytical databases should exhibit a high degree of openness to read and write external data in any system, location, or format. 4. **A convenient query language with support for performance introspection** . Real-world usage of OLAP databases poses additional \\\"soft\\\" requirements.\\nFor example, instead of a niche programming language, users often prefer to interface with databases in an expressive SQL dialect with nested data types and a broad range of regular, aggregation, and window functions. Analytical databases should also provide sophisticated tooling to introspect the performance of the system or individual queries. 5. **Industry-grade robustness and versatile deployment** . As commodity hardware is unreliable, databases must provide data replication for robustness against node failures. Also, databases should run on any hardware, from old laptops to powerful servers. Finally, to avoid the overhead of garbage collection in JVM-based programs and enable bare-metal performance (e.g. SIMD), databases are ideally deployed as native binaries for the target platform. Figure 1: ClickHouse timeline. ## 2 ARCHITECTURE [â€‹]( \\\"Direct link to 2 ARCHITECTURE\\\")\\n\\nFigure 2: The high-level architecture of the ClickHouse database engine.\\nAs shown by [Figure 2,]() the ClickHouse engine is split into three main layers: the query processing layer (described in Section [4)]() , the storage layer (Section [3)]() , and the integration layer (Section [5)]() . Besides these, an access layer manages user sessions and communication with applications via different protocols. There are orthogonal components for threading, caching, role-based access control, backups, and continuous monitoring. ClickHouse is built in C++ as a single, statically-linked binary without dependencies. Query processing follows the traditional paradigm of parsing incoming queries, building and optimizing logical and physical query plans, and execution. ClickHouse uses a vectorized execution model similar to MonetDB/X100 [[11]]() , in combination with opportunistic code compilation [[53]]() . Queries can be written in a feature-rich SQL dialect, PRQL [[76]]() , or Kusto's KQL [[50]]() .\\nThe storage layer consists of different table engines that encapsulate the format and location of table data. Table engines fall into three categories: The first category is the [MergeTree](/docs/concepts/glossary) A MergeTree in ClickHouse is a table engine designed for high data ingest rates and large data volumes. It is the core storage engine in ClickHouse, providing features such as columnar storage, custom partitioning, sparse primary indexes, and support for background data merges. \\\\* family of table engines which represent the primary persistence format in ClickHouse. Based on the idea of LSM trees [[60]]() , tables are split into horizontal, sorted [parts](/docs/concepts/glossary) A physical file (or directory) on disk that stores a portion of the table's data. This is different from a partition, which is a logical division of a table's data that is created using a partition key. , which are continuously merged by a background process.\\n ... \\nAs an optimization, the columns of a small part (smaller than 10 MB by default) are stored consecutively in a single file to increase the spatial locality for reads and writes. The rows of a part are further logically divided into groups of 8192 records, called granules. A [granule](/docs/concepts/glossary) A granule is a batch of rows in an uncompressed block. When reading data, ClickHouse accesses granules, but not individual rows, which enables faster data processing in analytical workloads. A granule contains 8192 rows by default. The primary index contains one entry per granule. represents the smallest indivisible data unit processed by the scan and index lookup operators in ClickHouse. Reads and writes of on-disk data are, however, not performed at the [granule](/docs/concepts/glossary) A granule is a batch of rows in an uncompressed block. When reading data, ClickHouse accesses granules, but not individual rows, which enables faster data processing in analytical workloads.\\n ... \\nEach block contains columnar data which is processed together to enhance performance during query execution. By processing data in blocks, ClickHouse utilizes CPU cores efficiently by minimizing cache misses and facilitating vectorized execution. ClickHouse uses various compression algorithms, such as LZ4, ZSTD, and Delta, to compress data in blocks. values. These indexes are best used with data with a small local cardinality, i.e. \\\"clumped together\\\" values. 3. Bloom filter indices [[9]]() build for row, token, or n-gram values with a configurable false positive rate. These indices support text search [[73]]() , but unlike min-max and set indices, they cannot be used for range or negative predicates. ### 3\\\\.3 Merge-time Data Transformation [â€‹]( \\\"Direct link to 3-3-merge-time-data-transformation\\\")\\n\\nBusiness intelligence and observability use cases often need to handle data generated at constantly high rates or in bursts.\\n ... \\n### 4\\\\.3 Multi-Node Parallelization [â€‹]( \\\"Direct link to 4.3 Multi-Node Parallelization\\\")\\n\\nIf the source table of a query is sharded, the query optimizer on the node that received the query (initiator node) tries to perform as much work as possible on other nodes. Results from other nodes can be integrated into different points of the query plan. Depending on the query, remote nodes may either 1. stream raw source table columns to the initiator node, 2. filter the source columns and send the surviving rows, 3. execute filter and aggregation steps and send local result groups with partial aggregation states, or 4. run the entire query including filters, aggregation, and sorting. Node 2 ... N in [Figure 8]() show plan fragments executed on other nodes holding shards of the hits table. These nodes filter and group the local data and send the result to the initiator node. The GroupStateMerge operator on node 1 merges the local and remote results before the results groups are finally sorted.\\n ... \\nMore recently, analytical execution kernels (e.g. Photon [[5]]() and Velox [[62]]() ) offer co-modified data processing for use in different analytical, streaming, and machine learning applications. The most similar databases to ClickHouse, in terms of goals and design principles, are Druid [[78]]() and Pinot [[34]]() . Both systems target real-time analytics with high data ingestion rates. Like ClickHouse, tables are split into horizontal [parts](/docs/concepts/glossary) A physical file (or directory) on disk that stores a portion of the table's data. This is different from a partition, which is a logical division of a table's data that is created using a partition key. called segments. While ClickHouse continuously merges smaller [parts](/docs/concepts/glossary) A physical file (or directory) on disk that stores a portion of the table's data. This is different from a partition, which is a logical division of a table's data that is created using a partition key.\\n ... \\nThe query processing layer optimizes queries using a wealth of techniques, and parallelizes execution across all server and [cluster](/docs/concepts/glossary) A collection of nodes (servers) that work together to store and process data. resources. Integration table engines and functions provide a convenient way to interact with other data management systems and data formats seamlessly. Through benchmarks, we demonstrate that ClickHouse is amongst the fastest analytical databases on the market, and we showed significant improvements in the performance of typical queries in real-world deployments of ClickHouse throughout the years. All features and enhancements planned for 2024 can be found on the public roadmap [[18]]() . Planned improvements include support for user transactions, PromQL [[69]]() as an alternative query language, a new datatype for semi-structured data (e.g.\\n ... \\nIn Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data (Athens, Greece) (SIGMOD '11). Association for Computing Machinery, New York, NY, USA, 37â€“48. <https://doi.org/10.1145/1989323.1989328>\\n* [8] Daniel Gomez Blanco. 2023. Practical OpenTelemetry. Springer Nature. * [9] Burton H. Bloom. 1970. Space/Time Trade-Ofs in Hash Coding with Allowable Errors. Commun. ACM 13, 7 (1970), 422â€“426. [https://doi.org/10.1145/362686. ](https://doi.org/10.1145/362686.362692) [362692](https://doi.org/10.1145/362686.362692)\\n* [10] Peter Boncz, Thomas Neumann, and Orri Erling. 2014. TPC-H Analyzed: Hidden Messages and Lessons Learned from an Infuential Benchmark. In Performance Characterization and Benchmarking. 61â€“76. [https://doi.org/10.1007/978-3-319-](https://doi.org/10.1007/978-3-319-04936-6_5) [04936-6\\\\_5](https://doi.org/10.1007/978-3-319-04936-6_5)\\n* [11] Peter Boncz, Marcin Zukowski, and Niels Nes. 2005. MonetDB/X100: Hyper-Pipelining Query Execution. In CIDR.\",\n    \"date\": null,\n    \"last_updated\": null,\n    \"source_host\": \"clickhouse.com\",\n    \"source_title\": \"Architecture Overview | ClickHouse Docs\",\n    \"source_link\": \"https://clickhouse.com/docs/academic_overview\"\n  }\n]",
                "executedAt": "2025-12-28T06:01:00.554Z",
                "output": "# Comprehensive Business and Data Analysis of ClickHouse Real-Time Analytics\n\n## 1. Executive Summary\nThe analysis of ClickHouse real-time analytics and related resources reveals a powerful database solution designed to handle high-throughput workloads while maintaining swift query performance. ClickHouse's architecture is optimized for real-time data ingestion and analysis, making it suitable for various applications, such as e-commerce optimization, fraud detection, supply chain analytics, and more. Key findings highlight the efficiency of ClickHouse in managing billions of rows and the distinct advantages its column-oriented storage architecture offers over traditional databases. \n\nPrimary recommendations include enhancing usage of built-in query optimization techniques, leveraging partitioning and indexing functionalities for better performance, and adequately training teams on the ClickHouse ecosystem. Critical risks affecting this architecture mainly revolve around managing data latency during peak loads and the potential complexity involved in maintaining high availability without impacting performance.\n\n## 2. Data Overview\nThe data analyzed for ClickHouse includes a comprehensive set of case studies, technical documents, and user experiences related to real-time analytics solutions. Below is an overview of the datasets:\n\n- **Data Sources Analyzed**: \n  - ClickHouse official website (use cases, documentation) \n  - Blogs (external and internal) detailing optimizations and user success stories\n  - Case studies across different industries using ClickHouse.\n\n- **Data Volume and Scope**: The dataset comprises multiple articles totaling thousands of words, with each source detailing specific implementations, use cases, and optimization strategies within ClickHouse.\n\n- **Data Quality Assessment**: The quality of the data appears high, with consistent formatting and relevant content providing in-depth insights into architecture, optimization techniques, and real-world applications.\n\n- **Key Entities and Relationships Identified**: \n  - ClickHouse functionalities (data ingestion, SQL querying, user dashboards)\n  - Interrelations with integration tools (Grafana, Tableau)\n  - User case studies showcasing performance improvements in analytics\n\n## 3. Detailed Analysis\n### System Architecture\nClickHouse employs a three-layer architecture consisting of:\n- **Query Processing Layer**: Handles incoming queries, optimizes plans, and executes them using a vectorized execution model for high performance.\n- **Storage Layer**: Utilizes column-oriented storage with MergeTree engines to facilitate high throughput and massive datasets; supports various compression algorithms to optimize storage.\n- **Integration Layer**: Facilitates connections with diverse data sources and formats, enhancing the usability of ClickHouse across various ecosystems.\n\nThis architecture supports efficient data handling for analytical queries, maximizing CPU core utilization while minimizing latency through compression and batch processing.\n\n### Business Processes\nClickHouse is utilized across many verticals, driving improvements in:\n- **Data Analytics**: Enabling near-instant answers to complex queries over large datasets.\n- **Operational Efficiency**: Streamlining processes from data ingestion to transformation and querying, creating responsive dashboards for user interaction.\n- **Real-Time Insights**: Supporting functions such as fraud detection, proactive maintenance, and intelligent automation through immediate data availability.\n\nApplication users report significant improvements in processing speed, enabling timely business decisions and enhanced operational capabilities.\n\n### Data Relationships\nThe interactions between data ingestion sources (like Kafka), ClickHouse tables, and querying interfaces are robust:\n- **Ingestion Sources**: ClickHouse can directly ingest from static or streaming data sources, optimizing how data flows from initial input to output queries.\n- **Transformations**: The database allows for the application of functions during ingestion and querying to create optimized datasets for analysis.\n- **User Output**: Seamless integration with BI tools provides a link between relational data storage and user-facing applications, enhancing usability.\n\n### Usage Patterns\nUse of ClickHouse typically exhibits:\n- **High Query Concurrency**: Enabling interactive applications with minimal performance degradation.\n- **Diverse Workloads**: Applications span various industries, implying flexibility and adaptability of ClickHouse for numerous scenarios.\n- **Engagement with BI Tools**: Extensive ecosystem integration supports tools like Grafana, optimizing visualization and reporting.\n\n### Organizational Context\nClickHouse's tenant structure allows for easy scaling and efficient management of resources. Users have reported positive experiences transitioning from traditional systems, highlighting advantages in scalability and performance.\n\n## 4. Key Insights\n- ClickHouse can efficiently process millions of rows per second, allowing businesses to extract real-time insights from large datasets.\n- Its columnar storage optimized for read-heavy activities leads to substantially reduced query times compared to traditional row-oriented databases.\n- Real-time dashboards built atop ClickHouse facilitate immediate business decisions, bolstered by responsiveness and operational insights.\n- The capacity to integrate with prominent BI tools maximizes data utilization and strengthens analytical capabilities across organizations.\n\n## 5. Recommendations\n### High Priority\n1. **Implement Partitioning Strategies**:\n   - **What to do**: Introduce partitioning based on time or cardinality to optimize data retrieval and manageability.\n   - **Why it's essential**: This can significantly speed up data queries and simplify the process of archiving old data.\n   - **Expected Impact**: Faster queries, reduced maintenance overhead.\n\n2. **Optimize Indexing**:\n   - **What to do**: Utilize skip indexes and Bloom filters for frequently queried attributes.\n   - **Why it's essential**: Reduces the data scanned during query execution, leading to quicker responses.\n   - **Expected Impact**: Enhanced performance and lower latency.\n\n### Medium Priority\n1. **Regular Training for Teams**:\n   - **What to do**: Conduct training sessions on ClickHouse optimizations and best practices for all relevant team members.\n   - **Why it's essential**: Equipping teams ensures they can exploit the full capabilities of ClickHouse.\n   - **Expected Impact**: Reduced misconfigurations and improved database handling.\n\n2. **Leverage Data Compression Techniques**:\n   - **What to do**: Implement appropriate compression codecs based on data access patterns (e.g., LZ4 for frequently accessed and ZSTD for rarely accessed data).\n   - **Why it's essential**: Managing data storage cost and optimizing retrieval speeds.\n   - **Expected Impact**: Lower storage costs and better CPU utilization.\n\n### Low Priority\n1. **Explore New Use Cases**:\n   - **What to do**: Investigate new analytical applications that can leverage ClickHouse's capabilities.\n   - **Why it's essential**: Continuous exploration ensures the database adapts and evolves with the organizationâ€™s needs.\n   - **Expected Impact**: Potential market advantages and enhanced business intelligence.\n\n## 6. Risks and Concerns\n### Critical Risks\n1. **Management of Latency During Peak Loads**:\n   - **Description**: High concurrency during peak events may lead to latency or errors.\n   - **Likelihood & Severity**: High likelihood in high-traffic scenarios, potential severe business impact.\n   - **Mitigation Strategy**: Implement load balancing and optimize resource allocation during high usage times.\n\n### Moderate Risks\n1. **Data Corruption or Loss During Migration**:\n   - **Description**: Transitioning from other systems may introduce data loss.\n   - **Likelihood & Severity**: Moderate, depending on current systems and processes.\n   - **Mitigation Strategy**: Develop a comprehensive data backup and migration strategy, including validation post-migration.\n\n### Minor Concerns\n1. **Limited Documentation on Advanced Features**:\n   - **Description**: Users may struggle with accessing and leveraging advanced capabilities.\n   - **Likelihood & Severity**: Relatively low; most users will adapt over time.\n   - **Mitigation Strategy**: Promote community forums and encourage sharing of best practices within teams.\n\n## 7. Things to Consider\n- **Future Scalability**: The growing amounts of data require ongoing evaluations of architecture capacity and potential for scale.\n- **Continuous Optimization**: Implementing strategies for regular monitoring and optimizing query performance and system efficiency.\n- **Best Practices**: Stay updated on the latest developments and best practices for ClickHouse to ensure competitive advantages.\n- **Integration Opportunities**: Evaluate potential integrations with emerging data sources and analytics tools.\n\n## 8. Conclusion\nClickHouse represents a robust solution for organizations demanding real-time analytics capabilities across varying workloads. The ability to process vast datasets quickly and integrate smoothly with various tools and technologies offers significant advantages. Nevertheless, proactive management of growth, continuous optimization efforts, and team training on best practices will be vital in harnessing the full potential of ClickHouse for future organizational growth and efficiency."
              }
            },
            {
              "id": "01KDHRMDJT8YYYV3N4FA1X271X",
              "title": "Summarize the general assistant report",
              "description": "Summarize the comprehensive report generated by the general assistant to provide a concise overview of the findings.",
              "status": "completed",
              "agentId": "professional-writing",
              "agentType": "chat",
              "dependencies": [
                "01KDHRMDJTW3B2MD242VDFDMR8"
              ],
              "input": {
                "body": {
                  "userPrompt": {
                    "__fromDependency": true,
                    "source": "previous-output"
                  },
                  "writingStyle": "summarizer"
                },
                "extra_body": {}
              },
              "createdAt": "2025-12-28T05:59:20.923Z",
              "completedAt": "2025-12-28T06:01:03.280Z",
              "output": "The analysis of ClickHouse emphasizes its effectiveness as a powerful database solution adept at handling high-throughput workloads while delivering impressive query performance. Its architecture is tailored for real-time data ingestion and analysis, making it fitting for various applications including e-commerce optimization, fraud detection, and supply chain analytics. Key findings indicate that ClickHouse is highly capable of managing vast datasets, leveraging its column-oriented storage design to outperform traditional databases. Recommendations urge organizations to optimize query performance through built-in techniques, effective partitioning and indexing, and to invest in team training on ClickHouse functionalities. There are notable risks around data latency during peak usage and the complexities of maintaining high availability, which require careful management.\n\nThe data examined comprises various case studies, technical documents, and user experiences that illustrate the real-time analytics capabilities of ClickHouse. These sources reveal high data quality, with consistent formatting and insightful content around system architecture, user interactions, and industry implementations. The architecture features a three-layer system that encompasses query processing, storage, and integration, facilitating efficient data handling for intensive analytical queries. Use cases across different industries demonstrate significant enhancements in operational efficiency and real-time insights. Ultimately, ClickHouse is positioned as a versatile and scalable solution for real-time analytics, particularly for organizations seeking to extract immediate insights from substantial datasets, although ongoing optimization, proactive management, and regular team training are necessary for maximizing its benefits and addressing the challenges ahead.",
              "tokenUsage": {
                "prompt_tokens": 5154,
                "completion_tokens": 278,
                "total_tokens": 5432,
                "pricing": {
                  "input_price_per_1m": 0.15,
                  "output_price_per_1m": 0.6,
                  "input_cost": 0.0007731,
                  "output_cost": 0.0001668,
                  "total_cost": 0.0009399,
                  "model_id": "gpt-4o-mini"
                }
              },
              "duration": 2571,
              "cost": 0.0009399,
              "responseFormat": "string",
              "chainMetadata": {
                "input": "# Comprehensive Business and Data Analysis of ClickHouse Real-Time Analytics\n\n## 1. Executive Summary\nThe analysis of ClickHouse real-time analytics and related resources reveals a powerful database solution designed to handle high-throughput workloads while maintaining swift query performance. ClickHouse's architecture is optimized for real-time data ingestion and analysis, making it suitable for various applications, such as e-commerce optimization, fraud detection, supply chain analytics, and more. Key findings highlight the efficiency of ClickHouse in managing billions of rows and the distinct advantages its column-oriented storage architecture offers over traditional databases. \n\nPrimary recommendations include enhancing usage of built-in query optimization techniques, leveraging partitioning and indexing functionalities for better performance, and adequately training teams on the ClickHouse ecosystem. Critical risks affecting this architecture mainly revolve around managing data latency during peak loads and the potential complexity involved in maintaining high availability without impacting performance.\n\n## 2. Data Overview\nThe data analyzed for ClickHouse includes a comprehensive set of case studies, technical documents, and user experiences related to real-time analytics solutions. Below is an overview of the datasets:\n\n- **Data Sources Analyzed**: \n  - ClickHouse official website (use cases, documentation) \n  - Blogs (external and internal) detailing optimizations and user success stories\n  - Case studies across different industries using ClickHouse.\n\n- **Data Volume and Scope**: The dataset comprises multiple articles totaling thousands of words, with each source detailing specific implementations, use cases, and optimization strategies within ClickHouse.\n\n- **Data Quality Assessment**: The quality of the data appears high, with consistent formatting and relevant content providing in-depth insights into architecture, optimization techniques, and real-world applications.\n\n- **Key Entities and Relationships Identified**: \n  - ClickHouse functionalities (data ingestion, SQL querying, user dashboards)\n  - Interrelations with integration tools (Grafana, Tableau)\n  - User case studies showcasing performance improvements in analytics\n\n## 3. Detailed Analysis\n### System Architecture\nClickHouse employs a three-layer architecture consisting of:\n- **Query Processing Layer**: Handles incoming queries, optimizes plans, and executes them using a vectorized execution model for high performance.\n- **Storage Layer**: Utilizes column-oriented storage with MergeTree engines to facilitate high throughput and massive datasets; supports various compression algorithms to optimize storage.\n- **Integration Layer**: Facilitates connections with diverse data sources and formats, enhancing the usability of ClickHouse across various ecosystems.\n\nThis architecture supports efficient data handling for analytical queries, maximizing CPU core utilization while minimizing latency through compression and batch processing.\n\n### Business Processes\nClickHouse is utilized across many verticals, driving improvements in:\n- **Data Analytics**: Enabling near-instant answers to complex queries over large datasets.\n- **Operational Efficiency**: Streamlining processes from data ingestion to transformation and querying, creating responsive dashboards for user interaction.\n- **Real-Time Insights**: Supporting functions such as fraud detection, proactive maintenance, and intelligent automation through immediate data availability.\n\nApplication users report significant improvements in processing speed, enabling timely business decisions and enhanced operational capabilities.\n\n### Data Relationships\nThe interactions between data ingestion sources (like Kafka), ClickHouse tables, and querying interfaces are robust:\n- **Ingestion Sources**: ClickHouse can directly ingest from static or streaming data sources, optimizing how data flows from initial input to output queries.\n- **Transformations**: The database allows for the application of functions during ingestion and querying to create optimized datasets for analysis.\n- **User Output**: Seamless integration with BI tools provides a link between relational data storage and user-facing applications, enhancing usability.\n\n### Usage Patterns\nUse of ClickHouse typically exhibits:\n- **High Query Concurrency**: Enabling interactive applications with minimal performance degradation.\n- **Diverse Workloads**: Applications span various industries, implying flexibility and adaptability of ClickHouse for numerous scenarios.\n- **Engagement with BI Tools**: Extensive ecosystem integration supports tools like Grafana, optimizing visualization and reporting.\n\n### Organizational Context\nClickHouse's tenant structure allows for easy scaling and efficient management of resources. Users have reported positive experiences transitioning from traditional systems, highlighting advantages in scalability and performance.\n\n## 4. Key Insights\n- ClickHouse can efficiently process millions of rows per second, allowing businesses to extract real-time insights from large datasets.\n- Its columnar storage optimized for read-heavy activities leads to substantially reduced query times compared to traditional row-oriented databases.\n- Real-time dashboards built atop ClickHouse facilitate immediate business decisions, bolstered by responsiveness and operational insights.\n- The capacity to integrate with prominent BI tools maximizes data utilization and strengthens analytical capabilities across organizations.\n\n## 5. Recommendations\n### High Priority\n1. **Implement Partitioning Strategies**:\n   - **What to do**: Introduce partitioning based on time or cardinality to optimize data retrieval and manageability.\n   - **Why it's essential**: This can significantly speed up data queries and simplify the process of archiving old data.\n   - **Expected Impact**: Faster queries, reduced maintenance overhead.\n\n2. **Optimize Indexing**:\n   - **What to do**: Utilize skip indexes and Bloom filters for frequently queried attributes.\n   - **Why it's essential**: Reduces the data scanned during query execution, leading to quicker responses.\n   - **Expected Impact**: Enhanced performance and lower latency.\n\n### Medium Priority\n1. **Regular Training for Teams**:\n   - **What to do**: Conduct training sessions on ClickHouse optimizations and best practices for all relevant team members.\n   - **Why it's essential**: Equipping teams ensures they can exploit the full capabilities of ClickHouse.\n   - **Expected Impact**: Reduced misconfigurations and improved database handling.\n\n2. **Leverage Data Compression Techniques**:\n   - **What to do**: Implement appropriate compression codecs based on data access patterns (e.g., LZ4 for frequently accessed and ZSTD for rarely accessed data).\n   - **Why it's essential**: Managing data storage cost and optimizing retrieval speeds.\n   - **Expected Impact**: Lower storage costs and better CPU utilization.\n\n### Low Priority\n1. **Explore New Use Cases**:\n   - **What to do**: Investigate new analytical applications that can leverage ClickHouse's capabilities.\n   - **Why it's essential**: Continuous exploration ensures the database adapts and evolves with the organizationâ€™s needs.\n   - **Expected Impact**: Potential market advantages and enhanced business intelligence.\n\n## 6. Risks and Concerns\n### Critical Risks\n1. **Management of Latency During Peak Loads**:\n   - **Description**: High concurrency during peak events may lead to latency or errors.\n   - **Likelihood & Severity**: High likelihood in high-traffic scenarios, potential severe business impact.\n   - **Mitigation Strategy**: Implement load balancing and optimize resource allocation during high usage times.\n\n### Moderate Risks\n1. **Data Corruption or Loss During Migration**:\n   - **Description**: Transitioning from other systems may introduce data loss.\n   - **Likelihood & Severity**: Moderate, depending on current systems and processes.\n   - **Mitigation Strategy**: Develop a comprehensive data backup and migration strategy, including validation post-migration.\n\n### Minor Concerns\n1. **Limited Documentation on Advanced Features**:\n   - **Description**: Users may struggle with accessing and leveraging advanced capabilities.\n   - **Likelihood & Severity**: Relatively low; most users will adapt over time.\n   - **Mitigation Strategy**: Promote community forums and encourage sharing of best practices within teams.\n\n## 7. Things to Consider\n- **Future Scalability**: The growing amounts of data require ongoing evaluations of architecture capacity and potential for scale.\n- **Continuous Optimization**: Implementing strategies for regular monitoring and optimizing query performance and system efficiency.\n- **Best Practices**: Stay updated on the latest developments and best practices for ClickHouse to ensure competitive advantages.\n- **Integration Opportunities**: Evaluate potential integrations with emerging data sources and analytics tools.\n\n## 8. Conclusion\nClickHouse represents a robust solution for organizations demanding real-time analytics capabilities across varying workloads. The ability to process vast datasets quickly and integrate smoothly with various tools and technologies offers significant advantages. Nevertheless, proactive management of growth, continuous optimization efforts, and team training on best practices will be vital in harnessing the full potential of ClickHouse for future organizational growth and efficiency.",
                "executedAt": "2025-12-28T06:01:03.280Z",
                "output": "The analysis of ClickHouse emphasizes its effectiveness as a powerful database solution adept at handling high-throughput workloads while delivering impressive query performance. Its architecture is tailored for real-time data ingestion and analysis, making it fitting for various applications including e-commerce optimization, fraud detection, and supply chain analytics. Key findings indicate that ClickHouse is highly capable of managing vast datasets, leveraging its column-oriented storage design to outperform traditional databases. Recommendations urge organizations to optimize query performance through built-in techniques, effective partitioning and indexing, and to invest in team training on ClickHouse functionalities. There are notable risks around data latency during peak usage and the complexities of maintaining high availability, which require careful management.\n\nThe data examined comprises various case studies, technical documents, and user experiences that illustrate the real-time analytics capabilities of ClickHouse. These sources reveal high data quality, with consistent formatting and insightful content around system architecture, user interactions, and industry implementations. The architecture features a three-layer system that encompasses query processing, storage, and integration, facilitating efficient data handling for intensive analytical queries. Use cases across different industries demonstrate significant enhancements in operational efficiency and real-time insights. Ultimately, ClickHouse is positioned as a versatile and scalable solution for real-time analytics, particularly for organizations seeking to extract immediate insights from substantial datasets, although ongoing optimization, proactive management, and regular team training are necessary for maximizing its benefits and addressing the challenges ahead."
              }
            },
            {
              "id": "01KDHRMDJVD2DSS41J3ED6T9AK",
              "title": "Create a GTA style image based on the summary",
              "description": "Generate a GTA style image that visually represents the summarized content about building stream analytics using ClickHouse.",
              "status": "completed",
              "agentId": "image-generator",
              "agentType": "image-generation",
              "dependencies": [
                "01KDHRMDJT8YYYV3N4FA1X271X"
              ],
              "input": {
                "body": {
                  "imageType": "gta-style",
                  "userPrompt": {
                    "__fromDependency": true,
                    "source": "previous-output"
                  },
                  "size": "1024x1024"
                },
                "extra_body": {
                  "output_format": "png"
                }
              },
              "createdAt": "2025-12-28T05:59:20.923Z",
              "completedAt": "2025-12-28T06:01:12.763Z",
              "output": "{\n  \"image\": {\n    \"url\": \"/api/images/Gradian_Image_01KDHRQTRTF1FKBNFA88MJ60R4.png\",\n    \"b64_json\": null,\n    \"revised_prompt\": null,\n    \"mimeType\": null\n  },\n  \"format\": \"png\",\n  \"imageType\": \"gta-style\",\n  \"userPrompt\": \"The analysis of ClickHouse emphasizes its effectiveness as a powerful database solution adept at handling high-throughput workloads while delivering impressive query performance. Its architecture is tailored for real-time data ingestion and analysis, making it fitting for various applications including e-commerce optimization, fraud detection, and supply chain analytics. Key findings indicate that ClickHouse is highly capable of managing vast datasets, leveraging its column-oriented storage design to outperform traditional databases. Recommendations urge organizations to optimize query performance through built-in techniques, effective partitioning and indexing, and to invest in team training on ClickHouse functionalities. There are notable risks around data latency during peak usage and the complexities of maintaining high availability, which require careful management.\\n\\nThe data examined comprises various case studies, technical documents, and user experiences that illustrate the real-time analytics capabilities of ClickHouse. These sources reveal high data quality, with consistent formatting and insightful content around system architecture, user interactions, and industry implementations. The architecture features a three-layer system that encompasses query processing, storage, and integration, facilitating efficient data handling for intensive analytical queries. Use cases across different industries demonstrate significant enhancements in operational efficiency and real-time insights. Ultimately, ClickHouse is positioned as a versatile and scalable solution for real-time analytics, particularly for organizations seeking to extract immediate insights from substantial datasets, although ongoing optimization, proactive management, and regular team training are necessary for maximizing its benefits and addressing the challenges ahead.\",\n  \"size\": \"1024x1024\",\n  \"model\": \"gemini-2.5-flash-image\"\n}",
              "tokenUsage": null,
              "duration": 9361,
              "cost": null,
              "responseFormat": "image",
              "chainMetadata": {
                "input": "The analysis of ClickHouse emphasizes its effectiveness as a powerful database solution adept at handling high-throughput workloads while delivering impressive query performance. Its architecture is tailored for real-time data ingestion and analysis, making it fitting for various applications including e-commerce optimization, fraud detection, and supply chain analytics. Key findings indicate that ClickHouse is highly capable of managing vast datasets, leveraging its column-oriented storage design to outperform traditional databases. Recommendations urge organizations to optimize query performance through built-in techniques, effective partitioning and indexing, and to invest in team training on ClickHouse functionalities. There are notable risks around data latency during peak usage and the complexities of maintaining high availability, which require careful management.\n\nThe data examined comprises various case studies, technical documents, and user experiences that illustrate the real-time analytics capabilities of ClickHouse. These sources reveal high data quality, with consistent formatting and insightful content around system architecture, user interactions, and industry implementations. The architecture features a three-layer system that encompasses query processing, storage, and integration, facilitating efficient data handling for intensive analytical queries. Use cases across different industries demonstrate significant enhancements in operational efficiency and real-time insights. Ultimately, ClickHouse is positioned as a versatile and scalable solution for real-time analytics, particularly for organizations seeking to extract immediate insights from substantial datasets, although ongoing optimization, proactive management, and regular team training are necessary for maximizing its benefits and addressing the challenges ahead.",
                "executedAt": "2025-12-28T06:01:12.763Z",
                "output": "{\n  \"image\": {\n    \"url\": \"/api/images/Gradian_Image_01KDHRQTRTF1FKBNFA88MJ60R4.png\",\n    \"b64_json\": null,\n    \"revised_prompt\": null,\n    \"mimeType\": null\n  },\n  \"format\": \"png\",\n  \"imageType\": \"gta-style\",\n  \"userPrompt\": \"The analysis of ClickHouse emphasizes its effectiveness as a powerful database solution adept at handling high-throughput workloads while delivering impressive query performance. Its architecture is tailored for real-time data ingestion and analysis, making it fitting for various applications including e-commerce optimization, fraud detection, and supply chain analytics. Key findings indicate that ClickHouse is highly capable of managing vast datasets, leveraging its column-oriented storage design to outperform traditional databases. Recommendations urge organizations to optimize query performance through built-in techniques, effective partitioning and indexing, and to invest in team training on ClickHouse functionalities. There are notable risks around data latency during peak usage and the complexities of maintaining high availability, which require careful management.\\n\\nThe data examined comprises various case studies, technical documents, and user experiences that illustrate the real-time analytics capabilities of ClickHouse. These sources reveal high data quality, with consistent formatting and insightful content around system architecture, user interactions, and industry implementations. The architecture features a three-layer system that encompasses query processing, storage, and integration, facilitating efficient data handling for intensive analytical queries. Use cases across different industries demonstrate significant enhancements in operational efficiency and real-time insights. Ultimately, ClickHouse is positioned as a versatile and scalable solution for real-time analytics, particularly for organizations seeking to extract immediate insights from substantial datasets, although ongoing optimization, proactive management, and regular team training are necessary for maximizing its benefits and addressing the challenges ahead.\",\n  \"size\": \"1024x1024\",\n  \"model\": \"gemini-2.5-flash-image\"\n}"
              }
            }
          ],
          "complexity": 0.8,
          "executionType": "todo_required"
        },
        "createdAt": "2025-12-28T05:59:20.944Z"
      },
      {
        "id": "01KDHRQEYA34Y2CJ8QHFY91XAA",
        "role": "assistant",
        "content": "# Comprehensive Business and Data Analysis of ClickHouse Real-Time Analytics\n\n## 1. Executive Summary\nThe analysis of ClickHouse real-time analytics and related resources reveals a powerful database solution designed to handle high-throughput workloads while maintaining swift query performance. ClickHouse's architecture is optimized for real-time data ingestion and analysis, making it suitable for various applications, such as e-commerce optimization, fraud detection, supply chain analytics, and more. Key findings highlight the efficiency of ClickHouse in managing billions of rows and the distinct advantages its column-oriented storage architecture offers over traditional databases. \n\nPrimary recommendations include enhancing usage of built-in query optimization techniques, leveraging partitioning and indexing functionalities for better performance, and adequately training teams on the ClickHouse ecosystem. Critical risks affecting this architecture mainly revolve around managing data latency during peak loads and the potential complexity involved in maintaining high availability without impacting performance.\n\n## 2. Data Overview\nThe data analyzed for ClickHouse includes a comprehensive set of case studies, technical documents, and user experiences related to real-time analytics solutions. Below is an overview of the datasets:\n\n- **Data Sources Analyzed**: \n  - ClickHouse official website (use cases, documentation) \n  - Blogs (external and internal) detailing optimizations and user success stories\n  - Case studies across different industries using ClickHouse.\n\n- **Data Volume and Scope**: The dataset comprises multiple articles totaling thousands of words, with each source detailing specific implementations, use cases, and optimization strategies within ClickHouse.\n\n- **Data Quality Assessment**: The quality of the data appears high, with consistent formatting and relevant content providing in-depth insights into architecture, optimization techniques, and real-world applications.\n\n- **Key Entities and Relationships Identified**: \n  - ClickHouse functionalities (data ingestion, SQL querying, user dashboards)\n  - Interrelations with integration tools (Grafana, Tableau)\n  - User case studies showcasing performance improvements in analytics\n\n## 3. Detailed Analysis\n### System Architecture\nClickHouse employs a three-layer architecture consisting of:\n- **Query Processing Layer**: Handles incoming queries, optimizes plans, and executes them using a vectorized execution model for high performance.\n- **Storage Layer**: Utilizes column-oriented storage with MergeTree engines to facilitate high throughput and massive datasets; supports various compression algorithms to optimize storage.\n- **Integration Layer**: Facilitates connections with diverse data sources and formats, enhancing the usability of ClickHouse across various ecosystems.\n\nThis architecture supports efficient data handling for analytical queries, maximizing CPU core utilization while minimizing latency through compression and batch processing.\n\n### Business Processes\nClickHouse is utilized across many verticals, driving improvements in:\n- **Data Analytics**: Enabling near-instant answers to complex queries over large datasets.\n- **Operational Efficiency**: Streamlining processes from data ingestion to transformation and querying, creating responsive dashboards for user interaction.\n- **Real-Time Insights**: Supporting functions such as fraud detection, proactive maintenance, and intelligent automation through immediate data availability.\n\nApplication users report significant improvements in processing speed, enabling timely business decisions and enhanced operational capabilities.\n\n### Data Relationships\nThe interactions between data ingestion sources (like Kafka), ClickHouse tables, and querying interfaces are robust:\n- **Ingestion Sources**: ClickHouse can directly ingest from static or streaming data sources, optimizing how data flows from initial input to output queries.\n- **Transformations**: The database allows for the application of functions during ingestion and querying to create optimized datasets for analysis.\n- **User Output**: Seamless integration with BI tools provides a link between relational data storage and user-facing applications, enhancing usability.\n\n### Usage Patterns\nUse of ClickHouse typically exhibits:\n- **High Query Concurrency**: Enabling interactive applications with minimal performance degradation.\n- **Diverse Workloads**: Applications span various industries, implying flexibility and adaptability of ClickHouse for numerous scenarios.\n- **Engagement with BI Tools**: Extensive ecosystem integration supports tools like Grafana, optimizing visualization and reporting.\n\n### Organizational Context\nClickHouse's tenant structure allows for easy scaling and efficient management of resources. Users have reported positive experiences transitioning from traditional systems, highlighting advantages in scalability and performance.\n\n## 4. Key Insights\n- ClickHouse can efficiently process millions of rows per second, allowing businesses to extract real-time insights from large datasets.\n- Its columnar storage optimized for read-heavy activities leads to substantially reduced query times compared to traditional row-oriented databases.\n- Real-time dashboards built atop ClickHouse facilitate immediate business decisions, bolstered by responsiveness and operational insights.\n- The capacity to integrate with prominent BI tools maximizes data utilization and strengthens analytical capabilities across organizations.\n\n## 5. Recommendations\n### High Priority\n1. **Implement Partitioning Strategies**:\n   - **What to do**: Introduce partitioning based on time or cardinality to optimize data retrieval and manageability.\n   - **Why it's essential**: This can significantly speed up data queries and simplify the process of archiving old data.\n   - **Expected Impact**: Faster queries, reduced maintenance overhead.\n\n2. **Optimize Indexing**:\n   - **What to do**: Utilize skip indexes and Bloom filters for frequently queried attributes.\n   - **Why it's essential**: Reduces the data scanned during query execution, leading to quicker responses.\n   - **Expected Impact**: Enhanced performance and lower latency.\n\n### Medium Priority\n1. **Regular Training for Teams**:\n   - **What to do**: Conduct training sessions on ClickHouse optimizations and best practices for all relevant team members.\n   - **Why it's essential**: Equipping teams ensures they can exploit the full capabilities of ClickHouse.\n   - **Expected Impact**: Reduced misconfigurations and improved database handling.\n\n2. **Leverage Data Compression Techniques**:\n   - **What to do**: Implement appropriate compression codecs based on data access patterns (e.g., LZ4 for frequently accessed and ZSTD for rarely accessed data).\n   - **Why it's essential**: Managing data storage cost and optimizing retrieval speeds.\n   - **Expected Impact**: Lower storage costs and better CPU utilization.\n\n### Low Priority\n1. **Explore New Use Cases**:\n   - **What to do**: Investigate new analytical applications that can leverage ClickHouse's capabilities.\n   - **Why it's essential**: Continuous exploration ensures the database adapts and evolves with the organizationâ€™s needs.\n   - **Expected Impact**: Potential market advantages and enhanced business intelligence.\n\n## 6. Risks and Concerns\n### Critical Risks\n1. **Management of Latency During Peak Loads**:\n   - **Description**: High concurrency during peak events may lead to latency or errors.\n   - **Likelihood & Severity**: High likelihood in high-traffic scenarios, potential severe business impact.\n   - **Mitigation Strategy**: Implement load balancing and optimize resource allocation during high usage times.\n\n### Moderate Risks\n1. **Data Corruption or Loss During Migration**:\n   - **Description**: Transitioning from other systems may introduce data loss.\n   - **Likelihood & Severity**: Moderate, depending on current systems and processes.\n   - **Mitigation Strategy**: Develop a comprehensive data backup and migration strategy, including validation post-migration.\n\n### Minor Concerns\n1. **Limited Documentation on Advanced Features**:\n   - **Description**: Users may struggle with accessing and leveraging advanced capabilities.\n   - **Likelihood & Severity**: Relatively low; most users will adapt over time.\n   - **Mitigation Strategy**: Promote community forums and encourage sharing of best practices within teams.\n\n## 7. Things to Consider\n- **Future Scalability**: The growing amounts of data require ongoing evaluations of architecture capacity and potential for scale.\n- **Continuous Optimization**: Implementing strategies for regular monitoring and optimizing query performance and system efficiency.\n- **Best Practices**: Stay updated on the latest developments and best practices for ClickHouse to ensure competitive advantages.\n- **Integration Opportunities**: Evaluate potential integrations with emerging data sources and analytics tools.\n\n## 8. Conclusion\nClickHouse represents a robust solution for organizations demanding real-time analytics capabilities across varying workloads. The ability to process vast datasets quickly and integrate smoothly with various tools and technologies offers significant advantages. Nevertheless, proactive management of growth, continuous optimization efforts, and team training on best practices will be vital in harnessing the full potential of ClickHouse for future organizational growth and efficiency.",
        "agentId": "general-assistant",
        "agentType": "chat",
        "metadata": {
          "responseFormat": "string",
          "tokenUsage": {
            "prompt_tokens": 16581,
            "completion_tokens": 1695,
            "total_tokens": 18276,
            "pricing": {
              "input_price_per_1m": 0.15,
              "output_price_per_1m": 0.6,
              "input_cost": 0.0024871499999999996,
              "output_cost": 0.001017,
              "total_cost": 0.0035041499999999993,
              "model_id": "gpt-4o-mini"
            }
          },
          "duration": 22065,
          "cost": 0.0035041499999999993,
          "todoId": "01KDHRMDJTW3B2MD242VDFDMR8",
          "todoTitle": "Create a general assistant report based on search results"
        },
        "createdAt": "2025-12-28T06:01:00.618Z"
      },
      {
        "id": "01KDHRQHK20FJFDJTNRJKMVGKC",
        "role": "assistant",
        "content": "The analysis of ClickHouse emphasizes its effectiveness as a powerful database solution adept at handling high-throughput workloads while delivering impressive query performance. Its architecture is tailored for real-time data ingestion and analysis, making it fitting for various applications including e-commerce optimization, fraud detection, and supply chain analytics. Key findings indicate that ClickHouse is highly capable of managing vast datasets, leveraging its column-oriented storage design to outperform traditional databases. Recommendations urge organizations to optimize query performance through built-in techniques, effective partitioning and indexing, and to invest in team training on ClickHouse functionalities. There are notable risks around data latency during peak usage and the complexities of maintaining high availability, which require careful management.\n\nThe data examined comprises various case studies, technical documents, and user experiences that illustrate the real-time analytics capabilities of ClickHouse. These sources reveal high data quality, with consistent formatting and insightful content around system architecture, user interactions, and industry implementations. The architecture features a three-layer system that encompasses query processing, storage, and integration, facilitating efficient data handling for intensive analytical queries. Use cases across different industries demonstrate significant enhancements in operational efficiency and real-time insights. Ultimately, ClickHouse is positioned as a versatile and scalable solution for real-time analytics, particularly for organizations seeking to extract immediate insights from substantial datasets, although ongoing optimization, proactive management, and regular team training are necessary for maximizing its benefits and addressing the challenges ahead.",
        "agentId": "professional-writing",
        "agentType": "chat",
        "metadata": {
          "responseFormat": "string",
          "tokenUsage": {
            "prompt_tokens": 5154,
            "completion_tokens": 278,
            "total_tokens": 5432,
            "pricing": {
              "input_price_per_1m": 0.15,
              "output_price_per_1m": 0.6,
              "input_cost": 0.0007731,
              "output_cost": 0.0001668,
              "total_cost": 0.0009399,
              "model_id": "gpt-4o-mini"
            }
          },
          "duration": 2571,
          "cost": 0.0009399,
          "todoId": "01KDHRMDJT8YYYV3N4FA1X271X",
          "todoTitle": "Summarize the general assistant report"
        },
        "createdAt": "2025-12-28T06:01:03.330Z"
      },
      {
        "id": "01KDHRQTW456BJ50H0W4Z36AXE",
        "role": "assistant",
        "content": "{\n  \"image\": {\n    \"url\": \"/api/images/Gradian_Image_01KDHRQTRTF1FKBNFA88MJ60R4.png\",\n    \"b64_json\": null,\n    \"revised_prompt\": null,\n    \"mimeType\": null\n  },\n  \"format\": \"png\",\n  \"imageType\": \"gta-style\",\n  \"userPrompt\": \"The analysis of ClickHouse emphasizes its effectiveness as a powerful database solution adept at handling high-throughput workloads while delivering impressive query performance. Its architecture is tailored for real-time data ingestion and analysis, making it fitting for various applications including e-commerce optimization, fraud detection, and supply chain analytics. Key findings indicate that ClickHouse is highly capable of managing vast datasets, leveraging its column-oriented storage design to outperform traditional databases. Recommendations urge organizations to optimize query performance through built-in techniques, effective partitioning and indexing, and to invest in team training on ClickHouse functionalities. There are notable risks around data latency during peak usage and the complexities of maintaining high availability, which require careful management.\\n\\nThe data examined comprises various case studies, technical documents, and user experiences that illustrate the real-time analytics capabilities of ClickHouse. These sources reveal high data quality, with consistent formatting and insightful content around system architecture, user interactions, and industry implementations. The architecture features a three-layer system that encompasses query processing, storage, and integration, facilitating efficient data handling for intensive analytical queries. Use cases across different industries demonstrate significant enhancements in operational efficiency and real-time insights. Ultimately, ClickHouse is positioned as a versatile and scalable solution for real-time analytics, particularly for organizations seeking to extract immediate insights from substantial datasets, although ongoing optimization, proactive management, and regular team training are necessary for maximizing its benefits and addressing the challenges ahead.\",\n  \"size\": \"1024x1024\",\n  \"model\": \"gemini-2.5-flash-image\"\n}",
        "agentId": "image-generator",
        "agentType": "image-generation",
        "metadata": {
          "responseFormat": "image",
          "tokenUsage": null,
          "duration": 9361,
          "cost": null,
          "todoId": "01KDHRMDJVD2DSS41J3ED6T9AK",
          "todoTitle": "Create a GTA style image based on the summary"
        },
        "createdAt": "2025-12-28T06:01:12.837Z"
      }
    ],
    "selectedAgentId": null,
    "lastMessage": "{\n  \"image\": {\n    \"url\": \"/api/images/Gradian_Image_01KDHRQTRTF1FKBNFA88MJ60R4.png\",\n    \"b64_json\"",
    "lastMessageAt": "2025-12-28T06:01:12.837Z"
  }
]